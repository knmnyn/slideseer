0	Towards a Robust Query Optimizer: A Principled and Practical Approach ACM Sigmod Conference, 2005  Babcock B. and Chaudhuri S. Presented by:  Esma Kılıç 
1	Outline Introduction Query Optimizer An Alternative Approach to Query Optimization Cardinality Estimation Related Work The New Approach The Performance/Predictability Tradeoff Incorporating the Probability Distribution Analysis of the Approach Experiments Future Work Conclusion 
2	IntroductionQuery Optimizer The task: To select a low-cost query plan. The difficulty: Only incomplete and imprecise information about query plan cost is available to the optimizer at query compilation time. The standard approach: First, generate rough guesses as the values of the relevant cost model parameters using some statistics of the data. Next, using rough guesses as inputs, invoke a search algorithm to find the least costly plan. 
3	IntroductionAn Alternative Approach to Query Optimization The problem: The standard approach ignores the uncertainty about the values of important cost parameters while calculating unique values. The solution: The new approach uses probabilistic reasoning to acknowledge uncertainties in the query planning process in a principled manner.  	Therefore, it is capable of producing query plans that are more robust to estimation errors and changes in the running environment.  	It focuses on the cardinality estimation phase of optimization. 
4	Cardinality Estimation,a central subproblem in query optimization The time that a particular query plan takes to execute is crucially dependent on the sizes of the relations accessed in the query for that plan. However, they cannot generally be computed exactly without first executing the query plan.  Therefore, query optimizer needs to estimate them to produce cost estimation of the plan. The problem of producing accurate size estimates for intermediate results is known as the cardinality estimation problem.  
5	Cardinality EstimationEstimating The Selectivities Typically, the sizes of base relations are known; the challenging part of cardinality estimation involves  estimating the selectivities of the various selection conditions and  estimating join predicates in a query.  From the optimizer’s point of view, the best case for the selectivity estimation would be  to produce a probability distribution over possible selectivities  instead of a point estimate of selectivity.   
6	Related Work Various estimation techniques have been proposed (histograms, Fourier transformations, sampling, ..). But they use AVI assumption to simplify calculations.  Under the attribute value independence (AVI) assumption, predicates on different attributes are assumed to be independent of each other.  	e.g. P(A=a and B=b) = P(A=a) . P(B=b) In fact, AVI assumption is violated frequently in real practice and causes serious query optimization errors. 
7	Related Work New techniques for modeling correlated multidimensional distributions, such as multidimensional histograms, have been proposed. But, none of them have yet been adopted in commercial DBMS and  they still provide a single-point estimate of cardinality without providing any information about the uncertainty of the estimate. The degree of uncertainty can be quite important in selecting the most appropriate query plan. 
8	The New Approach The proposed method: A cardinality estimation procedure that is based on Bayesian inference from pre-computed random samples. The advantages:  By using Bayes’ rule, the problematic AVI assumption of previous studies is avoided. An appropriate trade-off between predictability and performance is expressed based on user or application preferences. The procedure is compatible with the architecture of existing query optimizers. 
9	The Performance/Predictability Tradeoff An example query optimizer problem :  A single query Q running against a table with N rows Choose an access method to retrieve records from relation R that satisfy the predicate (A=a) and (B=b),  where A and B are two indexed attributes of R. Two different plans, P1 and P2, exist: Index intersection plan  Sequential scan plan The two plans have different degrees of dependency on the (unknown) query selectivity. 
10	The Performance/Predictability Tradeoff Costs of the plans: Index intersection plan identifies the qualifying records based on the indexes and then retrieves just those records. 	If the number of records to be retrieved is low, it performs well.    However, since the index intersection plan requires one random disk read per record, it overcomes poorly when the selectivity is high. Sequential scan plan’s cost is not directly depends the query selectivity. 
11	The Performance/Predictability Tradeoff 	Which plan is preferable? Plan-1, if query selectivity is less than 26%.  Plan-2, if the query selectivity is greater that 26%. 
12	The Performance/Predictability Tradeoff As selectivity varies, execution cost of Plan-2 does not vary so much, there is no risk for it to have high cost.  	However, cost of Plan-1 varies so much as selectivity varies. It may have very small or very large cost value, depending on the selectivity. When the probability distribution for selectivity is collapsed to its expected value, as done in previous studies, the information is lost. Therefore, it should not be claimed that “the query plan that has least expected cost is selected”.  
13	Reasoning About Uncertainity    How can knowledge of the probability distribution for selectivity be used to improve the query optimizer?    How can such a probability distribution be estimated? 
14	Incorporating the Probability Distribution 	Which plan is preferable?     Which part of the probability distribution is more important? The middle part, i.e. the “typical” behavior  The right-hand tail, i.e. the “realistic worst case” behavior 
15	Incorporating the Probability Distribution 	Which plan is preferable? Plan-1, if minimizing expected cost is the overriding concern. Plan-2, if users are more risk-averse,     since the slightly higher expected cost of Plan-2 has greater predictability, the risk that the query will take much longer than expected to execute is reduced.  
16	Incorporating the Probability DistributionConfidence Threshold The desired tradeoff between performance and predictability is expressed by means of an user/application-specified parameter called as confidence threshold, T%. T% can be alternatively (and equivalently) described in terms of the cumulative distribution function (cdf) for query execution cost.  T% =  cdf(c)    c = cdf-1(T%) 
17	Incorporating the Probability DistributionDerivation of Execution Cost Distribution pdf(c) can be derived from  pdf(s) where pdf(s’)= (# of occurence of s’ ) / (# of tuples) cost function c=g(s) that is available implicit form through the cost estimation module But deriving cost for various selectivity could be an expensive task. Under the assumption that query execution cost is monotonically increasing function of selectivity, it is enough to invert the cdf for selectivity. s’=cdf-1(T%) 
18	Selectivity Estimation via Sampling In this study, the selectivity estimation is performed using uniform random samples of the relations in the database. It improves robustness of the query process.  During query optimization, the cardinality estimation module is invoked for each relational subexpression. For each such expression, how many tuples satisfy the expression is counted. The number of satisfying tuples and the overall number of tuples in the sample are used in pdf calculation.  
19	Selectivity Estimation via Sampling Random sampling has some advantages over most selectivity estimation techniques: It does not use the AVI assumption, so estimation errors are lessen The dimensionality of the data does not affect the accuracy of random sampling It is simple to implement 
20	Deriving the Probability Distribution Suppose database consists of N tuples The predicate is satisfied by pN tuples  	(fraction of p of the database satisfies the predicate and 1-p do not)  Probability of selecting k tuples out of N satisfying the predicate P(X|p): Bernoulli distribution = pk.(1-p)(N-k) 
21	Deriving the Probability Distribution The conditional density: (Bayes’ Rule) 	pdf(p=z|X) = P(X|p=z).pdf(p=z) / P(X)    posterior = likelihood*prior / evidence The unknown quantity p is treated as a random variable  Aim is to find the conditional probability distribution for p=z, given the observed data X 
22	Deriving the Probability Distribution Bayes’ Rule: 	    pdf(p=z|X) = P(X|p=z).pdf(p=z) / P(X) P(X): Does no depend on z, common for all p=z P(X|p=z) = zk.(1-z)(N-k) pdf(p=z): Prior knowledge about query workload is lacking is assumed to be uniform distribution	f(z)=1 for 0<=z<=1 	OR is assumed to be beta distribution (most widely accepted non-informative prior)   f(z) ≡ z -1/2 .(1-z) -1/2 
23	Summary of the Estimation Procedure The optimizer searches the optimal plan through many possible query plans.  During this search, the optimizer makes a number of subroutine calls to the cardinality estimation module to estimate the size of various intermediate query results.  In this study, the cardinality estimation module is modified to estimate the selectivity of each query predicate: 
24	Summary of the Estimation Procedure Determine the appropriate random sample of the data to use, based on the relations involved in the query expression.  Evaluate the predicate on the sample and use Bayes’ rule to infer a probability distribution for selectivity.  Choose the proper confidence threshold T% based on user preferences and compute the selectivity 	 s = cdf-1(T%). Return ‘s’ as the estimated selectivity of the predicate. 
25	AnalysisAn example Assume a simple linear cost model: 	the execution time for query plan Pi = vi*pN + fi pN: the number of tuples satisfying the query predicate,  p: the query selectivity N: total number of rows vi is the incremental cost per tuple for plan Pi fi is the fixed overhead, independent of query selectivity, for plan Pi 
26	AnalysisAn example Plan-1 and Plan-2 roughly resemble an index intersection plan and a sequential scan plan.  The parameter values that are chosen empirically:  N = 6.000.000,  f1 = 5,    v1 = 3.5×10-3  f2 = 35,  v2 = 3.5×10-6  The crossover point where plan Plan-2 becomes better than plan Plan-1 is at a selectivity of; 	pc = (f1-f2) / (v2N-v1N) = 0.14% 
27	AnalysisAn example If the query optimizer knew the actual query selectivity exactly, it would always choose  plan Plan-2 whenever the selectivity p < the crossover point pc and  plan Plan-1 when p > pc 0.14%. Instead, the estimated selectivity s’=cdf-1(T%) will be compared with pc 
28	Analytical Results Higher T%  the query optimizer more prone to overestimation.  (The optimizer choses P2 when P1 would be better) Lower T% the query optimizer more prone to underestimation. (The optimizer chooses P1 when P2 would be better) 
29	Analytical Results The higher T%  the less variability occurs in the query optimization time The moderate settings T% are better than extremely high or low settings at producing low expected execution times. 
30	Experiments The query optimizer of a commercial DBMS, Microsoft SQL Server, is modified by replacing the existing histogram-based cardinality estimation module with the proposed module.  Larger selectivities are easier to estimate accurately, so estimation error is less when the selectivity is higher. Therefore, low-selectivity queries are concentrated.  The TPC-H benchmark dataset (~1 GB) is used. 
31	ExperimentsSingle-Table Query Scenario The template used for this scenario is  SELECT SUM(extendedprice) FROM lineitem WHERE shipdate BETWEEN ’07/01/97’ AND ’09/30/97’ 	 AND receiptdate BETWEEN (’07/01/97’+?) AND (’09/30/97’+?) The degree of overlap between the set of rows satisfying the condition on shipdate and receiptdate  is controlled by the value of ‘?’   varied so that the overall query selectivity was between 0% and 0.6% of the 6 million rows from the lineitem table. 
32	ExperimentsSingle-Table Query Scenario 	The commercial DBMS’s standard estimation module is worse!.. Because it always selects the index intersection plan, which performed poorly at higher selectivities. 
33	ExperimentsSingle-Table Query Scenario 	The variance in execution cost decreases steadily as T% increases.     Commercial DBMS’s standard estimation is significantly worse!.. Because it always uses AVI assumption even with correlated data instead of Bayes’ Rule. 
34	ExperimentsThree-Table Join Scenario In this query scenario, the query template consists of a natural join between  the lineitem table,  the orders table, and  the part table.  There is an additional selection condition on the part table which is used as the free parameter that is varied in this scenario. All relations clustered on their primary keys, with additional indexes on the foreign key columns. 
35	ExperimentsThree-Table Join Scenario In this query scenario, the optimal query plan has one of three different structures, depending on the number of rows selected from the part table that satisfy the predicate:  At low selectivities, the best plan is to first join part to lineitem using indexed nested loops join, and then hash join with orders.  When the selectivity gets a little higher, the best plan is a sequence of hash joins, first between lineitem and part and then with orders.  When the query selectivity goes higher than approximately 10%, the optimal plan is to first merge join the two larger relations, lineitem and orders, and then hash join with part. 
36	ExperimentsThree-Table Join Scenario    This shows that the properties of the proposed method is applicable for a broader class of queries.    Although the types of queries involved in two scenarios are quite different, each of subfigures is similar to its counterpart from the first scenario. 
37	Experimental Conclusions A confidence threshold of 80% achieves both good performance (low average execution time) and good predictability (little variability in execution time). A confidence threshold of 95% leads to very stable query plans so it is good for situations where predictability is the major concern. Confidence thresholds below 50% are rather speculative and are likely to be of limited applicability. 
38	Future Work As join expressions, only foreign-key joins are used. Extending the techniques to work with the full generality of SQL is a direction for future work. The time spent in query optimization was about 30%-40% more than time using standard histograms.  Because the implementation lacks even basic optimization such as memorizing. Validation of experimental conclusions will be checked through additional experimentation. 
39	Conclusion By one definition, a robust query optimizer is one that generates plans that work reasonably well even when optimizer assumptions fail to hold.  In this study, a novel cardinality estimation procedure is developed that manages uncertainty in a principled way by reasoning probabilistically about selectivity.  	“How can we increase the robustness of query optimizers?” 
40	Conclusion Users should be allowed to prioritize performance criterions.  In the estimation technique in this study, the query planning process is incorporated with user or application preferences about the predictability vs. performance tradeoff, explicitly expressed through the setting of a single parameter. 
41	Thanks for listening Questions 
