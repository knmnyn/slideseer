0	 Deriving Private Information from Randomized Data  
1	 Zhengli Huang, Wenliang Du and Biao Chen  
2	 Department of Electrical Engineering and Computer Science Syracuse University, Syracuse, NY 13244 Tel: 315-443-9180 Fax: 315-443-1122  
3	 { zhuang,wedu,bichen } @ecs.syr.edu  
4	 ABSTRACT  
5	 Randomization has emerged as a useful technique for data disguising in privacy-preserving data mining. Its privacy properties have been studied in a number of papers. Kargupta et al. challenged the randomization schemes, and they pointed out that randomization might not be able to preserve privacy. However, it is still unclear what factors cause such a security breach, how they affect the privacy preserving property of the randomization, and what kinds of data have higher risk of disclosing their private contents even though they are randomized.  
6	 We believe that the key factor is the correlations among attributes. We propose two data reconstruction methods that are based on data correlations. One method uses the Principal Component Analysis (PCA) technique, and the other method uses the Bayes Estimate (BE) technique. We have conducted theoretical and experimental analysis on the relationship between data correlations and the amount of private information that can be disclosed based our proposed data reconstructions schemes. Our studies have shown that when the correlations are high, the original data can be reconstructed more accurately, i.e., more private information can be disclosed.  
7	 To improve privacy, we propose a modified randomization scheme, in which we let the correlation of random noises "similar" to the original data. Our results have shown that the reconstruction accuracy of both PCA-based and BEbased schemes become worse as the similarity increases.  
8	 Keywords  
9	 Privacy-Preserving Data Mining, Randomization, PCA, and Bayes Estimate.  
10	 1. INTRODUCTION  
11	 With the advance of the information age, data collection and data analysis have exploded both in size and complexity. The attempt to extract important patterns and trends from the vast data sets has led to a challenging field called  
12	 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMOD 2005 June 14-16, 2005, Baltimore, Maryland, USA Copyright 2005 ACM 1-59593-060-4/05/06 $ 5.00. Data Mining. When a complete data set is available, various statistical, machine learning and modeling techniques can be applied to analyze the data. In many contexts, data are distributed across different sites. Traditionally, the data warehousing approach has been used to mine distributed databases. It requires that data from all the participating sites are collected at a centralized warehouse. However, many data owners may be reluctant to share their data with others due to privacy and confidentiality concerns. This is a serious impediment to perform mutually beneficial data mining tasks.  
13	 Privacy-Preserving Data Mining (PPDM) has emerged to address this issue. The research of PPDM is aimed at bridging the gap between collaborative data mining and data confidentiality. It involves many areas such as statistics, computer sciences, and social sciences. It is of fundamental importance to homeland security, modern science, and to our society in general.  
14	 Agrawal and Srikant first proposed using randomization to solve PPDM problems [2]. In their randomization scheme, a random number is added to the value of a sensitive attribute. For example, if x i is the value of a sensitive attribute, x i +r, rather than x i , will appear in the database, where r is a random value drawn from some distribution. It is shown that given the distribution of random noises, recovering the distribution of the original data is possible. The randomization techniques have been used for a variety of privacy preserving data mining work [1, 21, 9, 7].  
15	 Kargupta et al. challenged the randomization schemes, and they pointed out that randomization might not be secure [16]. They proposed a random matrix-based Spectral Filtering (SF) technique to recover the original data from the perturbed data. Their results have shown that the recovered data can be reasonably close to the original data. The results indicate that for certain types of data, randomization might not preserve privacy as much as we have believed.  
16	 Motivated by Kargupta et al's work, we want to answer a series of important questions that are still unanswered: what are the key factors that decide the accuracy of the data reconstruction? what are the conditions that make data less privacy preserving using randomization? can we improve randomization to achieve better privacy? Being able to answer these questions is important to understand how secure the randomization schemes are: first it tells us what types of data should not use the randomization to disguise; second, this understanding gives us a clear direction on how to improve the randomization to achieve better privacy preservation.  
17	 37  
18	 We hypothesize that the relationships among data attributes might be the key factor that decides how much privacy can be preserved. Our hypothesis is motivated by the following intuitive extreme case: Assume that there are m numbers that have exactly the same values z. If each of them is disguised by an independent uniformly-random number with mean zero, we can estimate the value z by calculating the mean of these m perturbed numbers. As we know, the mean converges to z when m becomes large. Although the above example is unrealistic, it indicates that when data are highly correlated (thus redundant), we are able to derive, from the disguised data, more accurate information about the original data. In other words, there exists a strong relationship between the correlation and the randomization's privacy-preserving property.  
19	 The goal of this paper is to find out such a relationship, then based on which to understand how well the randomization works in terms of privacy preserving. We have developed two methods that exploit the correlations among data to reconstruct the original data from a randomized data set.  
20	 Our first scheme is based on Principal Component Analysis (PCA) method, which provides a framework for us to control the degree of redundancy, we choose to use a scheme that is directly based on PCA theory. Kargupta's scheme is also based on PCA, but part of it is based on Matrix perturbation theory, which makes it difficult to achieve a clear understanding of the correlation-vs-privacy relationship. The choice of directly basing on PCA is not motivated by the performance (actually, both schemes have similar performance under some conditions), it is rather motivated by its simplicity and being able to give an intuitive theoretical explanation on why it works. We call both schemes the PCA-based schemes.  
21	 We have also developed a method that is more general than the PCA-base schemes. In this scheme, we formulate the data reconstruction problem as an estimation problem, i.e., given the disguised data Y , we find X, such that the posterior probability P (X | Y ) is maximized. This is exactly the Bayes estimate [20]. We use the Bayes estimate techniques to solve X, and then use X as the final reconstructed data. Our results show that this method can obtain more accurate results than the PCA-based schemes.  
22	 Based on our conclusion that correlationship can reveal private information, we propose a modified randomization scheme, in which we let correlations of random noise "similar" to the original data. We have shown that the results of data reconstructions based on both PCA-based and BEbased schemes become worse when the correlation of noise becomes more and more "similar" to the original data.  
23	 The rest of the paper is organized as follows. We discuss the related work in Section 2. In Section 3, we summarize the factors that can affect the privacy of randomization. In Section 4, we show a univariate data reconstruction scheme that does not exploit data correlations. The result of this scheme is used as the baseline data for our comparison. In Section 5 and 6, we present our PCA-based data reconstruction scheme and BE-based data reconstruction scheme, respectively. The experiment results are presented in Section 7. Section 8 describes an improved randomization scheme and its experiment results. Finally we summarize our work in Section 9. 2. RELATED WORK  
24	 There are two general approaches to privacy preserving data mining: the randomization approach and the Secure Multi-party Computation (SMC) approach. In the randomization approach, random noises are added to the original data, and only the disguised data are shared [2, 1, 21, 9, 7]. There are two different randomization methods: the Random Perturbation scheme and the Randomized Response scheme.  
25	 Agrawal and Srikant proposed a scheme for privacy-preserving data mining using random perturbation [2]. This work has been extended by Agrawal and Aggarwal [1]. Under the scheme, Evfimievski et al. proposed an approach to conduct Privacy-Preserving Association Rule Mining [9].  
26	 The randomized response is mainly used to deal with categorical data. Rizvi and Haritsa presented a scheme called MASK to mine associations with secrecy constraints [21]; Du and Zhan proposed an approach to conduct PrivacyPreserving Decision Tree Building [7]. All these approaches are based on the Randomized Response technique proposed by Warner [26].  
27	 Privacy is analyzed in most of the above studies, in addition, two studies have focused on the privacy analysis. The first one is due to Evfimievski et al. [8], and the other is due to Kargupta et al. [16]. In their paper, Evfimievski et al. presented a formula of privacy breaches and a methodology to limit the breaches in the field of association rule mining.  
28	 Kargupta et al. pointed out an important issue: arbitrary randomization is not safe [16]. Inspired by their work, we study why and how correlations affect privacy. In addition to correlations, we identify other potential factors that can influence privacy.  
29	 Another approach to achieve Privacy-Preserving Data Mining is to use Secure Multi-party Computation (SMC) techniques. Briefly, an SMC problem deals with computing certain function on multiple inputs, in a distributed network where each participant holds one of the inputs; SMC ensures that no more information is revealed to a participant in the computation than what can be inferred from the participant's input and the final output [11].  
30	 Several SMC-based privacy-preserving data mining schemes have been proposed [17, 19, 23, 24, 5]. Lindell and Pinkas used SMC to build decision trees over the horizontally partitioned data [17]. Vaidya and Clifton proposed the solutions to the clustering problem [24] and the association rule mining problem [23] for vertically partitioned data. Several SMC tools and fundamental techniques are also proposed in the literature [19, 5]. Some more schemes were presented in recent conferences as follows. Wright et al. [27] and Meng et al. [18] used SMC to solve privacy-preserving Bayesian network problems. Gilburd et al. proposed a new privacy model, k-privacy, for real-world large-scale distributed systems [10]. Sanil et al. described a privacy-preserving algorithm of computing regression coefficients [22]. Du et al. have developed building blocks to solve secure two-party Multivariate Linear Regression and Classification problems [6]. Wang et al. used an iterative bottom-up generalization to generate data, which remains useful to classification but difficult to disclose private sources [25].  
31	 38 3. DERIVING PRIVATE INFORMATION  
32	 Kargupta et al. used a data reconstruction approach to derive private information from a disguised data set [16]. Namely, a new data set X  is reconstructed from the disguised data using certain algorithms, and the difference between X  and the actual original data set X indicates how much private information can be disclosed. The further apart X  is from X, the higher level of the privacy preservation is achieved. Therefore, the difference between X  and X can be used as the measure to quantify how much privacy is preserved. Our work also uses data reconstruction approaches, but we propose two different data reconstruction algorithms.  
33	 A variety of information can lead to the disclosure of private information in a disguised data set. We summarize several of them in the following:  
34	 · Attribute Dependency: Attributes in many data sets are not independent, and some attributes might have a strong correlationship among themselves. It is important to understand how such relationship can cause private information disclosure.  
35	 · Sample Dependency: For certain types of data sets, such as the time series data, there exists serial dependency among the samples. Even after perturbing the data with random noise, this dependency can still be recovered. For instance, various techniques are available from the signal processing literature to de-noise the contaminated signals. One interesting research problem is: for different types of data, what kind of dependency relationships will help the adversaries reconstruct the original data?  
36	 · Partial Value Disclosure: In practice, it is possible that the values of some attributes can be disclosed (via other channels). For example, assume we have a medical database that is disguised by randomization schemes. Knowing that the patient Alice has diabetes and heart problems, we might be able to estimate the other information about her. How to quantify privacy under these circumstances?  
37	 · Data Mining Results: In the SMC approach, all the participating parties can see the final results. These results contain aggregate information about the data, which can lead to possible privacy breaches. For example, in the association rule mining, assume that there is a rule saying that A implies B with 90% of support. Even if one party knows only A and the association rule results, he or she will be able to infer B with high confidence. How do various data mining results, including classification models, association rules, and clustering affect individual privacy? Kantarcioglu et al. has initiated studies on this issue [15].  
38	 The scope of the problems described above are broader than what we have covered in this paper. In this paper, we focus on the first problem, i.e., how to use data correlation information to derive private information?  
39	 4. UNIVARIATE DATA RECONSTRUCTION  
40	 In this section, we describe two data reconstruction methods derived from the existing work on randomization. The first approach is only based on the distribution of noise. It does not consider the distribution of the original data X. The second approach bases its guess on the posterior distribution P (X|Y ), which can be estimated from the disguised data. Because both reconstruction methods treat each attribute independently without considering the dependency relationship among attributes, we treat X and Y as if they are one attribute.  
41	 We assume that the adversaries have the disguised data Y = X + R, where X is the original data, and R is the noise with a zero mean. Let X have n records or objects, which are considered as realizations of n independent identically distributed (i.i.d.) random variables or i.i.d. random vectors (when there are multiple attributes). Let R have the same size of data values as X. They are the realizations of n independent random variables or random vectors, drawn from a certain distribution.  
42	 4.1 Using Noise Distribution  
43	 This is a naive guessing method: for each disguised data item y, the adversaries always use y as its guess of the original, i.e., the adversaries always guess the value of the random noise to be zero. We call this method the Noise Distributionbased Reconstruction (NDR).  
44	 Let y i = x i + r i for i = 1, . . . , n, where x i , y i , r i are samples of X, Y , and R, respectively. The mean square error (m.s.e.) of the NDR scheme can be derived in the following:  
45	 m.s.e. = 1  
46	 n n  
47	 i=1 (y i - x i ) 2 = 1  
48	 n n  
49	 i=1 r 2 i = 1  
50	 n n  
51	 i=1 (r i - 0) 2  
52	 From the above equation, the m.s.e. of NDR is exactly the variance of the random numbers. When the random numbers have a large variance, the reconstruction accuracy of NDR is low.  
53	 4.2 Using Univariate Distribution  
54	 NDR scheme is not good for reconstructing the original data. It does not consider the distribution of X and Y , which can be helpful for data reconstruction.  
55	 In this subsection, we show how to reconstruct the original data for each attribute based on some distributions. Since we treat each attribute of the data set independently, we call this method the Univariate Distribution-based Reconstruction (UDR).  
56	 Let f X , f Y , f R represent the distribution of X, Y , and R, respectively. We first derive the posterior distribution P (X|Y ), which gives us the probability for different values of X after having observed the value of Y . Since our goal is to reconstruct the original data, we need to pick a value that can minimize the overall mean square error. Our next theorem indicates that picking the expected value of the distribution achieves the minimum mean square error:  
57	 Theorem 4.1. Given a distribution f (x), let ¯ x be the expected value of x. Let z be a constant. The mean square error e =  (x - z) 2 f (x)dx is minimized when z = ¯ x.  
58	 Proof. If we want to find what value of z makes the e =  (x - z) 2 f (x)dx minimum, we can differentiate the equation twice on z. Then we find a value which makes the first derivative equal to zero and the second derivative larger than zero. This value indeed minimizes the value of e. The  
59	 39 first derivative is:  
60	   (x - z) 2 f (x)dx  
61	 z = 0  
62	 then  
63	   
64	 2  (x - z)f (x)dx = 0.  
65	 2    
66	 xf (x)dx - 2z    
67	 f (x)dx = 0  
68	   
69	 xf (x)dx - z = 0  
70	 z = ¯ x  
71	 (1)  
72	 The second derivative is:  
73	  2  (x - z) 2 f (x)dx  
74	  2 z = 2 &gt; 0 (2)  
75	 Therefore to minimize mean square errors, z must be the expected value of x.  
76	 Next we will show how to compute the posterior distribution P (X|Y ) and its expected value. To compute P (X|Y ), we need to know the distributions f X , f Y , and f R . R's distribution f R is public. Y 's distribution f Y can be estimated from the samples, i.e., the disguised data set. X's distribution f X is unknown; however, it has been shown by the studies in the privacy preserving data mining area that f X can be estimated from the disguised data [2]. Therefore, in our next analysis, we assume all three distributions f X , f Y , and f R are known. We have the following:  
77	 P (x|Y = y) = f (y|x)f X (x)  
78	 f Y (y) = f R (y - x)f X (x)  
79	 f Y (y) . (3)  
80	 Therefore the expected value of X given the disguised value Y = y is the following:  
81	 E(x|Y = y) =   
82	 x f X (x)f R (y - x)  
83	 f Y (y) dx  
84	 =  xf X (x)f R (y - x)dx  
85	 f Y (y) . (4)  
86	 We thus use E(x|Y = y) as our guess to reconstruct the original data.  
87	 It should be noted that UDR only considers the distribution of one dimension; it does not use any correlation between different attributes. If the attributes are highly correlated, the use of the correlations will greatly help the adversaries' estimations. In the following sections, we will study how to take advantage of the correlations among the attributes.  
88	 5. PCA-BASED DATA RECONSTRUCTION  
89	 In this section, we will present a different estimation technique which is based on PCA (principal component analysis). We called this technique PCA-Based Data Reconstruction (PCA-DR). To help readers understand PCA-DR, we briefly describe how PCA works. 5.1 Principal Components Analysis  
90	 Principal Component Analysis (PCA)[14] is a way to reduce the dimensionality of a data set with interrelated variables, but still contain as much variance of the data set as possible. If a data set has m variables, each of which has n implementations, PCA can transform the data set to a new data set with p  m variables, which are uncorrelated and are ordered by the variances they contain. We usually say there is a strong trend along a direction if the variance in the direction is large.  
91	 Let D be a data set of n records of m variables (also called attributes). It can be viewed as a transposed vector of m variables. Let us start to search for the first principal component (PC), which presents the largest variance of the original data set in the direction of a certain vector. We look for a linear function De 1 of the variables of D which has maximum variance, where e 1 is a vector of m constants. To get the second PC. we look for a linear function De 2 , uncorrelated with De 1 , and having maximum variance. Accordingly, a linear function De p , uncorrelated with De 1 , ..., De p-1 , is found which has maximum variance. The result vectors De 1 , De 2 , ..., De p are called principal components (PCs). Since the value of p is always smaller than or equal to m, PCA is used for compression. The variances in the directions of the vectors decrease from De 1 to De p .  
92	 If some variables have significant correlations among them, the first few generated PCs will count most of the variances in D. Accordingly, the subsequently-generated PCs will count a smaller portion of the variances of D.  
93	 In order to find PCs, the covariance matrix C is computed. This is the matrix whose (i, j)-th entry is the covariance between the ith and jth variables of D (when i = j, it is the variance of the ith attribute of D). Then, e k is an eigenvector of C corresponding to its kth largest eigenvalue  k . The kth PC is De k , the variance of which is equal to  k . We briefly introduce the procedures of decreasing the number of the data dimensions and of restoring data below.  
94	 5.1.1 Decreasing the Dimensionality  
95	 Let the original data set be D as before. The mean of each attribute of the data set is 0 due to the requirement of PCA. A non-0-mean data set can always be adjusted to a 0-mean data set by subtracting the mean of each attribute from it. Then all operations can be executed on the adjusted data set. When the operations are done and restoring the data set is wanted, the mean will be added back. For the simplicity of presentation, we ignore the adjustment steps and consider all data sets we use here are 0-mean data sets.  
96	 From D, its covariance matrix C can be computed. Using C, eigenvectors [e 1 , e 2 , ..., e m ] can then be obtained, so is their corresponding eigenvalues:  1 , ...,  m , where  1 &gt;=  2 &gt;= ... &gt;=  m .  
97	 Assume that the data have large variances along the directions of the first p eigenvectors, and small variances along the directions of the other m - p eigenvectors. Let E = [e 1 , e 2 , ..., e p ] be a matrix of size m  p. The following equation describes the transformation that can reduce dimensions.  
98	 D n = DE, (5)  
99	 where D n is a new data matrix of size n  p. Thus, the number of the dimensions of the data set decreases from m  
100	 40 to p.  
101	 5.1.2 Restoring the Original Data  
102	 Next we restore the original data from the new data. If p = m, E is orthogonal, meaning that its transpose is its inverse. We have:  
103	 D = D n (E) -1 = D n E T  
104	 If E is only composed of p (&lt; m) eigenvector, the above equation is only an approximation and becomes  
105	 ^ D = D n E T (6)  
106	 where ^ D is the estimated original data set.  
107	 When we use reduced E (e.g. the first p eigenvectors), the restored data will not reflect the variances along the directions of the other m - p eigenvectors. However, the restored data still contains most information of the data set because the variances in the directions of the principal eigenvectors are maintained. We call an eigenvector "principal" if it is used to calculate the principal components. We call an eigenvalue "principal" when it is corresponding to a principal eigenvector.  
108	 5.2 PCA-Based Data Reconstruction  
109	 Existing methods have not exploited the correlations when quantifying privacy. We believe that the correlations can help the adversaries make more accurate guess. For example, let A i be an attribute in the data set. If several attributes are highly correlated with A i , then we have redundant information about A i . We should be able to estimate A i with better accuracy based on this information.  
110	 When a data set has strong correlations among its data, the data set has large variances in the directions of some vectors but small variances in the other directions. The addition of noise does not change the trends too much, because if the random numbers added to the original data are independent, their variances will be evenly distributed among all the directions.  
111	 Principal Component Analysis is a technique that identifies those trends. Let us consider the information loss when we only select p principals out of the total m. There will be information loss. All the variance along the other m - p directions will be lost. However, when the data are highly correlated, the variances along the first p directions are much larger than the variances along the rest m - p directions. Thus removing those m - p directions during the transformation does not cause much information loss.  
112	 The information loss for the random numbers is different. In randomization scheme, random numbers are independent for each attribute. Their correlations are zero. Therefore, their variance will be evenly distributed to those m directions. If we remove m - p directions in the PCA transformation, we are able to remove m-p m portion of the random numbers' variance. The more variance of random numbers we remove, the better.  
113	 Therefore, if the data is highly correlated, then more dimensions can be reduced without causing too much information loss for the original data; at the same time, the information loss for the noise increases. Intuitively speaking, during the PCA transformation using the first p &lt; m principals, we can filter out a portion of the random numbers.  
114	 Based on the above observation, we present a PCA-based data reconstruction scheme. Since PCA introduces information loss, it is important to understand how much of the original information is lost and how much of the noise is lost. Using the PCA-based scheme proposed by Kargupta et al. [16], the information loss and noise loss cannot be clearly quantified, because of matrix perturbation theory also used in that scheme complicates the analysis. Therefore, to be able to understand how correlation affect the privacy of the randomization scheme, we choose to use PCA directly to reconstruct the data. Namely we reconstruct the original eigenvalues and eigenvectors, and then based on the original eigenvalues, we select the principal components. Since the original eigenvalues reflect the degree of correlations among data attributes, the number of principal components and the sum of their variances indicate how much of the original information is kept via PCA.  
115	 5.2.1 Estimating Covariance Matrix  
116	 To conduct PCA, we need to know the covariance matrix for the original data. How do we get the covariance matrix for the original data? The following theorem provides the answer.  
117	 Theorem 5.1. Let X i and X j represent two variables in the data set. Cov(X i + R i , X j + R j ) represents the (i, j)th entry of the covariance matrix from the disguised data set, where R i and R j are random variables with zero means, and they are independent from X i or X j . Cov(X i , X j ) represents the (i, j)-th entry of the covariance matrix from the original data set. Let  be the standard deviation of R i . We have the following relationship:  
118	 Cov(X i + R i , X j + R j )  
119	 = Cov(X i , X j ) +  2 , for i = j  
120	 Cov(X i , X j ), for i = j  
121	 Proof. Based on the definition of the covariance, we have the following equations:  
122	 Cov(X i + R i , X j + R j )  
123	 = E((X i + R i )(X j + R j )) - E(X i + R i )E(X j + R j )  
124	 = E(X i X j ) + E(R i )E(X j ) + E(X i )E(R j )  
125	 +E(R i R j ) - E(X i )E(X j )  
126	 = E(X i Y i ) + 0 + 0 + E(R i R j ) - E(X i )E(X j )  
127	 = Cov(X i , X j ) + E(R i R j ).  
128	 When i = j, R i and R j are independent, so E(R i R j ) = E(R i )E(R j ) = 0; when i = j, E(R i R i ) = E((R i -0) 2 ) =  2 , where  is the standard deviation of the random variables R i . Combining this with the above equation, we have proved the theorem.  
129	 When the sample size is large enough, the relationship presented in the above theorem carries over to the sample covariance matrices (PCA is applied to sample covariance matrices). The above theorem indicates that we can derive the covariance matrix of the original data based on the disguised data. All we need to do is to subtract  2 from the diagonal elements of the covariance matrix that is derived from the disguised data. Although the derived matrix is only an approximation, when the number of samples becomes larger, the approximation becomes more accurate.  
130	 41 5.2.2 Applying PCA  
131	 After getting the approximated sample covariance matrix for the real data, we can use the following PCA-based method to reconstruct the original data (recall that Y represents the disguised data. We use C to represent the sample covariance matrix derived from Y ):  
132	 1. Conduct PCA to get C = QQ T , where  is a diagonal matrix consisting of eigenvalues, and Q a matrix formed by eigenvectors.  
133	 2. Let p be the number of principal components to be selected. Set ^ Q to be the first p columns of Q. 1  
134	 3. Reconstruct the data using ^ X = Y ^ Q ^ Q T .  
135	 Let m be the number of attributes in the data set. If p = m (i.e., we do not reduce the dimension), the above reconstruction procedure gets back to Y , and nothing is filtered out. When p &lt; m, ^ X will lose information. If we lose the same amount information on both X and R, then such a transformation is not helpful. However, as we have discussed earlier, when the data are highly correlated, we do not lose much information on X; more importantly, we lose information about R, and the amount of information loss with regard to R should, intuitively speaking, be proportional to the ratio of p m . In the next sub-section, we will formally quantify the information loss on R.  
136	 5.3 Analysis  
137	 For the sake of simplicity, we only analyze PCA-DR using covariance matrix from the original data. That is, the covariance matrix is directly obtained from the original data, rather than being estimated from the disguised data. There are only minor differences between the covariance matrices from original data and the estimated one. From the last step of PCA procedure described earlier, we have  
138	 ^ X = Y ^ Q ^ Q T ,  
139	 so we get  
140	 ^ X = (X + R) ^ Q ^ Q T  
141	 = X ^ Q ^ Q T + R ^ Q ^ Q T . (7)  
142	 The error between ^ X and the original data X comes from two sources: one is the error caused by X ^ Q ^ Q T , the other is the error caused by R ^ Q ^ Q T . The former is decided by the correlations among data and the number of principals included in ^ Q. The latter can be quantified by the following theorem:  
143	 Theorem 5.2. Let m be the number of the attributes of the original data set, p be the number of principal components being used in PCA-DR. The variance of the random noise is  2 , and its mean is 0. Let  2 be the mean square error of PCA-DR caused by R ^ Q ^ Q T . We have the following relationship:  
144	 1 There are a number of ways to select principal components. We can fix the number of selected principal components; we can also fix the portion of the original information that we want to keep; we can also choose the dominant eigenvalues by finding the largest gap between the dominant eigenvalues and the non-dominant ones. The last method is used in our experiments.  2 =  2 p  
145	 m . (8)  
146	 Proof. See Appendix A.  
147	 Due to Theorem (5.2), the mean square error that is caused by R in PCA-DR scheme is proportional to the variance of the random numbers and the ratio of p m . This confirms our intuitive explanation. We will also use experiments to verify these relationships.  
148	 6. BAYES-ESTIMATE-BASED DATA RECON 
149	 STRUCTION  
150	 The PCA-based reconstruction works well when the data are highly correlated. However, when the correlations of data are not high enough, the non-principal components will not be many; according to Theorem (5.2), we will not be able to filter out significant amount of the noise. Our results in Section 7 will show that when the correlations become low, the accuracy achieved by the PCA-based scheme is even worse than the univariate data reconstruction scheme.  
151	 In this section, we describe a more accurate data reconstruction method. We want to fully take advantage of the correlationship among data. Unlike the PCA-based schemes, which use this correlationship to filter out noise, We formulate the data reconstruction problem as an estimation problem, i.e., based on the disguised data that we have observed and on the data correlationship that we know, we use a value that can most likely produce such an observation as our reconstructed data. In other words, given the disguised data Y , we search for X, such that the posterior distribution P (X | Y ) is maximized. We then use X as the data reconstructed from the disguised data. This is the idea of the Bayes estimate [20]. We call our scheme the BayesEstimate-based Data Reconstruction (BE-DR).  
152	 To simplify derivation, we assume that the original data have multivariate normal distribution. This assumption is reasonable since this distribution is a good approximate distribution in many situations [13]. Because of the appealing properties of multivariate normal distribution, the calculation of Bayes estimate is simplified to computation of matrices and vectors even though the form of the distribution is more complicated. As we will explain later, this assumption can be relaxed.  
153	 6.1 Data Reconstruction  
154	 Suppose that random noise used for each attribute has normal distribution and it is independent from those used for other attributes. Suppose that an adversary only has the disguised data set and the distribution of the random noise. Let the original data have m attributes and n records. They can be considered as observations of a random vector of length m. Let the random vector of the original data be X. Similarly let the random vector of the noise be R, and the random vector of the disguised data Y . Let f X , f Y ,  
155	 f R represent the distributions of X, Y , R, respectively. The  
156	 distribution of X and R are described in the following:  
157	 f X (x) = 1  
158	 (2) m/2 | x | 1/2 e 1 2 (x-µ x ) T  -1 x (x-µ x )  
159	 f R (r) = 1  
160	 (2) m/2 e 1 2 (r-µ r ) T (r-µ r )/ 2 ,  
161	 42 where  x is the covariance matrix of the original data,  2  
162	 is the variance of the random noise, µ x , µ r are the mean vectors of the original data and the noise data.  
163	 We use the posterior distribution P (X|Y ) to represent the  
164	 probabilities for different values of X given an observation of Y . By using Bayesian rule, we get the following formulae:  
165	 P X,Y (x|Y = y) = f X (x)f Y |X (y|x)  
166	 f Y (y) , (9)  
167	 where f Y |X (y|x) represents the probability of getting y from  
168	 x, which is exactly the probability of the random number (y - x). Therefore, f Y |X (y|x) = f R (y - x).  
169	 We want to find a value of X, such that P X,Y (x|Y = y)  
170	 is maximized. Noticing that the denominator f Y (y) does  
171	 not change when X changes, we only need to consider the numerator. Thus we only need to maximize:  
172	 f X (x)f Y |X (y|x) = f X (x)f R (y - x)  
173	 = 1  
174	 (2) m/2 | x | 1/2 e 1 2 (x-µ x ) T  -1 x (x-µ x ) ·  
175	 1  
176	 (2) m/2 e 1 2 (y-x-µ r ) T (y-x-µ r )/ 2 (10)  
177	 Since the logarithm is a monotone one-to-one function, we could maximize the following function instead:  
178	 1  
179	 2 (x - µ x ) T  -1 x (x - µ x ) 1  
180	 2 (y - x) T (y - x)/ 2 .  
181	 Note that in the above equation the constant terms are ignored because it does not affect computing the maximum estimator of x; the mean vector µ r is ignored too since it is assumed to be zero vector in randomization schemes.  
182	 We let the first derivative of the above equation with respect to x be 0. We get:  
183	  -1 x (x - µ x ) + (x - y)/ 2 = 0.  
184	 After simplifying and rearranging the above equation, we have  
185	 ^ x = ( -1 x + 1/ 2 · I) -1 ( -1 x µ x + y/ 2 ), (11)  
186	 where I is the identity matrix of the same size as  x .  
187	 Equipped with Equation (11), we now describe our BayesEstimate-based data reconstruction scheme:  
188	 1. Derive  x from Theorem (5.1) using disguised data Y .  
189	 2. Derive µ x by computing the mean vector of the disguised data. We know that µ x  µ y because random noises have zero means.  
190	 3. For each y, derive ^ x using Equation (11).  
191	 4. Use ^ x as the reconstructed value.  
192	 The BE-based scheme, the PCA-based scheme, and the univariate data reconstruction scheme have the following relationship: when the correlations among data are low, e.g., data are independent, the results of BE-DR should converge to the univariate data reconstruction. This is because when data are independent, data from one attribute cannot help the reconstruction of another attribute. Thus, the BE-DR scheme is equivalent to the univariate data reconstruction. On the other hand, when the correlations among data become high, the results of BE-DR should be similar to those of PCA-DR, because they both fully exploit the correlationship among data.  
193	 Regarding our assumption on the multivariate normal distribution: although we have only shown the results for data sets that satisfy multi-normal distribution, the approach can be extended to data sets that satisfy other distribution. However, for other distributions, we might not be able to derive an equation with a simple analytic form for its first derivative. In such situations, the Bayes estimate must be sought using numerical methods, such as Gradient descent methods [12, 3]. We will study them in our future work.  
194	 7. EXPERIMENT  
195	 7.1 Methodology  
196	 We have designed a series of experiments to evaluate the PCA-DR scheme and the BE-DR scheme. Our goal is to find out how the correlations among the attributes affect the accuracy of these methods. Data correlations can be affected by a number of parameters, including the ratio of the number of principal components to the number of attributes and the variance of data on the principal and non-principal components. We have designed experiments to study how these parameters affect our schemes. We also compare our results with SF algorithm [16].  
197	 We decided to use synthetic data for our experiments, because it is difficult to find real data sets that bear properties pre-determined for each experiment. Our approach is to generate a covariance matrix first, then generate the synthetic data set based on the covariance matrix, and finally conduct the PCA-DR or BE-DR procedure. However, generating a covariance matrix with pre-determined properties is not a trivial task either. To better control the properties of the matrix, we generated the covariance matrix in a reverse way: we generated the eigenvalues and eigenvectors first, and then we computed the covariance matrix from the eigenvalues and eigenvectors. We can control the properties of covariance matrix by changing eigenvalues. Our procedure is described in the following:  
198	 1. We specify  as a diagonal matrix with the corresponding eigenvalues on its diagonal. The size of  is m by m.  
199	 2. By using Gram-Schmidt orthonormalization process [4], we generate an orthogonal matrix Q of size m by m. We consider each column of Q as an eigenvector.  
200	 3. We let the covariance matrix C = Q ×  × Q T .  
201	 4. We generate a data set based on the covariance matrix. In our experiments, we use mvnrnd from Matlab to generate data from C. The function mvnrnd generates a data set of multivariate normal distribution based on the provided covariance matrix and the mean vector. This resultant data set will be used as the original data set X.  
202	 5. We randomly generate a noise data set using normal distributions. This noise data set is then added to the original data set to form the disguised data set Y .  
203	 43 0 10 20 30 40 50 60 70 80 90 100 2.5 3 3.5 4 4.5 5  
204	 Number of Attributes Root Mean Square Error UDR Scheme SF Scheme PCA-DR Scheme BE-DR Scheme  
205	 Figure 1: Experiment 1: Increase the Number of Attributes  
206	 Benchmark. The objective of this paper is to show how much the correlations among the attributes can help disclose private information. Therefore, we compare our results with the data reconstruction method that does not consider correlations. UDR (Univariate Data Reconstruction) described in Section 4 is such a data reconstruction scheme. We use its results as our baseline comparison. The difference between our PCA-DR, BE-DR results and the UDR results indicates how much the correlations can help disclose private information.  
207	 7.2 Experiment 1: Increasing the Size of the  
208	 Covariance Matrix  
209	 In this experiment, we change the correlations among data by increasing the number of attributes while fixing the number of principal components. We first fixed the number of principal components to p by letting the first p eigenvalues in  to be , and the other m-p eigenvalues to be relatively small numbers. p is 5 in the experiment. We then increased the size of  from p to m.  
210	 Since UDR is used as the benchmark, we would like to keep its result a constant when we generate different data sets for this experiment. However, we found that this was not an easy task. Since the mean square error of UDR only depends on the standard deviation of the original data (we have already fixed the distribution of the disguising noises), to keep it the same, we should keep the average standard deviation of each attribute the same. Due to the following property of eigenvalues, by selecting the eigenvalues that satisfy this property, we can keep the results of UDR almost the same:  
211	 m  
212	 i=1  i = m  
213	 i=1 a ii , (12)  
214	 where a ii is the diagonal element of the covariance matrix, i.e., it is the variance of the ith attribute.  
215	 Once the data is generated, we use the UDR, PCA-DR, BE-DR and SF schemes to reconstruct the original data from the disguised data, and measure the mean square errors between the reconstructed data and the original data. The results are depicted in Figure 1. 0 10 20 30 40 50 60 70 80 90 100 2.5 3 3.5 4 4.5 5  
216	 Number of Principal Components Root Mean Square Error UDR Scheme SF Scheme PCA-DR Scheme BE-DR Scheme  
217	 Figure 2: Experiment 2: Increase the Number of Principal Components  
218	 The experimental results clearly show that all the correlationbased reconstruction schemes (SF, PCA-DR, and BE-DR) have lower reconstruction errors when the number of attributes increase. Since the number of principal components in this experiment is fixed, the larger the number of the attributes, the higher the correlations, and as this experiment indicates, the more accurately one can reconstruct the original data. We also see that UDR scheme is not sensitive to the change of correlations because it does not exploit such a relationship. The performance of UDR is much worse compared to the other schemes when the data correlations are high.  
219	 We also observed that SF scheme does not perform as well as the PCA-DR scheme in this experiment. However, this comparison result does not always hold. The key difference of the SF scheme and the PCA-DR scheme is how to separate the principal components and the non-principal components. SF scheme uses the bounds derived from the matrix perturbation theory to identify and separate the principal components from the non-principal components. Our experiments show that when the eigenvalues on the non-principal components are not very small, the derived bounds tend not to be quite accurate. That might be the cause of SF's worse performance. In our experiment 3, we will show that when the eigenvalues on the non-principal components are small, the performance of SF and that of PCA-DR are indeed close.  
220	 Most importantly, the experiment shows that BE-DR achieves better performance than PCA-DR and SF schemes. This result is consistent throughout all our experiments. The reason is that the BE-DR method utilizes all the information, including the non-principal components, while the PCAbased schemes discard the non-principal components.  
221	 7.3 Experiment 2: Increasing the number of  
222	 principal components  
223	 In this experiment, we change the correlations among data from another angle: instead of changing the value of m as in the previous experiment, we change the value of p, the number of principal components. When p increases, the number of principal components increases, thus the correlations among data decrease.  
224	 We fixed the size of the covariance matrix to be 100  100,  
225	 44 0 5 10 15 20 25 30 35 40 45 50 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5 7  
226	 Eigenvalues of the Non-Principal Components Root Mean Square Error UDR Scheme SF Scheme PCA-DR Scheme BE-DR Scheme  
227	 Figure 3: Experiment 3: Increase the Eigenvalues of the non-Principal Components  
228	 i.e., there were 100 attributes. We let the first p eigenvalues in matrix  be , and the rest m - p eigenvalues be very small numbers compared with . This way, the number of principal components is p. In the experiment, we increase the value of p from 2 to 100. We use the same method as the previous experiment to maintain the result of UDR method a constant.  
229	 The results of this experiment are depicted in Figure 2. The results show that SF, PCA-DR and BE-DR achieve better accuracy when the number of principal components becomes less, i.e., the correlations become higher.  
230	 For PCA-based schemes, intuitively speaking, when the number of principal components increases, the correlations among the attributes become weaker. Thus we need to keep more directions in the PCA-based data compression. As a result, more noises are kept because they are evenly distributed along all directions; therefore the accuracy of the data reconstruction decreases.  
231	 For the same reason as the previous experiment, the BEDR scheme demonstrates a better performance than the PCA-DR schemes.  
232	 7.4 Experiment 3: Increasing the Eigenvalues  
233	 of non-principal Components  
234	 Eigenvalues of non-principal components represent the discarded variances on those non-principal directions. They can affect the reconstruction results of PCA-based schemes and BE-based scheme. In this experiment, we examine how the eigenvalues influence the estimation results of PCA-DR and BE-DR.  
235	 We fixed the size of  to 100  100. We let the first 20 eigenvalues be  and the other 80 eigenvalues be variables. Here we let them change from 1 to 50, which is still smaller than (=400 in our experiment).  
236	 The results are depicted in Figure 3. From the results, changing the eigenvalues of the non-principal components does not affect UDR, but it does significantly affect SF, PCA-DR and BE-DR. When the eigenvalues become larger or the correlations of the original data are low, the accuracy of SF, PCA-DR and BE-DR all become worse. Because SF and PCA-DR discard the information along the nonprincipal eigenvectors, when the eigenvalues of those eigenvectors become larger, more information of the original data is discarded; thus the estimation errors become higher. After certain points, the original information is discarded so much that the errors of SF and PCA-DR schemes are even higher than UDR.  
237	 BE-DR scheme does not have the above drawbacks. As demonstrated by the figure, the performance of BE-DR converges to the performance of UDR when the correlations of data are low. Note that UDR also "implicitly" uses the Bayes estimate principle, but instead on single attribute. When data correlations are low, the data become more and more independent; thus multivariate Baye estimate reconstruction becomes equivalent to univariate data reconstruction because no correlation among attributes can be exploited.  
238	 8. IMPROVING PRIVACY PRESERVATION  
239	 OF DATA RANDOMIZATION  
240	 8.1 An Improved Randomization Scheme  
241	 From the analysis of PCA, we know that the variances of the random noises are evenly distributed among both principal and non-principal components, while most of the information of the original data concentrates on the principal components. Therefore, when we discard those nonprincipal components, we can remove (or filter) far more noises than what we do to the original data. However, if random noises also concentrate on the principal components, separating original data from random noises becomes difficult.  
242	 Motivated by the above observation, we propose to use correlated random noises to disguise original data. In particular, we let the correlations of the random noises similar to the correlations of the original data. For example, when the original data have a multivariate normal distribution with covariance matrix C, we generate the random noises using the same covariance matrix. This guarantees that noises also concentrate on the principal components (of the original data).  
243	 Under the new randomization scheme, the PCA-based data reconstruction scheme is still the same, but the formula in Equation (11) for BE-based scheme needs to be modified. We have the following formula for BE-DR.  
244	 Theorem 8.1. Let the covariance matrix of the original data be  x , and the covariance matrix of the random noises be  r . Let the mean vectors of original data and noise data be µ x and µ r , respectively. Let the disguised data vector y. Then the Bayes estimate of the original data vector is  
245	 ^ x = ( -1 x +  -1 r ) -1 ( -1 x µ x -  -1 r µ r +  -1 r y). (13)  
246	 Proof. Similar to the derivation of Equation (11)  
247	 Because the original data cannot be directly observed, we do not have the covariance matrix  x . However, we can estimate  x using the disguised data Y and the covariance matrix of the random noise. The following theorem describes how to derive  x :  
248	 Theorem 8.2. Let the covariance matrix of the original data, random noise, and the disguised data be  x ,  r , and  y , respectively. We have the following relationship:  
249	  y =  x +  r . (14)  
250	 45 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 2 2.5 3 3.5 4 4.5 5 5.5  
251	 Correlation Dissimilarity Root Mean Square Error SF Scheme PCA-DR Scheme Improved BE-DR Scheme  
252	 Noise is     independent   
253	 Figure 4: Experiment 4: Increasing the correlation dissimilarity of the original data and random noise.  
254	 Proof. Similar to the proof of Theorem 5.1  
255	 There are two important aspects of privacy preserving data mining. One is to preserve the original data's privacy, the other is to do the data mining. Therefore, regardless of how the random noises are generated, the disguised data set should be useful for data mining. Equation (14) indicates that when the data are multivariate normal, we can reconstruct the probability distribution of the original data from the disguised data set. This distribution is sufficient for many data mining computations. Actually, the reason why randomization can be used in achieving privacypreserving data mining is that those supported data mining operations are based on aggregate information (e.g. distribution). Therefore, our improved randomization scheme can still be used in many privacy-preserving data mining computations.  
256	 8.2 Experiment Results  
257	 To understand how the correlations among random noises affect the data reconstruction methods, we use random noises with different levels of "similarity" compared to the covariance matrix  x of the original data. We hypothesize that the privacy is better protected when the noises are more "similar" to  x . We use the following correlation dissimilarity metric to quantify the difference between the correlations among random noises and the correlations among the original data.  
258	 Definition 8.1. (Correlation Dissimilarity) Let two data sets be X, R with the same size of n by m. Let the matrix of correlation coefficients for X be C X . Let the matrix of correlation coefficients for R be C R . The correlation dissimilarity between X and R is defined as:  
259	 Dis(X, R) = 1  
260	 m 2 - m m  
261	 i=1 m  
262	 j=1,j=i (C X (i, j) - C R (i, j)) 2 ,  
263	 where (i, j) means an element in the ith row and jth column. Note that, we do not consider the diagonal elements of the matrices of correlation coefficients because they are always 1 and should not be counted for the dissimilarity.  
264	 Similar to the experiments in Section 7, we generate an original data set with 100 attributes, where the first 50 eigenvalues have large numbers and the others have small numbers. To make it convenient to control the degree of correlation dissimilarity between the noises and the original data, we generate random noises based on these eigenvalues and the corresponding eigenvectors of the original data. More specifically, we fix the eigenvectors of the noises to be the same as those of the original data, and we then change the values of the eigenvalues. As a result, we will get a new covariance matrix, based on which we can generate random noises. It should be noted that there are many other ways to generate random noises with different correlation dissimilarities.  
265	 Our experiment results are depicted in Figure 4. The figure indicates that when the correlations of the random noises are almost the same as that of the original data, data reconstruction has the highest error, i.e., the privacy is best preserved. When the dissimilarity increases, the privacy preservation becomes worse (with the SF algorithm being the exception). The vertical line we draw in the middle of the figure represents the situation where the random noises are not correlated (i.e., the random noises of different attributes are independent). Therefore, the curves on the left side of the vertical line show how much privacy we can gain by using correlated random noises as opposed to independent noises.  
266	 On the right hand of the vertical line, noises are still correlated, but they are correlated very differently compared to the original data X. Actually, based on the ways we generate those noises, the noises on the right side of the vertical line start to concentrate on the non-principal components, whereas the noises on the left side of the line concentrate on the principal components. Our results have shown that the accuracy of PCA-DR and BE-DR continues to increase. This is because the amount of noises used to disguised the principal components of the original data becomes less.  
267	 We noticed that the SF algorithm behaves differently after the point of the vertical line. The main reason for this is that the filtering bounds of the SF are derived from independent random noises. It may not optimally filter the noises when noises are correlated.  
268	 9. CONCLUSION  
269	 In this paper, we studied how correlations affect the privacy of a data set disguised via the random perturbation scheme. We presented two methods to reconstruct original data from a disguised data set. One scheme is based on PCA (Principal Component Analysis), and the other scheme is based on the Bayes estimate. Using PCA concepts, we give an intuitive and theoretical explanation on how correlation can affect the data privacy for a randomized data set. Our results have shown that both the PCA-based schemes and the BE-based scheme can reconstruct more accurate data when the correlation of data increases. Our results have also shown that the BE-based scheme is always better than the PCA-based schemes.  
270	 To defeat the data reconstruction methods that exploit the data correlation, we proposed a modified random perturbation, in which the random noises are correlated. Our experiments show that the more the correlation of noises resembles that of the original data, the better privacy preservation can be achieved.  
271	 46  
272	 Our future work will study how information other than correlation can affect privacy. For example, we will study how partial knowledge of a disguised data set can compromise privacy.  
273	 10. ACKNOWLEDGMENT  
274	 The authors acknowledge supports from the United States National Science Foundation IIS-0219560 and IIS-0312366. The authors would also like to thank all the anonymous reviewers for their valuable comments.  
275	 11. REFERENCES [1] D. Agrawal and C. Aggarwal. On the design and quantification of privacy preserving data mining algorithms. In Proccedings of the 20th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, Santa Barbara, California, USA, May 21-23 2001.  
276	 [2] R. Agrawal and R. Srikant. Privacy-preserving data mining. In Proceedings of the 2000 ACM SIGMOD on Management of Data, pages 439­450, Dallas, TX USA, May 15 - 18 2000.  
277	 [3] R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. V. der Vorst. Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. SIAM, Philadelphia, PA, 1994.  
278	 [4] R. Bronson. Linear Algebra,An Introduction. Academic Press, 1991.  
279	 [5] C. Clifton, M. Kantarcioglu, J. Vaidya, X. Lin, and M. Y. Zhu. Tools for privacy preserving data mining. SIGKDD Explorations, 4(2), December 2002.  
280	 [6] W. Du, Y. S. Han, and S. Chen. Privacy-preserving multivariate statistical analysis: Linear regression and classification. In Proceedings of the 4th SIAM International Conference on Data Mining, Lake Buena Vista, Florida, USA, April 2004.  
281	 [7] W. Du and Z. Zhan. Using randomized response techniques for privacy-preserving data mining. In Proceedings of The 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 24-27 2003.  
282	 [8] A. Evfimievski, J. E. Gehrke, and R. Srikant. Limiting privacy breaches in privacy preserving data mining. In Proceedings of the 22nd ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS 2003), San Diego, CA, June 2003.  
283	 [9] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. In Proceedings of 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Edmonton, Alberta, Canada, July 2002.  
284	 [10] B. Gilburd, A. Schuster, and R. Wolff. A new privacy model and association-rule mining algorithm for large-scale distributed environments. In 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), Seattle, WA, USA, August 2004.  
285	 [11] S. Goldwasser. Multi-party computations: Past and present. In Proceedings of the 16th Annual ACM Symposium on Principles of Distributed Computing, Santa Barbara, CA USA, August 21-24 1997.  
286	 [12] R. Hamming. Numerical Methods for Scientists and Engineers. Dover Pubns, 2nd edition, 1987.  
287	 [13] W. Hardle and L. Simar. Applied Multivariate Statistical Analysis. Springer-Verlag, 2003.  
288	 [14] I.T.Jolliffe. Principal Component Analysis. Springer-Verlag, 1986.  
289	 [15] M. Kantarcioglu, J. Jin, and C. Clifton. When do data mining results violate privacy? In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2004), Seattle, WA, USA, August 2004.  
290	 [16] H. Kargupta, S. Datta, Q. Wang, and K. Sivakumar. On the privacy preserving properties of random data perturbation techniques. In the IEEE International Conference on Data Mining, Melbourne, Florida, November 19 - 22, 2003 2003.  
291	 [17] Y. Lindell and B. Pinkas. Privacy preserving data mining. In Advances in Cryptology - Crypto2000, Lecture Notes in Computer Science, volume 1880, 2000.  
292	 [18] D. Meng, K. Sivakumar, and H. Kargupta. Privacy sensitive bayesian network parameter learning. In The Fourth IEEE International Conference on Data Mining(ICDM), Brighton, UK, November 2004.  
293	 [19] B. Pinkas. Cryptographic techniques for privacy-preserving data mining. SIGKDD Explorations, 4(2), December 2002.  
294	 [20] H. V. Poor. An Introduction to Signal Detection and Estimation. Springer-Verlag, New York, 1994.  
295	 [21] S. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. In Proceedings of the 28th VLDB Conference, Hong Kong, China, 2002.  
296	 [22] A. Sanil, A. Karr, X. Lin, and J. Reiter. Privacy preserving regression modelling via distributed computation. In 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), Seattle, WA, USA, August 2004.  
297	 [23] J. Vaidya and C. Clifton. Privacy preserving association rule mining in vertically partitioned data. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 23-26 2002.  
298	 [24] J. Vaidya and C. Clifton. Privacy-preserving k-means clustering over vertically partitioned data. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Ddata Mining, 2003.  
299	 [25] K. Wang, P. Yu, and S. Chakraborty. Bottom-up generalization: A data mining solution to privacy protection. In The Fourth IEEE International Conference on Data Mining(ICDM), Brighton, UK, November 2004.  
300	 [26] S. L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. The American Statistical Association, 60(309):63­69, March 1965.  
301	 [27] R. Wright and Z. Yang. Privacy-preserving bayesian network structure computation on distributed heterogeneous data. In 10th ACM SIGKDD  
302	 47 International Conference on Knowledge Discovery and Data Mining (KDD), Seattle, WA, USA, August 2004.  
303	 APPENDIX  
304	 A. PROOF OF THEOREM 5.2  
305	 Suppose that the random data is a matrix R of size n  m, and the eigenvectors of the covariance matrix of the original data set is Q:  
306	 R =   
307	   r 11 . . . r 1m .. . . .. .. . r n1 . . . r nm   
308	   , Q =   
309	   e 11 . . . e 1m .. . . .. .. . e m1 . . . e mm   
310	    
311	 where n is the number of records and m is the number of attributes. Let the first p eigenvectors of Q be principal and form another matrix ^ Q. Before calculating R ^ Q ^ Q T , we compute the mean square of RC, where C is an m × m matrix:  
312	 C =   
313	   c 11 . . . c 1m .. . . .. .. . c m1 . . . c mm   
314	   ,  
315	 then RC is:  
316	 RC =   
317	   m i=1 r 1i c i1 . . . m i=1 r 1i c im .. . . .. .. . m i=1 r ni c i1 . . . m i=1 r ni c im   
318	   .  
319	 The mean square of the first column of RC is:  
320	 1  
321	 n n  
322	 j=1 ( m  
323	 i=1 r ji c i1 ) 2 = 1  
324	 n m  
325	 i=1 m  
326	 k=1 ( n  
327	 j=1 r ji r jk c i1 c k1 )  
328	 Then we investigate the term in the parenthesis. If i = k, the term is:  
329	 ( 1  
330	 n n  
331	 j=1 r jt r jt )c t1 c t1  
332	 where t is any integer from 1 to m. If n is large enough, the term becomes:  
333	 ( 2 )c t1 c t1 .  
334	 If i = j, the term is:  
335	 ( 1  
336	 n n  
337	 j=1 r js r jt )c s1 c t1 ,  
338	 where, s, t are any integer from 1 to m. If considering the sth and tth column of the random number to be the variables R s and R t , E(R s R t ) = E(R s )E(R t ) = 0. When n is large enough, 1 n n j=1 r js r jt = 0. Therefore the above term when s = t is 0 when n is large enough.  
339	 Then the mean square of the first column is:  
340	  2 m  
341	 t=1 c 2 t1 . (15)  
342	 Similarly, we have the mean square errors of all columns. We have:  
343	  2 m  
344	 t=1 c 2 tk , k = 1, ..., m. (16) The the mean square of all columns is  
345	  2  
346	 m m  
347	 k=1 m  
348	 t=1 c 2 tk . (17)  
349	 Then we know that the mean square of RC is the variance of the random numbers times the summation of the square of all elements of C divided by m.  
350	 Let us calculate ^ Q ^ Q T .  
351	 ^ Q ^ Q T  
352	 =   
353	   e 11 . . . e 1p .. . . .. .. . e m1 . . . e mp   
354	     
355	   e 11 . . . e m1 .. . . .. .. . e 1p . . . e mp   
356	    
357	 =   
358	   p i=1 e 1i e 1i . . . p i=1 e 1i e mi .. . . .. .. . p i=1 e mi e 1i . . . p i=1 e mi e mi   
359	   (18)  
360	 From the previous description, the mean square of R ^ Q ^ Q T is  
361	  2  
362	 m m  
363	 j=1 m  
364	 k=1 ( p  
365	 i=1 e ji e ki ) 2 =  2  
366	 m p  
367	 t=1 p  
368	 i=1 ( m  
369	 j=1 m  
370	 k=1 e ji e ki e jt e kt )  
371	 We investigate the terms in the parenthesis. If i = t,  
372	 m  
373	 j=1 m  
374	 k=1 e ji e ki e jt e kt = m  
375	 j=1 e 2 js m  
376	 k=1 e 2 ks , s = 1, ..., p  
377	 = 1(Q is orthogonal).  
378	 If i = t,  
379	 m  
380	 j=1 m  
381	 k=1 e ji e ki e jt e kt = m  
382	 j=1 e ji e jt m  
383	 k=1 e ki e kt  
384	 = 0(Q is orthogonal).  
385	 Thus, the mean square of R ^ Q ^ Q T is  2 p m . Then the mean  
386	 square error of PCA-DR caused by R ^ Q ^ Q T is:  
387	  2 =  2 p  
388	 m (19)  
