0	 Conceptual Partitioning: An Efficient Method for Continuous   
1	 Nearest Neighbor Monitoring   
2	 Kyriakos Mouratidis   Marios  Hadjieleftheriou §  Dimitris  Papadias      Department of Computer Science  Hong Kong University of Science and Technology  Clear Water Bay, Hong Kong  {kyriakos, dimitris}@cs.ust.hk   §   Department of Computer Science  Boston University  Boston, MA, USA  marioh@cs.bu.edu    
3	 A  
4	 BSTRACT    
5	 Given a set of objects P</i> and a query point <i>q</i>, a <i>k nearest neighbor  (k</i>-NN) query retrieves the <i>k</i> objects in <i>P</i> that lie closest to <i>q. Even  though the problem is well-studied for static datasets, the  traditional methods do not extend to highly dynamic  environments where multiple continuous queries require real-time  results, and both objects and queries receive frequent location  updates. In this paper we propose conceptual partitioning (CPM),  a comprehensive technique for the efficient monitoring of  continuous NN queries. CPM achieves low running time by  handling location updates only from objects that fall in the  vicinity of some query (and ignoring the rest).  It can be used with  multiple, static or moving queries, and it does not make any  assumptions about the object moving patterns. We analyze the  performance of CPM and show that it outperforms the current  state-of-the-art algorithms for all problem settings. Finally, we  extend our framework to aggregate NN (ANN) queries, which  monitor the data objects that minimize the aggregate distance with  respect to a set of query points (e.g., the objects with the  minimum sum of distances to all query points).         
6	 1.   I NTRODUCTION    
7	 Early work in spatial databases focused on the point k</i>-NN<i> query  that retrieves the k  ( 1) objects from a static dataset that are  closest (according to Euclidean distance) to a static query point.  The existing algorithms (e.g., [H84, RKV95, HS99]) consider that  the data are indexed with a spatial access method and utilize some  pruning bounds to restrict the search space. In addition, several  papers study variations of NN search such as reverse NNs  [SRAA01] and constrained NNs [FSAA01]. Recently, the focus  has shifted towards moving NN queries and/or objects in clientserver architectures. Song and Roussopoulos [SR01] reduce the  number of moving NN queries over static objects by introducing  some redundancy. In particular, when a k-NN query is processed,  the server sends to the client a number m</i> &gt; <i>k</i> of neighbors. The <i>k  nearest neighbors at a new location q' </i>will be among the <i>m objects  of the first query q</i> provided that the distance between <i>q</i> and <i>q' is  within a range determined by k</i> and <i>m. For the same settings  (moving query - static data objects), Zhang et al. [ZZP+03]  propose the concept of location-based queries that return the NN  of  q</i> along with its <i>Voronoi cell, i.e., the area around the query  point where the NN set remains the same. The Voronoi cell is  computed on-the-fly using an R-tree on the data objects. Given  clients and data objects that move with linear and known  velocities,  time-parameterized [TP03] queries report, in addition  to the current NN set, its validity period and the next change of  the result (that will occur at the end of the validity period). Linear  NN [BJKS02, TP03] queries return all NN sets up to a future  timestamp  q t  assuming that there are no updates of the velocity  vectors between the current time and q t .    
8	 All the above techniques target the efficient processing of a single  snapshot query since they report the NN set at the query time,  possibly with some validity information (e.g., expiry time,  Voronoi cell), or generate future results based on predictive  features (e.g., velocity vectors of queries or data objects). On the  other hand, continuous monitoring: (i) involves multiple longrunning queries (from geographically distributed clients), (ii) is  concerned with both computing and keeping the results up to  date, (iii) usually assumes main-memory processing to cope with  the intensive (object or query) location updates, (iv) attempts to  minimize factors such as the CPU or communication cost (as  opposed to I/O overhead). Continuous monitoring of spatial  queries is becoming increasingly important due to the wide  availability of inexpensive and compact positioning devices, the  evolution of mobile communications and the need for improved  location-based services. Consequently, several techniques  (reviewed in Section 2) have been developed in the last few years  for continuous range and NN queries.    
9	 In this paper, we propose the conceptual partitioning monitoring  (CPM) method for NN queries in highly dynamic environments.  The data objects are indexed by a main-memory grid G consisting  of cells with size   ×   (assuming two-dimensional space). Each  cell  c in the grid is associated with the list of objects residing  therein. The running queries are stored along with their current  result in a query table QT</i>. When a query <i>q arrives at the system,  its initial result is computed by the NN search module of CPM.  CPM organizes the cells into (hyper) rectangles based on their  proximity to q. This conceptual partitioning provides a natural  processing order of the cells in G, so that the NN search considers  the minimal set of cells in order to retrieve the NNs of q. We refer  to the set of encountered cells as the influence region</i> of <i>q. The  next task of CPM is to monitor the results of the queries upon the  arrival of object updates. Clearly, only updates affecting the  influence region of a query can potentially invalidate its current  result. To restrict processing to such updates and to efficiently  compute the changes in the results, we maintain book-keeping  information in the object index and the query table. We also show  that it is often possible to compute the new result of an affected  query among the objects that issue updates, without searching in     
10	 Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  SIGMOD 2005, June 14­16, 2005, Baltimore, Maryland, USA.  Copyright 2005 ACM 1-59593-060-4/05/06   $5.00.     
11	 G at all. Finally, we tackle the case that the query points also  move. As we show qualitatively and verify experimentally, CPM  outperforms the existing state-of-the-art methods, usually by more  than an order of magnitude.   
12	 Furthermore, CPM provides a general methodology that can be  applied to several types of spatial queries. As a case study we use  aggregate nearest neighbor (ANN) queries. Given a set of query  points  Q </i>= {<i>q 1 ,q 2 ,...,q m } and an object p, the aggregate distance  adist</i>(<i>p</i>,<i>Q</i>) is defined as a monotonically increasing function <i>f over  the individual distances dist</i>(<i>p</i>,<i>q i ) between p</i> and each point <i>q i Q.  Assuming, for example, n</i> users at locations <i>q 1 , ...</i> <i>q n  and f</i>=<i>sum,  an ANN query outputs the data object p</i> that minimizes <i>adist</i>(<i>p</i>,<i>Q)  =  q i Q  dist</i>(<i>p</i>,<i>q i ), i.e., the sum of distances that the users have to  travel in order to meet at the position of p</i>. Similarly, if <i>f</i>=<i>max, the  ANN query reports the object p that minimizes the maximum  distance that any user has to travel to reach p. In turn, this leads to  the earliest time that all users will arrive at the location of p  (assuming that they move with the same speed). The sum ANN  query has been studied in [PSTM04] for static queries and data  indexed by R-trees. The adaptation of CPM to the continuous  monitoring of ANN queries can handle arbitrary aggregate  functions and preserves the excellent performance of the  algorithm in the presence of frequent updates.     
13	 The rest of the paper is organized as follows. Section 2 surveys  related work on continuous monitoring of spatial queries,  focusing mostly on NN search. Section 3 presents the conceptual  partitioning monitoring method. Section 4 provides an analysis of  the space and time requirements of CPM, as well as a qualitative  comparison with existing systems. Section 5 discusses ANN  monitoring, while Section 6 experimentally evaluates CPM.  Finally, Section 7 concludes the paper with directions for future  work.         
14	 2.   R ELATED  W ORK    
15	 The first monitoring method for spatial queries, called Q-index  [PXK+02], assumes static range queries over moving objects. The  queries are indexed by an R-tree and moving objects probe the  index to find the queries that they influence. Q-index avoids the  expensive (due to intensive updates) maintenance of an index on  the objects. In addition, it utilizes the concept of safe regions to  reduce the number of updates. In particular, each object p is  assigned a circular or rectangular region, such that p needs to  issue an update only if it exits this area (otherwise, it does not  influence the result of any query). MQM [CHC04], another range  monitoring method, partitions the workspace into rectangular subdomains. Each object in the system is assigned a resident domain,  consisting of adjacent sub-domains. An object is aware only of  the range queries intersecting its resident region, and reports its  location to the server when it crosses the boundary of any of these  queries. The number of sub-domains that form an object's  resident region depends on how many queries it can store and  process concurrently. When an object exits its resident region, it  requests a new one from the server. To decide the new resident  region, the server uses a binary partitioning tree, which maintains  for each sub-division of the workspace the queries that intersect it.  This method applies only to static ranges.   
16	 To deal with moving range queries, Gedik and Liu [GL04]  propose another distributed system, called Mobieyes</i>.  <i>Mobieyes  partitions the workspace using a grid and maintains the  monitoring regions of the queries. The monitoring region of a  query is defined as the union of the grid cells it can potentially  intersect, provided that its center remains within its current cell.  Objects falling in the monitoring region of a query receive  information about the query position and velocity, and notify the  server when they enter or leave the predicted query region. Note  that this way the objects store locally and monitor their spatial  relationship only with queries that they might actually affect  when they move, saving their limited storage and processing  resources. On the other hand, queries issue updates to the server  when they change velocity vector, or when they move out of their  current cell.    
17	 Mokbel et al. [MXA04] present SINA, a system that centrally  processes continuous range queries over mobile data. SINA is  based on shared execution</i> and <i>incremental evaluation. Shared  execution is achieved by implementing query evaluation as a  spatial join between the objects and the queries. Incremental  evaluation implies that the query processor computes only the  updates of the previously reported answers, as opposed to reevaluating the queries from scratch. The result updates are either  positive or negative. The former category corresponds to objects  entering the range of a query, while the latter one to objects  leaving a range. Both the object and the query indexes are  implemented as disk-resident regular grids. Let U P  and U q  be the  set of objects and queries that issue location updates since the  previous evaluation cycle. Processing begins with the hashing  phase</i> that joins <i>U P  and U q  in-memory to produce positive  updates. Next, the invalidation phase generates negative updates  for objects in U P  that move out of their current cell and queries in  U q  that exit cells that they used to overlap with. Finally,  movement within the same cell is handled in the joining phase;  for each cell that contains objects in U P  or intersects queries in  U q , SINA joins the new objects with the existing queries, and the  new queries with the static objects. The resulting updates are  merged with the updates of the previous phases (potentially  canceling out some of them), and are reported to the client.     
18	 All the aforementioned methods focus on range query monitoring,  and their extension to NN queries is either impossible or nontrivial. Henceforth, we discuss algorithms that target explicitly  NN processing. Koudas et al. [KOTZ04] describe DISC, a  technique for e</i>-approximate  <i>k-NN queries over streams of  multidimensional points. The returned k th  NN lies at most e  distance units farther from q</i> than the actual <i>k th  NN of q. DISC  partitions the space with a regular grid of granularity such that the  maximum distance between any pair of points in a cell is at most  e</i>. To avoid keeping all arriving data in the system, for each cell <i>c  it maintains only K points falling therein and discards the rest. It  is proven that an exact k-NN search in the retained points  corresponds to a valid ek-NN answer over the original dataset  provided that k</i><i>K. DISC indexes the data points with a B-tree  that uses a space-filling curve mechanism to facilitate fast updates  and query processing. The authors show how to adjust the index  to: (i) use the minimum amount of memory in order to guarantee a  given error bound e, or (ii) achieve the best possible accuracy,  given a fixed amount of memory. DISC can process both snapshot  and continuous ek-NN queries.   
19	 Yu et al. [YPK05] propose a method, hereafter referred to as  YPK-CNN 1 , for continuous monitoring of exact k-NN queries.                                                                      
20	 1  Yu et al. [YPK05] actually propose three methods. YPK-CNN is  shown to be the best in their experimental evaluation.   
21	 Objects are assumed to fit in main memory and are indexed with a  regular grid of cells with size   ×  . YPK-CNN does not process  updates as they arrive, but directly applies the changes to the grid.  Each NN query installed in the system is re-evaluated every T  time units. When a query q is evaluated for the first time, a twostep NN search technique retrieves its result. The initial step visits  the cells in a square R</i> around the cell <i>c q  covering q</i> until <i>k objects  are found. Figure 2.1a, shows an example of a single NN query  where the first candidate NN is p 1  with distance d</i> from <i>q</i>;  <i>p 1  is  not necessarily the actual NN since there may be objects (e.g., p 2 )  in cells outside R</i> with distance smaller than<i>  d. To retrieve such  objects, the second step searches in the cells intersecting the  square  SR</i> centered at <i>c q  with side length 2 d+  , and determines  the actual k</i> NN set of <i>q therein. In Figure 2.1a, YPK-CNN  processes  p 1  up to p 6  and returns p 2  as the actual NN. The  accessed cells appear shaded.    
22	 2 . d+   
23	   2 . d ma x +   
24	 (a) NN search  (b) Update handling  Figure 2.1: YPK-CNN examples   
25	 When re-evaluating an existing query q, YPK-CNN makes use of  its previous result in order to restrict the search space. In  particular, it computes the maximum distance d max  of the current  locations of the previous NNs (i.e., d max  is the distance of the  previous neighbor that moved furthest). The new SR is a square  centered at c q  with side length 2 d max +  . In Figure 2.1b, assume  that the current NN p 2  of q</i> moves to location <i>p  2 . Then, the  rectangle defined by d max  = dist</i>(<i>p  2 ,q) is guaranteed to contain at  least one object (i.e., p 2 ). YPK-CNN collects all objects (p 1  up to  p 10 ) in the cells intersecting SR</i> and identifies the new NN <i>p 1 .  Finally, when a query q changes location, it is handled as a new  one (i.e., its NN set is computed from scratch). Yu et al. also  discuss the application of YPK-CNN with a hierarchical grid that  improves performance for highly skewed data.    
26	 SEA-CNN [XMA05] focuses exclusively on monitoring the NN  changes, without including a module for the first-time evaluation  of an arriving query q (i.e., it assumes that the initial result is  available). Objects are stored in secondary memory, indexed with  a regular grid. The answer region</i> of a query <i>q is defined as the  circle with center q</i> and radius <i>best_dist</i>, where <i>best_dist is the  distance of the current k th  NN. Book-keeping information is stored  in the cells that intersect the answer region of q to indicate this  fact. When updates arrive at the system, depending on which cells  they affect and whether these cells intersect the answer region of  the query, SEA-CNN determines a circular search region SR  around  q</i>, and computes the new <i>k</i> NN set of <i>q therein. To  determine the radius r</i> of <i>SR, the algorithm distinguishes the  following cases: (i) If some of the current NNs move within the  answer region or some outer objects enter the answer region,  SEA-CNN sets r</i>=<i>best_dist and processes all objects falling in the  answer region in order to retrieve the new NN set. (ii) If any of  the current NNs moves out of the answer region, processing is  similar to YPK-CNN; i.e., r </i>= <i>d max  (where d max  is the distance of  the previous NN that moved furthest from q), and the NN set is  computed among the objects lying in SR. Assume that in Figure  2.2a the current NN p 2  issues an update reporting its new location  p  2 . SEA-CNN sets r</i>=<i>d max =dist</i>(<i>p  2 ,q), determines the cells  intersecting  SR (these cells appear shaded), collects the  corresponding objects (p 1  up to p 10 ), and retrieves the new NN p 1 .  (iii) Finally, if the query q</i> moves to a new location <i>q  , then SEACNN sets r </i>= <i>best_dist+dist</i>(<i>q</i>,<i>q  ), and computes the new k NN set  of q by processing all the objects that lie in the circle centered at  q   with radius r. For instance, in Figure 2.2b the algorithm  considers the objects falling in the shaded cells (i.e., objects from  p 1  up to p 10  except for p 7  and p 9 ) in order to retrieve the new NN  (p 5 ).    
27	 q  
28	 p 1 d max p 2  
29	 p 6 p 5 p 4 p 3  
30	 SR  
31	 p' 2 p 8 p 7  
32	 p 10  
33	 p 9 Answer region best_dist  
34	   q  
35	 p 1 bes t_d ist+ dis t</i>(<i>q ,q')  
36	 p 2  
37	 p 6 p 5 p 4 p 3  
38	 SR q' p 8 p 7  
39	 p 10 p 9 Answer region best_dist  
40	 (a) p 2  issues an update  (b) q</i> moves to <i>q    Figure 2.2: SEA-CNN update handling examples    
41	 Table 2.1 summarizes the properties of existing methods for  monitoring spatial queries. The processing type refers to whether  mobile objects have some computing capabilities, or the entire  processing cycle takes place in a central server. For instance, Qindex is classified as a distributed method since the objects decide  whether they exit their safe regions before they issue an update.  On the other hand, SINA follows a centralized paradigm since  each object issues an update whenever it moves, independently of  whether it influences any query or not. In summary, the only  existing techniques applicable to continuous monitoring of exact  k-NN queries are YPK-CNN and SEA-CNN. Similar to these  methods CPM also assumes centralized processing (in main  memory 2 ). We compare CPM against YPK-CNN and SEA-CNN  both qualitatively (in Section 4) and experimentally (in Section  6). In the next section, we present CPM in detail.   
42	 Method Query    Memory  Processing  Result  Q-index  Range Main  Distributed Exact  MQM Range  Main  Distributed  Exact  Mobieyes  Range Main  Distributed Exact  SINA Range  Disk  Centralized  Exact  DISC NN  Main  Centralized  Approximate YPK-CNN NN Main  Centralized  Exact  SEA-CNN NN Disk  Centralized  Exact  Table 2.1: Properties of monitoring methods   
43	                                                                     
44	 2  Even though SEA-CNN assumes that objects reside in  secondary memory, it can be also used for memory-resident  data.   
45	 3.   C ONCEPTUAL  P ARTITIONING  M ONITORING    
46	 In accordance with real-world scenarios, we assume 2D 3  data  objects and queries that change their location frequently and in an  unpredictable manner. An update from object p</i> is a tuple &lt;<i>p.id,  x old ,  y old ,  x new ,  y new &gt;, implying that p</i> moves from (<i>x old ,  y old ) to  (x new ,  y new ). A central server receives the update stream and  continuously monitors the k</i> NNs of each query <i>q installed in the  system. Similar to existing approaches (e.g., YPK-CNN, SEACNN), we use a grid index since a more complicated datastructure (e.g., main memory R-tree) would be very expensive to  maintain dynamically. The extent of each cell on every dimension  is   , so that the cell c i,j  at column i</i> and row <i>j (starting from the  low-left corner of the data space) contains all objects with x coordinate in the range [i   , (i+1)   ) and y co-ordinate in the range  [j   , (j+1)   ). Conversely, an object with co-ordinates (x</i>,<i>y)  belongs to the cell c i,j , where i=  x/    and j</i>=  <i>y/  . CPM (and  SEA-CNN) can also be applied with the hierarchical grid of  [YPK05].   Section 3.1 describes the NN computation algorithm, which  constitutes the core module of CPM. Then, Sections 3.2 and 3.3  discuss the handling of location updates. Table 3.1 summarizes  the primary symbols and functions we use throughout this section.    
47	 Symbol Description    P  The set of moving objects  N  Number of objects in P  G  The grid that indexes P     Cell side length  q   The query point   c q   The cell containing q  n  The number of queries installed in the system  dist</i>(<i>p</i>,<i>q</i>)<i>  Euclidean distance from object p</i> to query point <i>q   best_NN  The best NN list of q  best_dist  The distance of the k th  NN from q   
48	 mindist</i>(<i>c</i>,<i>q)  Minimum distance between cell c</i> and query point <i>q  Table 3.1: Frequently used symbols and functions   
49	 3.1 The NN computation module of CPM   
50	 Given a cell c</i> and a query <i>q</i>, <i>mindist</i>(<i>c</i>,<i>q) is the minimum possible  distance between any object p c</i> and <i>q</i>. Let <i>best_NN be the list of  the k</i> best NNs (of <i>q</i>) found so far, and <i>best_dist be the distance of  the  k th  of them. If mindist</i>(<i>c</i>,<i>q</i>)<i>best_dist</i>, we can safely prune <i>c  because it cannot contain any object lying closer to q than any of  the current NNs. Based on this observation, a naive way to  process a NN query q</i> in <i>P</i>, is to sort all cells <i>c G according to  mindist</i>(<i>c</i>,<i>q</i>), and visit them in ascending <i>mindist</i>(<i>c</i>,<i>q) order. For  each considered cell, we compute dist</i>(<i>p</i>,<i>q</i>) for the objects <i>p  inside, and update accordingly the best_NN list. The search  terminates when the cell c</i> under consideration has <i>mindist</i>(<i>c</i>,<i>q)   best_dist</i>. Figure 3.1a illustrates this process for a 1-NN query <i>q.  The algorithm visits only the shaded cells and encounters in total  two objects, p 1  and p 2 . Between them, p 2  is returned as the result  of the query.    
51	 It can be easily shown that the above algorithm processes only the   
52	                                                                     
53	 3  We focus on two-dimensional Euclidean spaces, but the  proposed techniques can be applied to higher dimensionality  and other distance metrics. Furthermore, for ease of  presentation, the examples demonstrate retrieval of a single NN.  cells that intersect the circle centered at q with radius equal to the  distance between q</i> and its <i>k th  NN. These cells have to be visited  anyway in order to avoid false misses; therefore, the naïve  algorithm is optimal in terms of the number of processed cells.  Nevertheless, in practice it may be very expensive, since it  requires computing the mindist for all cells and subsequently  sorting them. CPM overcomes this problem and avoids  unnecessary computations by utilizing a conceptual space  partitioning.   
54	 q  
55	 p 1  p 2 best_dist  
56	   (a) Retrieval of one NN  (b) Partitioning into rectangles Figure 3.1: NN search and conceptual partitioning    
57	 Figure 3.1b illustrates the conceptual partitioning of the space  around the cell c q  of q</i>. Each rectangle <i>rect is defined by a  direction</i> and a <i>level number. The direction could be U, D, L, or R  (for up, down, left and right) depending on the relative position of  rect</i> with respect to <i>q. The level number indicates the number of  rectangles between rect</i> and <i>c q . Lemma 3.1 regulates the visiting  order among rectangles of the same direction.    
58	 Lemma 3.1: For rectangles DIR j  and DIR j+1  of the same direction  DIR</i> with level numbers <i>j</i> and <i>j+1, respectively, it holds that  mindist</i>(<i>DIR j+1 ,q</i>) = <i>mindist</i>(<i>DIR j ,q) +   .   
59	 Proof: Without loss of generality, assume that the direction is D.  The minimum distance of q from either rectangle equals the  length of its projection on the top edge of the rectangle. Since the  side length of the cells is   , it follows that mindist</i>(<i>DIR j+1 ,q) =  mindist</i>(<i>DIR j ,q) +   .                   
60	 Based on Lemma 3.1, the NN computation module of CPM visits  cells in ascending mindist</i>(<i>c</i>,<i>q) order, thus, preserving the property  of processing the minimal set of cells. In particular, CPM  initializes an empty heap H</i> and inserts (i) the cell <i>c q  with key  mindist</i>(<i>c q ,q)=0, and (ii) the level zero rectangles for each  direction  DIR</i>, with key <i>mindist</i>(<i>DIR 0 ,q). Then, it starts deheaping entries iteratively. If the de-heaped entry is a cell, it  examines the objects inside and updates accordingly the best_NN.  If the de-heaped entry is a rectangle DIR lvl , it inserts into H (i)  each cell c DIR lvl  with key mindist</i>(<i>c</i>,<i>q) and (ii) the next level  rectangle DIR lvl+1  with key mindist</i>(<i>DIR lvl+1 ,q</i>) = <i>mindist</i>(<i>DIR lvl ,q)  +  . The algorithm terminates when the next entry in H  (corresponding either to a cell or a rectangle) has key greater than  or equal to best_dist.    
61	 Proof of correctness</i>: Let <i>best_NN be the list of NNs returned by  the algorithm, and best_dist</i> be the distance of the <i>k th  NN. Clearly,  all cells c</i> inserted at some point into <i>H do not contain any better  NN than the objects in best_NN. This is guaranteed by the sorting  property of the heap and the fact that dist</i>(<i>p</i>,<i>q</i>)  <i> mindist</i>(<i>c</i>,<i>q)  holds  p</i><i>c. In order to prove correctness, it suffices to show that  each cell that was not inserted into H cannot contain any object   
62	 closer to q</i> than <i>best_dist. This part of the proof is based on the  observation that, at any point, the heap H contains exactly four  rectangle entries, one for each direction. We call these rectangles  boundary boxes</i>. Let the boundary box of direction <i>DIR</i> be <i>DIR lvl .  The algorithm has considered all cells falling into rectangles DIR i   with i</i>&lt;<i>lvl</i>. From Lemma 3.1 it follows that all cells <i>c belonging to  DIR i  with i</i>&gt;<i>lvl</i> have <i>mindist</i>(<i>c</i>,<i>q</i>)&gt;<i>mindist</i>(<i>DIR lvl ,q). Since  mindist</i>(<i>DIR lvl ,q</i>)    <i>best_dist</i> for each boundary box <i>DIR lvl , and  since all the unexplored space falls in some rectangle of some  direction  DIR</i> with level greater than <i>lvl</i>,  <i>best_NN is the correct  result of q.     
63	 In the example of Figure 3.2a, CPM initially inserts into the heap  the  </i>cell  <i>c q  = c 4</i>,<i>4  and the rectangles of level zero, i.e.,  H =  {&lt;c 4</i>,<i>4 ,0&gt;, &lt;U 0 ,0.1&gt;, &lt;L 0 ,0.2&gt;, &lt;R 0 ,0.8&gt;, &lt;D 0 ,0.9&gt;} (the numbers  indicate mindist assuming that   =1). Then it de-heaps c 4</i>,<i>4 , which  is empty 4  and ignored. The next entry in H</i> is <i>U 0 . CPM en-heaps  the cells of U 0 , as well as rectangle U 1  and proceeds in the same  way until it de-heaps &lt;c 3</i>,<i>3 ,1&gt;, where it finds the first candidate  NN  p 1  with best_dist</i>=<i>dist</i>(<i>p 1 ,q</i>)=1.7. Since, the next entry in <i>H  has key less than best_dist</i>, it continues until it de-heaps <i>c 2</i>,<i>4  and  discovers the new candidate p 2 , with best_dist </i>= <i>dist</i>(<i>p 2 ,q) = 1.3.  The algorithm terminates (with p 2  as the NN) when the top heap  entry is c 5</i>,<i>6  because mindist</i>(<i>c 5</i>,<i>6 ,q</i>) <i>best_dist.   
64	   D 1 R 1 L 2 q p 2 U 2  
65	 (a) NN computation  (b) Search heap contents  Figure 3.2: A NN computation example   
66	 The final point that requires clarification concerns the bookkeeping information and related structures maintained for  efficient search and handling of updates (to be discussed shortly).  CPM keeps (in main memory) a query table QT that stores for  each query, its co-ordinates, the current result, the best_dist, the  visit list</i>, and the search heap <i>H:    
67	    best_dist</i> determines the <i>influence region</i> of <i>q, i.e., the set of  cells that intersect the circle centered at q</i> with radius <i>best_dist.  Only updates affecting these cells can influence the NN result.    
68	    The visit list</i> of <i>q</i> consists of all cells <i>c processed during NN  search, sorted on mindist</i>(<i>c</i>,<i>q</i>). Each cell entry de-heaped from <i>H  is inserted at the end of the list. In our example, the visit list</i> of <i>q  contains the shaded cells in Figure 3.2a.   
69	    The search heap H contains the cell and rectangle entries that  were en-heaped, but not de-heaped during NN search (i.e., their  mindist</i> from <i>q</i> is greater than or equal to <i>best_dist). The contents  of H in our example are the shaded cells in Figure 3.2b, plus the  four boundary boxes U 2 , D 1 , L 2 , and R 1 .   
70	                                                                     
71	 4  Note that, from now on, we ignore the empty cells in our  examples for the sake of clarity.  In addition, each cell c of the grid is associated with (i) the list of  data objects within its extents, and (ii) the list of queries whose  influence region contains c</i>. For example, cell <i>c 3</i>,<i>3  contains q in its  influence list, while c 5</i>,<i>6  does not.  The structures of the query  table and the object grid are shown in Figure 3.3.    
72	    
73	 (a) Query table  (b) Object grid   Figure 3.3: Query table and object grid structures   
74	 Figure 3.4 presents the full functionality of the CPM NN  computation including the maintenance of the data structures. The  influence lists of the encountered cells are updated in line 11,  while, in line 12, each processed cell is inserted into the visit list  of  q</i>. Line 18 stores the new <i>best_dist value in the query table.  Upon termination, the heap H</i> is also stored in <i>QT. The algorithm  is optimal in the sense that it processes the minimal set of cells for  retrieving the NN set of q. As opposed to the naïve algorithm  discussed in the beginning of the section, the only redundant  mindist computations concern the cells that were en-heaped but  not de-heaped (i.e., the shaded cells in Figure 3.2b). As shown in  Section 4.1, the number of such cells and rectangles is small.  Furthermore, as discussed next, CPM utilizes these computations  for the efficient handling of updates.     
75	 NN Computation (G</i>, <i>q)  // Input= G</i>: the grid indexing <i>P   // q: the query     1.  best_dist =  ; best_NN = NULL;   2.  Insert a new entry for q into the query table  3.  Initialize an empty heap H  4.  Insert &lt;c q , 0&gt; into H  5.  For each direction DIR</i> insert &lt;<i>DIR 0 , mindist</i>(<i>DIR 0 ,q</i>)&gt; into <i>H  6.  Initialize an empty list visit_list   7.  Repeat  8.    Get the next entry of H  9.    If it is a cell entry &lt;c</i>, <i>mindist</i>(<i>c</i>,<i>q)&gt;  10.      For each object p c</i>, update <i>best_NN </i>&amp;<i> best_dist if necessary  11.      Insert an entry for q</i> into the influence list of <i>c  12.      Insert  &lt;c</i>, <i>mindist</i>(<i>c</i>,<i>q</i>)&gt; at the end of <i>visit_list      13.    Else // it is a rectangle entry &lt;DIR lvl , mindist</i>(<i>DIR lvl ,q)&gt;  14.      For each cell c</i> in <i>DIR lvl   15.         Insert &lt;c</i>, <i>mindist</i>(<i>c</i>,<i>q</i>)&gt; into <i>H  16.      Insert &lt;DIR lvl+1 , mindist</i>(<i>DIR lvl ,q)+  &gt; into H  17.  Until the next entry has key   best_dist</i> or <i>H is empty  18.  Update the influence region information of q</i> to &lt;<i>q</i>, <i>best_dist&gt;  Figure 3.4: The NN computation module of CPM    
76	 3.2 Handling a single object update   
77	 Assume, for simplicity, that a single update from p P arrives at a  time. The first step is to delete p</i> from its old cell <i>c old . CPM scans  the influence list of c old  and identifies the queries that contain p in  their best_NN</i> set. Specifically, for each query <i>q (in the influence  list of c old ), if p q</i>.<i>best_NN</i> and <i>dist</i>(<i>p</i>,<i>q</i>)<i>  </i>  <i>best_dist</i>, then the <i>k   
78	 NN set of q remains the same, but the order of the NNs can  potentially change. Therefore, CPM updates the order in  q</i>.<i>best_NN</i> to reflect the new <i>dist</i>(<i>p</i>,<i>q</i>)<i>.</i> On the other hand, if<i>  p q</i>.<i>best_NN</i> and <i>dist</i>(<i>p</i>,<i>q</i>)<i>  </i>&gt;  <i>best_dist</i> (i.e., <i>p is a NN that has  moved farther from q</i> than <i>best_dist), there may exist objects (not  in  q</i>.<i>best_NN</i>) that lie closer to <i>q</i> than <i>p</i>; thus, <i>q is marked as  affected to indicate this fact and ignored for now. Next, CPM  inserts p</i> into its new cell <i>c new , and scans the influence list of c new .  For each entry q</i> therein, if <i>q has been marked as affected it  ignores it. Otherwise, if dist</i>(<i>p</i>,<i>q</i>) &lt; <i>q</i>.<i>best_dist, it evicts the  current  k th  NN from the result, inserts p</i> into <i>q</i>.<i>best_NN, and  updates q.best_dist. The last step re-computes the NN set of every  query q that is marked as affected.    
79	 Figure 3.5a illustrates update handling, assuming that object p 4   moves to position p' 4 . CPM first deletes p 4  from the object list of  c 5</i>,<i>6 , which has an empty influence list and, hence, the deletion  does not affect any result. Next, it inserts p 4  into its new cell c 5</i>,<i>3 ,  whose influence list contains an entry for q</i>. Since <i>dist</i>(<i>p' 4 ,q) &gt;  best_dist, update handling terminates without any change in the  result. Assume that, later on, object p 2  moves to a new position  p' 2 , as shown in Figure 3.5b. Since the old cell c 2</i>,<i>4  contains q in  its influence list, CPM checks the query table entry for q and  detects that p 2  = best_NN</i>. Query <i>q is marked as affected because  dist</i>(<i>p' 2 ,q</i>) &gt;<i> best_dist</i>. The insertion of <i>p 2  into its new cell c 0</i>,<i>6   does not trigger any additional processing (because the influence  list of c 0</i>,<i>6   </i>is empty). Finally, CPM invokes the NN <i>recomputation module</i> to find the new NN (<i>p' 4 ) of the affected  query q.   
80	 q  
81	 p 1 p 2 p 4 p 3  
82	 c 2,4 c 3,3 c 2,6 c 5,6  
83	 p' 4  
84	 Influence region c 5,3 best_dist  
85	   q  
86	 p 1 p 2 p 3  
87	 c 2,4 c 3,3 c 2,6  
88	 p' 4  
89	 New influence region c 5,3 p' 2 c 0,6  
90	 bes t_d ist  
91	 (a) p 4  issues an update  (b) p 2  issues an update  Figure 3.5: Update examples   
92	 Figure 3.6 illustrates the re-computation module that retrieves the  new NN set of the affected queries. The algorithm is based on the  same principles as the NN search module of CPM (Figure 3.4),  but re-uses the information stored in the query table to reduce the  running time. In particular, it starts processing sequentially the  cells stored in the visit list</i> of <i>q, and then it continues with the  entries of the search heap H</i>. Note that all the cells in the <i>visit list  have mindist</i> less than (or equal to) the entries of <i>H. It follows that  the NN re-computation algorithm considers cells c in ascending  mindist</i>(<i>c</i>,<i>q) order, which guarantees the correctness of the result,  as well as the minimality of the set of processed cells. The  benefits of NN re-computation over computation from scratch are:  (i) it utilizes the previously computed mindist values, and (ii) it  significantly reduces the number of heap operations  (insertions/deletions). Recall that the cost of each heap operation  is logarithmic to the heap size, while the "get next" operation on  the visit list (in line 3 of Figure 3.6) is O(1).    NN Re-Computation (G</i>, <i>q)  // Input= G</i>: the grid indexing <i>P</i>, <i>q: the affected query     1.  best_dist =  ; best_NN = NULL;   2.  Repeat  3.    Get the next element &lt;c</i>, <i>mindist</i>(<i>c</i>,<i>q</i>)&gt; of <i>visit_list  4.    For each object p c</i>, update <i>best_NN </i>&amp;<i> best_dist if necessary  5.    Insert an entry for q</i> into the influence list of <i>c  6.  Until the next element has key   best_dist</i> or <i>visit_list is empty  7.  If the first entry in H</i> has key &lt; <i>best_dist  8.     (Same as lines 7-17 of Figure 3.4)    9.  Set influence region information of q</i> to &lt;<i>q</i>, <i>best_dist&gt;  Figure 3.6: The NN re-computation module of CPM    
93	 3.3 Handling multiple updates   
94	 So far we have dealt with processing a single update. However, in  the general case, there is a set U P  of object updates that arrive  during the time interval between two consecutive update handling  cycles. Processing incrementally each update in U P , as discussed  in Section 3.2, guarantees correctness of the result. However, this  can be improved upon. Consider the example of Figure 3.7a,  where  U P   </i>contains location updates for <i>p 2  and p 3 . If p 2  is  processed first, q</i> will be marked as affected (<i>p 2  is the current NN  and moves farther than best_dist), triggering the NN recomputation module. This, however, is unnecessary because  object  p 3  moves closer to q</i> than the previous <i>best_dist, and we  could simply replace the outgoing NN p 2  with the incoming p 3 .    
95	 q  
96	 p 1 p 2 p 4 p 3  
97	 c 2,4 c 3,3 c 2,6 c 5,6  
98	 Influence region p' 2 p' 3 best_dist  
99	   q  
100	 p 1 p 4  
101	 c 3,3 c 5,6  
102	 New influence region p' 3 be st_ dis t c 3,5  
103	 (a) p 2  and p 3  issue updates  (b) p 3  </i>becomes the NN of <i>q  Figure 3.7: An update handling example   
104	 In general, let O be the set of outgoing NNs (i.e., NNs that move  farther from q</i> than <i>best_dist</i>) and <i>I be the set of incoming objects  (i.e., objects other than the current NNs that move closer to q than  best_dist</i>). The circle with center <i>q</i> and radius <i>best_dist contains  objects  I    best_NN ­ O</i>. If |<i>I</i>||<i>O</i>| (where |<i>I</i>| and |<i>O| are the  cardinalities of I</i> and <i>O, respectively), this circle includes at least  k</i> objects. Therefore, we can form the new NN set from the <i>k best  objects in I   best_NN ­ O without invoking re-computation. We  embed this enhancement in the CPM algorithm as follows. Before  processing U P , we record the current best_dist</i> of <i>q. During update  handling, we maintain the in_list</i> of the <i>k best incoming objects  (we do not need more than the k best incomers in any case). At  the end of the procedure, if in_list</i> contains more than |<i>O| objects,  we merge the NNs in best_NN ­ O</i> with <i>in_list, and keep the best  k</i> among them to form the new result of <i>q. We resort to NN recomputation only if in_list</i> contains fewer than |<i>O| objects.    
105	 Figure 3.8 shows the complete update handling module of CPM.  An important remark is that if |I</i>||<i>O</i>|, the influence region of <i>q  shrinks. Consequently, line 22 deletes q from the influence lists of   
106	 the cells that no longer belong to it. Note that, at any time, the  visit list contains a superset of the cells in the influence region of  q</i>. Therefore, we can simply scan the cells <i>c</i> in the <i>visit list with  mindist</i>(<i>c</i>,<i>q</i>) between the new and the old value of <i>best_dist, and  delete q</i> from their influence lists. The new influence region of <i>q  in our example is shown in Figure 3.7b. After update handling,  the visit list contains a superset of the cells in the influence region  (i.e., the visit list still includes the shaded cells in Figure 3.7a).   
107	 Update Handling (G</i>, <i>QT</i>, <i>U P )  // Input= G</i>: the grid, <i>QT</i>: query table, <i>U P : set of updates in P     1.  For each query q</i> in <i>QT  2.    Set q</i>.<i>out_count=0; // Counter of outgoing NNs  3.    Initialize a sorted list q.in_list</i> of size <i>k     4.  For each update &lt;p.id</i>,<i>x old ,y old ,x new ,y new &gt; U P   5.    Delete p</i> from its old cell <i>c old    6.    For each query q</i> in the influence list of <i>c old   7.      If p q.best_NN  8.        If dist</i>(<i>p</i>,<i>q</i>)  <i>q</i>.<i>best_dist </i>// <i>p remains in the NN set  9.          Update the order in q.best_NN  10.        Else // p is an outgoing NN    11.          Evict p</i> from <i>q.best_NN  12.          q.out_count </i>=<i> q.out_count +1;  13.    Insert p</i> into its new cell <i>c new    14.    For each query q</i> in the influence list of <i>c new   15.      If dist</i>(<i>p</i>,<i>q</i>)<i>q</i>.<i>best_dist</i> and <i>p q.best_NN</i>  // <i>p is an incomer  16.        Update q</i>.<i>in_list</i> with <i>p  17.  For each query q</i> in <i>QT  18.    If q</i>.<i>in_list</i> contains at least <i>q.out_count objects  19.      candidate_list</i> = <i>q.in_list   q.best_NN ;  20.      q.best_NN</i> = the best <i>k</i> objects in <i>candidate_list   21.      Update q.best_dist</i>, Set inf. region of <i>q</i> to &lt;<i>q</i>, <i>p.best_dist&gt;  22.      Delete q from inf. lists of cells no longer in its inf. region    23.    Else // Not enough incoming objects  24.      NN Re-Computation (G</i>, <i>q);  Figure 3.8: The update handling module of CPM   
108	 In addition to data objects, queries may also  be dynamic; i.e.,  some are terminated, new ones arrive at the system, while others  move. When a query is terminated, we delete its entry from QT  and remove it from the influence lists of the cells in its influence  region. For new arrivals, we execute the NN computation  algorithm of Figure 3.4. When an existing query q moves, we  treat the update as a termination of the old query, and an insertion  of a new one, posed at its new location. Queries that receive  updates are ignored when handling object updates in order to  avoid waste of computations for obsolete queries. Figure 3.9  presents the complete CPM algorithm, covering all update types.    
109	 NN Monitoring (G</i>, <i>QT)  // Input= G</i>: the grid indexing <i>P</i>, <i>QT: query table  1.  In every processing cycle do   2.     U q  = set of query updates  3.     U P  = set of updates in P  4.     Invoke Update Handling (G</i>, <i>QT</i>, <i>U P ) ignoring queries in U q   5.     For each query q</i> in <i>U q    6.       If q is a terminated or a moving query     7.         Delete q</i> from <i>QT and from inf. lists of cells in its inf. region  8.       If q is a new or a moving query  9.         NN Computation (G</i>, <i>q);  10.     Inform client for updated results  Figure 3.9: The CPM algorithm   
110	 In general, the nearest neighbors of q are concentrated in a small  area of the workspace and the influence region of q contains few  cells. Therefore, the influence list overhead, and the search  heap/visit list sizes are expected to be small. However, in case that  the physical memory of the system is exhausted, we can directly  discard the search heap and the visit list</i> of <i>q to free space. Even  without this information, CPM can continue monitoring q; the  difference is that we have to invoke the NN computation  algorithm from scratch (instead of NN re-computation) in line 24  of the update handling module of Figure 3.8.   
111	 4.   P ERFORMANCE  A NALYSIS    
112	 Section 4.1 analyzes the performance of CPM in terms of space  requirements and running time. Section 4.2 compares CPM with  the existing algorithms for continuous NN monitoring.   
113	 4.1 Analysis of CPM   
114	 In order to study the performance of CPM and analyze the effect  of the cell size   , we assume that the objects (queries) are  uniformly distributed 5  in a unit square workspace. First, we  provide formulae for the space/time overhead with respect to: (i)  the number of cells C inf  in the influence region of a k</i>-NN query <i>q,  (ii) the number O inf  of objects in the influence region, and (iii) the  total number C SH  of cells stored either in the visit list or in the  search heap of q. Then, we estimate the values of these  parameters as functions of   , and conclude with observations  about the expected performance of CPM in practice.    
115	 For simplicity, we assume that the minimum unit of memory can  store a (real or integer) number. The amount of memory required  for an object is s obj =3 for its id and two co-ordinates. Similarly,  each heap/visit list</i> entry consumes <i>s etr =3 memory units for the  cell (rectangle) column/row and mindist.  The first component of  the space overhead is the size of the grid index. The grid contains  N</i> objects, consuming <i>s obj N</i>=3<i>N space, plus the auxiliary  influence lists of the cells. For each query q, we insert its id into  the influence lists of C inf  cells. Assuming n</i> concurrent <i>k-NN  queries, the grid index has total size Space G  = 3 N</i> + <i>n</i><i>C inf . The  query table contains one entry for each query q. The memory  dedicated for an entry is s obj  + 2 k</i> + <i>s etr (C SH +4); s obj =3 is required  for the id and co-ordinates of q, while 2 k space is used for the  object ids of the k</i> NNs and their distances from <i>q. The  s etr (C SH +4)=3 (C SH +4) component corresponds to the storage  overhead of the visit list</i> and the search heap <i>H; these two  structures combined contain C SH  cells plus four rectangle entries.  It follows that the size of the query table is Space QT  =  n (15+2k</i>+3<i>C SH ). In total, the memory requirements of CPM are  Space CPM  = Space G  + Space QT  = 3 N</i> + <i>n</i>(15+2<i>k</i>+3<i>C SH +C inf )  memory units.     
116	 In order to estimate the running time per processing cycle, we  assume that N f obj  </i>objects and <i>n f qry  queries issue location updates  following random displacement vectors. The total cost is Time CPM   = N f obj Time ind  + n f qry Time mq  + n (1-f qry ) Time sq , where Time ind  is  the index update time for a single object, Time mq  is the time  required for the NN computation of a moving query, and Time sq  is  the time required for updating the NNs of a static query. The   
117	                                                                     
118	 5  Although, admittedly, the uniformity assumption does not hold  in practice, similar to previous work [YPK05], we use it to  obtain general observations about the effect of the problem  parameters.   
119	 object lists of the cells are implemented as hash tables so that the  deletion of an object from its old cell and the insertion into its  new one takes expected Time ind =2. For each moving query we  have to invoke the NN computation algorithm of Figure 3.4 with  cost  Time mq  = C SH logC SH  + O inf logk</i> + 2<i>C inf . The first factor is  due to the heap operations. The number of entries in H throughout  the NN search procedure is upper-bounded by C SH +4     C SH .  Since insertion and deletion is logarithmic to the size of the heap,  the overall time spent on heap operations is C SH logC SH . The  algorithm processes O inf  objects, taking O inf logk time  cumulatively; each object is probed against the best_NN list to  update the result, taking logk time with a red-black tree  implementation of best_NN</i>. Removing or inserting <i>q from/into  the influence list of a cell takes constant expected time (the lists  are implemented as hash-tables). Therefore, updating the  influence lists of all cells falling in the old and the new influence  region costs 2 C inf . For estimating Time sq , observe that at any time  instant, the objects are distributed uniformly in the workspace.  This implies that the circle with radius best_dist</i> always contains <i>k  objects, or equivalently, there are as many incoming objects as  outgoing NNs. Let there be |O| outgoing NNs. In the worst case,  all the remaining k</i>-|<i>O| NNs move. Re-ordering the remaining  NNs and inserting the |I</i>|=|<i>O</i>| incomers into <i>best_NN</i> takes <i>Time sq  = k logk. Summing over all queries and the index update time, the  computational overhead of a processing cycle is Time CPM  =  2 N</i><i>f obj  + n f qry (C SH logC SH  + O inf logk</i> + 2<i>C inf ) + n (1-f qry ) k</i>log<i>k.   
120	 It remains to estimate the numbers C inf  (O inf ) of influencing cells  (objects) and cells C SH  in the visited list and heap of a random  query  q. Let   q  be the circle centered at q with radius equal to  best_dist. For uniform data, the ratio of the area of   q  to the area   
121	 of the workspace equals k</i>/<i>N</i> so that <i>best_dist</i>= <i>k/  N .  The  influence region of q consists of cells intersecting   q . The number  of these cells is roughly C inf  =   best_dist/   2 , and the  corresponding objects are O inf  = C inf N   2  (each cell contains N    2   objects on average). As     </i>decreases,  <i>C inf  increases, the shape of  the influence region better approximates   q , and O inf  approaches k  (which is its minimum value). On the other hand, a large    leads  to a small number of cells which, however, contain a large  number of objects. Figure 4.1 illustrates the effect of    on C inf  and  O inf , assuming a 1-NN query q. The shaded cells correspond to the  influence region of q</i>, which in Figure 4.1a contains <i>C inf =39 cells  and O inf =1 objects. For a larger value of   , in Figure 4.1b, C inf =8  and  O inf =8. To estimate C SH , assume for simplicity that q is  located at the center of its cell c q . The boundary boxes are of the  same level in each direction. It follows that C SH  is the number of  cells that intersect the circumscribed square of   q . Thus, C SH  can  be approximated by 4 best_dist/   2 . Similar to C inf , C SH  decreases  as    increases, e.g., in Figure 4.1a, C SH =49, while in Figure 4.1b,  C SH =9.  In summary, the space consumed by the influence lists of the cells  and the query table, is inversely proportional to     2 . Similarly,  both the size of the influence lists and the size of the query table  are linear to n</i> and <i>k. Concerning the computational cost of CPM,  index update time is linear to N</i> and <i>f obj . The result maintenance  task takes linear time with respect to n, and is expected to grow as  f qry  increases. The time of NN computation for a new or a moving  query depends strongly on the cell size; a small value for    incurs  high overhead due to heap operations, while a large value implies  a high number O inf  of processed objects.       Influence region  
122	 best_dist  
123	 Cells in H and visit list q  
124	 p 1 p 2  
125	 p 6 p 5 p 4 p 3 p 8 p 7  
126	 (a) Small     (b)  Large     Figure 4.1: The effect of    on the performance of CPM   
127	 4.2 Qualitative comparison with existing methods   
128	 Next, we illustrate the superiority of CPM over the existing  methods through some update handling scenarios. YPK-CNN reevaluates periodically every query q, even if the object updates  have not affected any cell in its vicinity. This is due to the fact  that it does not include a mechanism for detecting queries  influenced by location updates. Furthermore, in the general case,  YPK-CNN visits more cells than necessary when performing NN  search for moving and new queries. Consider the 1-NN  computation of query q in Figure 4.2a. As discussed in Section 2  (the example is the same as Figure 2.1), YPK-CNN processes 25  cells and six objects (p 1  up to p 6 ). Finally, it also incurs redundant  computations for static queries. Assuming that in Figure 4.2b the  current NN p 2  moves to location p  2 , YPK-CNN processes 49  cells and ten objects (p 1  up to p 10 ). Clearly, the unnecessary  computations increase with dist</i>(<i>p  2 ,q). On the other hand, CPM  (i) only processes queries whose influence region intersects some  updated cell, and (ii) the NN computation and re-computation  modules restrict the search space to the minimum number of cells  around q (i.e., shaded cells in Figure 4.2).   
129	 q  
130	 p 1 p 2  
131	 p 6 p 5 p 4 p 3  
132	 best_dist  
133	 Cells visited by YPK-CNN Cells visited by CPM (shaded)  
134	   be st _d is t  
135	 (a) NN search   (b) Update handling  Figure 4.2: CPM versus YPK-CNN   
136	 SEA-CNN also performs redundant computations in several  cases. First, assume that the only updates are from incoming  objects and/or NNs that move within distance best_dist</i> from <i>q.  For instance, in Figure 4.3a, p 6  moves closer to q</i> than <i>best_dist.  SEA-CNN visits all cells intersecting the circle centered at q with  radius r </i>= <i>best_dist</i> and determines the new NN (<i>p' 6 ) among the  processed objects p 1 , p 2  and p' 6 . On the other hand, CPM directly  compares  dist</i>(<i>p' 6 ,q</i>) with <i>best_dist</i> and sets <i>p' 6  as the result  without visiting any cells. When k is larger, the computational  waste of SEA-CNN increases because it considers a higher  number of objects, even though there might be few changes in the   
137	 result. Another weak point of SEA-CNN concerns handling of  outgoing NNs, which is similar to YPK-CNN. Recall that when p 2   moves to p  2 ,  </i>SEA-CNN processes ten objects <i>p 1  up to p 10  (see  Figure 2.2a), while CPM considers only four objects (see Figure  4.2b). SEA-CNN incurs higher cost than CPM also in the case  that q</i> changes position. In Figure 4.3b, assuming that <i>q moves to  q  , CPM considers only cells intersecting the circle with center at  q   and radius dist</i>(<i>p 5 ,q  ), and retrieves the NN (p 5 ) by processing  only two objects (p 4  and p 5 ) in total. SEA-CNN considers 33 cells  and eight objects. A final remark about SEA-CNN is that it does  not handle the case where some of the current NNs go off-line.  On the contrary, CPM trivially deals with this situation by  treating off-line NNs as outgoing ones.   
138	   q  
139	 p 1 p 2  
140	 p 6 p 5 p 4 p 3  
141	 q' p 8  
142	 p 10 best_dis t  
143	 Cells visited by SEA_CNN Cells visited by CPM  
144	 (a) p 6  issues an update  (b) q</i> moves to <i>q    Figure 4.3: CPM versus SEA-CNN   
145	 Summarizing, the speed of the objects does not affect the running  time of CPM since update handling is restricted to the influence  regions of the queries. On the other hand, the performance of both  YPK-CNN and SEA-CNN (as also observed in [YPK05] and  [XMA05]) degrades with object speed because the search region  for a static query is determined by how far the furthest previous  NN has moved since the last evaluation. For moving queries,  CPM examines the minimum possible number of cells (which is  independent of the query moving distance), whereas the cost of  SEA-CNN increases with the velocity of q.     
146	 5.   A GGREGATE  NN S AND  O THER  Q UERY  T YPES    
147	 In this section we extend the CPM algorithm to aggregate NN  queries starting with the sum function. Given a set of query points  Q  </i>= {<i>q 1 ,q 2 ,...,q m }, a sum ANN query continuously reports the  data object p</i> that minimizes <i>adist</i>(<i>p</i>,<i>Q) =  q i Q   dist</i>(<i>p</i>,<i>q i ). The  basis of our method remains the conceptual partitioning of the  space around the query Q</i>. Since <i>Q now consists of a set of query  points, the partitioning applies to the space around the minimum  bounding rectangle M</i> of <i>Q. Figure 5.1a exemplifies the  partitioning into rectangles in the case of a 1-ANN query Q =  {q  
148	 1 ,q 2 ,q 3 }. We define amindist</i>(<i>c</i>,<i>Q) =  q i Q   mindist</i>(<i>c</i>,<i>q i ), which  is a lower bound for the distance adist</i>(<i>p</i>,<i>Q</i>) of any object <i>p c.  The definition of amindist</i>(<i>DIR lvl ,Q</i>) for a rectangle <i>DIR lvl  is  similar. The cell processing order is derived by corollary 5.1,  which is based on the same geometric observations as Lemma 3.1  (and, hence, we omit its proof).   
149	 Corollary 5.1 (<i>f</i>=<i>sum</i>): For rectangles DIR j  and DIR j+1  of the  same direction DIR</i> with level numbers <i>j</i> and <i>j+1, it holds that  amindist</i>(<i>DIR j+1 ,Q</i>) = <i>amindist</i>(<i>DIR j ,Q</i>) + <i>m   , where m is the  number of  points in Q.    
150	 The ANN search module of CPM is essentially the same as the  algorithm in Figure 3.4. The difference is that in the beginning of  the search, we en-heap (in line 4) all cells c</i> intersecting <i>M.  The  sorting key is amindist</i>(<i>c</i>,<i>Q</i>) and <i>amindist</i>(<i>DIR lvl ,Q) for the enheaped cells and rectangles, respectively. When an object p is  processed, we compute adist</i>(<i>p</i>,<i>Q) and update accordingly the list  of best ANNs found so far (i.e., best_NN). The algorithm  terminates when the next entry in H</i> has <i>amindist greater than or  equal to best_dist. In our example, the algorithm terminates with  p 2  as the result, after processing all the shaded cells in Figure  5.1b. Similar to Section 3.1, the influence region of Q is the set of  cells c</i> with <i>amindist</i>(<i>c</i>,<i>Q</i>)<i>best_dist; only updates affecting these  cells can change the ANN result. Note that the influence region of  a query is no longer a circle, but has an irregular shape (i.e., the  shaded region in Figure 5.1b). Update handling is the same as in  Section 3, the difference being that we use the aggregate distance  function instead of the Euclidean one.     
151	   p 1 p 2 p 5 p 4 p 3 q 1 M q 2 q 3 L 1 U 1  
152	 R 1  
153	 D 1  
154	 (a) Partitioning into rectangles  (b) Influence region  Figure 5.1: ANN monitoring for f</i>=<i>sum    
155	 When  f</i>=<i>min</i>, an ANN query <i>Q</i> retrieves the object(s) in <i>P with  the smallest distance(s) from any point in Q. The ANN search  considers cells and rectangles in ascending amindist order. For a  cell c</i>, <i>amindist</i>(<i>c</i>,<i>Q) = min q i Q  mindist</i>(<i>c</i>,<i>q i ), while for a rectangle  DIR lvl ,  amindist</i>(<i>c</i>,<i>DIR lvl ) = min q i Q   mindist</i>(<i>DIR lvl ,q i ). Corollary  5.2 dictates the cell processing order.   
156	 Corollary 5.2 (<i>f</i>=<i>min</i> or <i>f</i>=<i>max</i>): For rectangles DIR j  and DIR j+1   of the same direction DIR</i> with level numbers <i>j</i> and <i>j+1, it holds  that amindist</i>(<i>DIR j+1 ,Q</i>) = <i>amindist</i>(<i>DIR j ,Q) +   .   
157	 The ANN search and update handling modules of CPM are  similar to the sum</i> case. Furthermore, for the <i>min function, we can  improve the O(m</i>) time required to compute <i>amindist</i>(<i>DIR 0 ,Q) to  O(1). The MBR M</i> of <i>Q</i> contains by definition one point of <i>Q on  each edge. Therefore, computing amindist</i>(<i>DIR 0 ,Q) for each  direction  DIR reduces to calculating the minimum distance  between rectangle DIR 0  and the closest edge of M. For example,  amindist</i>(<i>D 0 ,Q</i>) equals to the distance between the top edge of <i>D 0   and the bottom edge of M. An interesting observation about the  min</i> aggregate function is that the influence region of <i>Q contains  cells that intersect at least one of the circles centered at some q i   with radius best_dist</i>. Figure 5.2a shows an example where <i>Q =  {q 1 ,q 2 ,q 3 } and f</i>=<i>min</i>. The result of the query is <i>p 2 , and the  influence region of Q appears shaded.      
158	 When  f</i>=<i>max</i>, CPM monitors the object(s) of <i>P that have the  lowest maximum distance(s) from points in Q</i>. For each cell <i>c,  amindist</i>(<i>c</i>,<i>Q) = max q i Q   mindist</i>(<i>c</i>,<i>q i ), while for each boundary  box  DIR lvl ,  amindist</i>(<i>DIR lvl ,Q) = max q i Q   mindist</i>(<i>DIR lvl ,q i ).  Corollary 5.2 holds also in the case of max, whereas computing  amindist</i>(<i>DIR 0 ,Q</i>) for each direction <i>DIR can be performed in   
159	 O(1) time: amindist</i>(<i>DIR 0 ,Q) equals the minimum distance  between  DIR 0  and the opposite edge of M. In Figure 5.2b we  illustrate the case where Q</i> = {<i>q 1 ,q 2 ,q 3 } and f</i>=<i>max. The result of  the query is object p 4 , and the corresponding influence region  consists of the shaded cells.   
160	 p 1 p 2 p 5 p 4 p 3 q 1 M q 2 q 3 L 1 U 1  
161	 R 1  
162	 D 1  
163	   p 1 p 2 p 4  
164	 q 1 M q 2 q 3 L 1 U 1  
165	 R 0  
166	 D 1 best _dis t  
167	 (a) f</i>=<i>min (b)  f</i>=<i>max  Figure 5.2: ANN monitoring for f</i>=<i>min</i> and <i>f</i>=<i>max   
168	 Finally, CPM can easily handle constrained variations of NN (and  ANN) search that retrieve the NNs of a query point in a userspecified area of the data space. Ferhatosmanoglu et al. [FSAA01]  propose algorithms for static datasets indexed by R-trees. The  adaptation of CPM to this problem inserts into the search heap  only cells and conceptual rectangles that intersect the constraint  region. Assume, for instance, that in Figure 5.3 we want to  monitor the NN to the northeast</i> of <i>q. CPM en-heaps only the  cells  c 4,4 ,  c 4,5 ,  c 5,4 ,  c 5,5   </i>and rectangles <i>U 0 ,  R 0 ,  U 1 ,  R 1 . Inside c 5,5 ,  object  p 3  is identified as the NN. Note that object p 1  (the  unconstrained NN) is not encountered at all since its cell is not  visited, whereas p 2  is processed but not reported.    
169	   Figure 5.3: Monitoring of a constrained NN query   
170	 6.   E XPERIMENTAL  E VALUATION    
171	 In this section we evaluate the performance of CPM and compare  it with YPK-CNN and SEA-CNN. In accordance with the  experimental study of [XMA05], our datasets are created with the  spatiotemporal generator of [B02]. The input of the generator is  the road map of Oldenburg (a city in Germany). The output is a  set of objects (e.g., cars, pedestrians) moving on this network,  where each object is represented by its location at successive  timestamps. An object appears on a network node, completes the  shortest path to a random destination, and then disappears. We use  the default velocity values of the generator for slow</i>, <i>medium, and  fast object speeds. Objects with slow speed cover a distance that  equals 1/250 of the sum of the workspace extents per timestamp.  Medium and fast speeds correspond to distances that are 5 and 25  times larger, respectively. The NN queries are generated  similarly, i.e., they are objects moving on the same network, but  they stay in the system throughout the simulation. The queries are  evaluated at every timestamp and the simulation length is 100  timestamps. In the implementation of SEA-CNN, we use the NN  search algorithm of YPK-CNN to compute the initial results of  the queries, or to retrieve the new NN sets when some of the  current NNs disappear. Table 5.1 summarizes the parameters  under investigation, along with their ranges and default values. In  each experiment we vary a single parameter, while setting the  remaining ones to their default values. For all simulations we use  a Pentium 2.4 GHz CPU with 1 GByte memory.   
172	 Parameter Default   Range  Object population (N)  100K  10, 50,100,150,200 (K)  Number of queries (n)   5K  1,2,5,7,10 (K)  Number of NNs (k) 16  1,4,16,64,256  Object/Query speed  medium  slow, medium, fast  Object agility (f obj ) 50%  10,20,30,40,50  (%)  Query agility (f qry ) 30%  10,20,30,40,50  (%)  Table 6.1: System parameters (ranges and default values)   
173	 Initially, we generate 5K queries and 100K objects, according to  the default parameters of Table 6.1. We process the queries with  each monitoring algorithm, and measure the overall running time  by varying the grid granularity. Figure 6.1 illustrates the results  for grid sizes ranging between 32 ×32 and 1024×1024.  CPM  clearly outperforms both competitors for all grid sizes. SEA-CNN  is worse than YPK-CNN because it incurs unnecessary  computations for moving queries, as explained in Section 4.2. A  128 ×128 grid (i.e.,    = 1/128) constitutes a good tradeoff between  the CPU time and the space requirements for all methods 6 .  Therefore, we perform the remaining experiments using    =  1/128.    
174	 CPM SEA-CNN YPK-CNN    
175	 32 64 128 256 512 1024 2 2 2 2 2 2 Number of cells in G 0 100 200 300 400 500 600 700 800 900 1000 CPU time  
176	   Figure 6.1: CPU time versus grid granularity    
177	 Next we examine scalability issues. Figure 6.2a measures the  effect of the object population N on the running time. The  generator is tuned so that the average object population during the  simulation equals the desired value N. Similarly, Figure 6.2b  illustrates the CPU overhead as a function of the number n of  queries in the system. The cost of all algorithms increases linearly   
178	                                                                     
179	 6  The space overhead is 2.854 MBytes, 3.074 MBytes, and 3.314  MBytes for YPK-CNN, SEA-CNN and CPM, respectively.    
180	 to both N</i> and <i>n. However, YPK-CNN and SEA-CNN are much  more sensitive than CPM to these parameters, confirming the  scalability of our approach.   
181	 CPM SEA-CNN YPK-CNN    
182	 CPU time  
183	 Number of objects 0 200 400 600 800 1000 1200  
184	 10K 50K 100K 150K 200K   CPU time  
185	 Number of queries 0 200 400 600 800 1000 1200  
186	 1K 2K 5K 7K 10K  
187	 (a) Effect of N  (b) Effect of n  Figure 6.2: CPU time versus N</i> and <i>n   
188	 Figure 6.3a shows the CPU time as a function of the number k of  NNs (using the default values for the remaining parameters).  Figure 6.3b plots (in logarithmic scale) the number of cell  accesses per query per timestamp. A cell visit corresponds to a  complete scan over the object list in the cell. Note that a cell may  be accessed multiple times within a cycle, if it is involved in the  processing of multiple queries. For CPM, cell accesses occur  during the NN computation algorithm (for moving queries), and  during NN re-computation (for stationary queries, when there are  more outgoing NNs than incomers). YPK-CNN re-evaluates the  queries in every timestamp, and therefore induces cell visits for  each query in every processing cycle. SEA-CNN accesses cells  whenever some update affects the answer region of a query and/or  when the query moves. CPM significantly outperforms its  competitors because: (i) it does not search the grid if the update  information suffices to maintain the results, and (ii) even if the  updates necessitate computation from scratch or re-computation  of the NN sets, CPM processes the minimal number of cells. An  interesting observation is that for k</i>=1 and <i>k=4, CPM accesses less  than one cell per query on the average. This happens because  queries of case (ii) have a small cost (i.e., 1-2 cell visits), which is  counter-balanced by queries of case (i) that do not incur any  visits.       
189	 CPM SEA-CNN YPK-CNN    
190	 CPU time  
191	 0 500 1000 1500 2000 2500  
192	 1 4 16 64 256 Number of NNs   Cell accesses  
193	 Number of NNs 0.1 1 10  
194	 1 4 16 64 256 10 2 10 3  
195	 (a) CPU time  (b) Cell accesses  Figure 6.3: Performance versus k   
196	 Figure 6.4a illustrates the CPU time with respect to the object  speed. The performance of CPM is practically unaffected by the  speed of objects. On the contrary, both YPK-CNN and SEA-CNN  degenerate when objects move fast, as anticipated in Section 4.2.  Figure 6.4b depicts the effect of the query speed on the running  time of the algorithms. The cost of CPM and YPK-CNN is  independent of the query velocity, since both techniques compute  the results of the moving queries from scratch. On the other hand,  SEA-CNN is negatively affected because, as discussed in Section  4.2, the search region grows when the queries move far from their  previous position, increasing the number of computations.    
197	 CPM SEA-CNN YPK-CNN    
198	 CPU time  
199	 0 100 200 300 400 500 600 700 800 900  
200	 Slow Medium Fast Object speed   CPU time  
201	 Query speed 0 100 200 300 400 500 600 700 800 900 1000  
202	 Slow Medium Fast  
203	 (a) Effect of object speed  (b) Effect of query speed  Figure 6.4: CPU time versus object and query speed   
204	 Figure 6.5a compares the performance of CPM, YPK-CNN and  SEA-CNN versus the percentage of objects that move within a  timestamp (i.e., the object agility f obj ). As expected (see Section  4.1), the running time of CPM scales linearly with the object  agility, due to the increasing index update cost. In order to  quantify the effect of the query agility f qry  (i.e., the probability  that a query moves within a timestamp), we vary f qry  from 10% to  50% and keep the remaining parameters fixed to their default  values. As shown in Figure 6.5b, the CPU time of CPM increases  linearly with f qry  because NN computations (for moving queries)  are more expensive than result maintenance for static queries.  Note that YPK-CNN is rather insensitive to the query agility  because the incremental maintenance of the NN set (for stationary  queries) has similar cost to the two-step NN computation (for  moving queries).    
205	 CPM SEA-CNN YPK-CNN    
206	 CPU time  
207	 Object agility 0 100 200 300 400 500 600 700  
208	 10% 20% 30% 40% 50%   CPU time  
209	 Query agility 0 100 200 300 400 500 600 700  
210	 10% 20% 30% 40% 50%  
211	 (a) Effect of object agility (f obj )  (b) Effect of query agility (f qry ) Figure 6.5: CPU time versus object and query agility   
212	 In the remaining two experiments, we compare individually the  NN computation and result maintenance modules of the  alternative methods. First, we monitor 5K constantly moving  queries (i.e., queries that issue location updates in every  timestamp), while varying the object population N. The query  results are computed from scratch at every processing cycle;  therefore, we can study the efficiency of the NN computation  modules. SEA-CNN is omitted (since it does not include an  explicit mechanism for obtaining the initial NN set). As shown in  Figure 6.6a, CPM outperforms YPK-CNN and the performance  gap increases with N. Finally, we process 5K static queries (i.e.,  f qry =0%), while varying the object population N. This way we  eliminate the NN computations from scratch (apart from the  initial query evaluation) and measure the pure result maintenance  cost. As shown in Figure 6.6b, the behavior of YPK-CNN and  SEA-CNN is similar, while CPM induces considerably fewer  computations.   
213	 CPM SEA-CNN YPK-CNN    
214	 CPU time  
215	 Number of obejcts 0 20 40 60 80 100 120 140 160  
216	 10K 50K 100K 150K 200K   CPU time  
217	 Number of obejcts 0 200 400 600 800 1000 1200  
218	 10K 50K 100K 150K 200K  
219	 (a) Constantly moving queries   (b) Static queries  Figure 6.6: CPU time for constantly moving and static queries   
220	 7.   C ONCLUSIONS    
221	 This paper investigates the problem of monitoring continuous NN  queries over moving objects. The task of the query processor is to  constantly report the results of all queries, as location updates  stream by from both the objects and the queries. Our contribution  is an efficient processing method, referred to as the conceptual  partitioning monitoring (CPM) algorithm. CPM is based on a  conceptual partitioning of the space around each query q, in order  to restrict the NN retrieval and the result maintenance  computations to objects that lie in the vicinity of q. The core of  CPM is its NN computation module, which retrieves the first-time  results of incoming queries, and the new results of existing  queries that change location. This module produces and stores  book-keeping information to facilitate fast update handling.  Keeping the NN set of a query q up-to-date is performed by  processing on-line the object updates as they arrive. If the new  NN set of a query can be determined solely by the previous result  and the set of updates, then access to the object grid G is avoided.  Otherwise, CPM invokes the NN re-computation module, which  uses the book-keeping information stored in the system to reduce  the running time (compared to NN computation from scratch).  CPM is a generally applicable technique, since it does not require  any knowledge about the object or query moving patterns (e.g.,  velocity vectors), and can concurrently process multiple (static or  moving) queries. We analyze its performance and compare it with  the existing state-of-the-art methods. As demonstrated by a  qualitative analysis and by an extensive experimental study, CPM  outperforms its competitors.   Finally, to support the generality of the proposed methodology,  CPM is applied to aggregate NN monitoring, where a query  consists of a set of points and the optimization goal depends on an  aggregate function (such as sum</i>, <i>min</i> and <i>max). In the future, we  intend to explore the problem of continuous monitoring for  variations of NN search, such as reverse NNs. A preliminary  approach on this topic considers one-dimensional streams and  aggregate reverse NN [KMS02]. It would be interesting to  develop alternative approaches for the continuous monitoring of  multiple (conventional) reverse NN queries in spaces of higher  dimensionality.     
222	 A  
223	 CKNOWLEDGEMENTS    
224	 This work was supported by grant HKUST 6180/03E from Hong  Kong RGC. The authors would like to thank Kevin Di Filippo for  proof-reading the paper.   R EFERENCES    
225	 [B02]    Brinkhoff, T. A Framework for Generating Networkbased Moving Objects. GeoInformatica, (6)2: 153180, 2002.   
226	 [BJKS02]    Benetis, R., Jensen, C., Karciauskas, G., Saltenis, S.  Nearest Neighbor and Reverse Nearest Neighbor  Queries for Moving Objects. IDEAS, 2002.    
227	 [CHC04]   Cai, Y., Hua, K., Cao, G. Processing RangeMonitoring Queries on Heterogeneous Mobile  Objects. MDM, 2004.   
228	 [FSAA01]  Ferhatosmanoglu,  H., Stanoi, I., Agrawal, D.,  Abbadi, A. Constrained Nearest Neighbor Queries.  SSTD, 2001.   
229	 [GL04]    Gedik, B., Liu, L. MobiEyes: Distributed Processing  of Continuously Moving Queries on Moving Objects  in a Mobile System. EDBT, 2004.   
230	 [H84]   Henrich, A. A Distance Scan Algorithm for Spatial  Access Structures. ACM GIS, 1984.   
231	 [HS99]      Hjaltason,  G.,  Samet, H. Distance Browsing in  Spatial Databases. ACM TODS, 24(2): 265-318,  1999.   
232	 [KMS02]   Korn, F., Muthukrishnan, S. Srivastava, D. Reverse  Nearest Neighbor Aggregates Over Data Streams.  VLDB, 2002.   
233	 [KOTZ04]    Koudas, N., Ooi, B., Tan, K., Zhang, R.  Approximate NN queries on Streams with  Guaranteed Error/performance Bounds. VLDB, 2004.   
234	 [MXA04]   Mokbel, M., Xiong, X., Aref, W. SINA: Scalable  Incremental Processing of Continuous Queries in  Spatio-temporal Databases. SIGMOD, 2004.   
235	 [PSTM04]   Papadias, D., Shen, Q., Tao, Y., Mouratidis, K.  Group Nearest Neighbor Queries. ICDE, 2004.   
236	 [PXK+02]  Prabhakar, S., Xia, Y., Kalashnikov, D., Aref, W.,  Hambrusch, S. Query Indexing and Velocity  Constrained Indexing: Scalable Techniques for  Continuous Queries on Moving Objects. IEEE  Transactions on Computers, 51(10): 1124-1140,  2002.   
237	 [RKV95]      Roussopoulos,  N.,  Kelly, S., Vincent, F. Nearest  Neighbor Queries. SIGMOD, 1995.   
238	 [SR01]   Song, Z., Roussopoulos, N. K-Nearest Neighbor  Search for Moving Query Point. SSTD, 2001.   
239	 [SRAA01]  Stanoi, I., Riedewald, M., Agrawal, D., Abbadi, A.  Discovery of Influence Sets in Frequently Updated  Databases. VLDB, 2001.   
240	 [TP03]   Tao, Y.,  Papadias, D. Spatial Queries in Dynamic  Environments. ACM TODS, 28(2): 101-139, 2003.   
241	 [XMA05]  Xiong, X., Mokbel, M., Aref, W. SEA-CNN:  Scalable Processing of Continuous K-Nearest  Neighbor Queries in Spatio-temporal Databases.  ICDE</i>,<i> 2005.   
242	 [YPK05]   Yu, X., Pu, K., Koudas, N. Monitoring K-Nearest  Neighbor Queries Over Moving Objects. ICDE,  2005.    
