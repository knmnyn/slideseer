0	CMPE 521 Database Systems QUERY SENSITIVE EMBEDDINGS Vassilis Athitsos, Marios Hadjieleftheriou,  George Kollios and Stan Sclaroff İDRİS YILDIZ 19.10.2005 
1	Summary General Information Related Work Required Background Building Query Sensitive Embeddings Evaluation of Method Conclusion 
2	General Information Motivation of such a work Retrieve the most similar matches to a query object Nearest neighbor methods are used currently Existing solutions are too slow Especially in domains where objects are compared using computationally expensive similarity (or distance) measures Method offered by the paper: A novel method for approximating nearest neighbor retrieval for object spaces with expensive distance measures Embedding based : Maps objects to real vector space while preserving the proximity structure of original space Query sensitive (distance measure change according to query) measures while finding distances 
3	General Information (cont.) Usage: Estimating the properties of a biological sequence (like a protein, or DNA sequence) Nearest neighbor classification used by pattern recognition techniques For both cases a proximity measure is needed and this measure should be calculated easily. Numerous methods have been proposed for speeding up nearest-neighbor retrieval but all are for metric spaces (coordinate space).  Metric spaces have fixed dimensions. In case of variable dimensions existing methods do not help Query sensitive embeddings methods provide a way of nearest neighbor calculation for non-Euclidean (also non metric) spaces 
4	General Information (cont.) Method Summary Construct a function that maps objects into a real vector space Find query sensitive coefficients Measure the distance between objects by using the vector space calculated with L1 and L2 distances L1 or L2 over vectors should be more efficient than finding the original distance (usually holds) The results obtained (details later) The proposed method gives better results according to the two well-known existing embedding methods: BoostMap and FastMap 
5	Related Work Hashing and tree structures Similarity indexing in multi-dimensional datasets The performance degrades in high dimensions Rely on Euclidean or metric properties, problem in non-metric spaces Embedding based Lipschitz embeddings, FastMap, MetricMap, SparseMap, and BoostMap … Speed up nearest neighbor retrieval Construct an embedding, which maps objects into another space with a more efficient distance measure BoostMap Preserves the original space proximity with original space (after the embedding) The proposed method is an extension to BoostMap 
6	Required Background Embeddings Classifiers 
7	Embeddings 1D Embeddings r is called the reference object Dx is the (non-metric) distance between x and r Project x into the line x1x2 (Building block for FastMap) 1D Embeddings are considered weak classifiers (high error rate) 1D embeddings can be used as building blocks for constructing higher-dimensional embeddings Embedding Types: Fx1,x2 is used to construct FastMap Frs can be combined to form Lipschitz embeddings AdaBoost combines many weak classifiers into a strong classifier 
8	Embeddings (Cont.)  
9	Embeddings (Cont.) 
10	Classifiers Weak Classifiers Given three objects q, a, b ∈ X, is q closer to a or to b? Find exact distances Dx(q,a) and Dx(q,b) ! Find checking if F(q) is closer to F(a) or to F(b) where F is a d dimensional embedding F:XRd If the two methods stated above give the same result then F succeeds over (q,a,b) otherwise F fails over (q,a,b)  1D Embeddings act as weak classifiers because they have a high error rate High dimensional  Strong Classifiers can be built by using 1D Embeddings (Method of AdaBoost) Purpose of weak classifiers : Reducing problem to find a strong classifier by using weak classifiers 
11	Building Query Sensitive Embeddings Motivation for constructing Query Sensitive Embeddings Construction Method Training Algorithm Training Data 
12	Motivation for constructing Query Sensitive Embeddings Lack of contrasting: Two high-dimensional objects are unlikely to be very similar in all the dimensions. Statistical sensitivity: The data is rarely distributed, and for a pair of objects there only relatively few coordinates that are significant for comparing those objects. Query sensitive embeddings are required: Use weigths while finding distances Those weights will be query specific values 
13	Query Sensitive Embedding Example 
14	Query Sensitive Embedding (2) F(x) = (Fr1(x), Fr2(x), Fr3(x)) L1 distance is used to compare the embeddings of two objects 20 database objects (3 reference points), 10 query objects F fails on 23.5% of the 3800 triples (q, a, b) where q from query objects and a,b from database objects 1D embeddings Fr1, Fr2, Fr3 fail respectively on 39.2%, 36.4%, and 26.6% of the triples 
15	Query Sensitive Embedding (3) If q = q1 then for triples(q1, a, b)  Fr1 does better than F Fr1 fails on 5.8% of those triples F fails on 11.6% of those triples Similarly if q = q2 and q = q3 respectively, Fr2 and Fr3 are more accurate than F.  For query objects q1, q2, q3 it would be beneficial to use a query-sensitive weighted L1 measure, that would respectively use only the first, second, and third coordinate of F 
16	Construction Method Specify a large family of 1D embeddings Use 1D embeddings to define binary classifiers for estimating  for object triples (q, a, b) if q is closer to a or to b.  These classifiers are expected to be better than a random classifier  Run AdaBoost to combine many classifiers into a single classifier (H) H is significantly more accurate than the simple classifiers associated with 1D embeddings Use H to define a d-dimensional embedding Fout, and a query-sensitive weighted L1 distance measure Dout H is equivalent to the combination of Fout and Dout  For three objects q, a, b ∈ X, H predicts that q is closer to a than it is to b, then Fout(q) is closer to Fout(a) than it is to Fout(b) 
17	Construction Method (cont.) The classifier: 1  if q is closer to a -1  if q is closer to b -1  if q is in equal distance (D type) to a,b Classifier may fail if the reference is outside a region of the objects use Splitters The splitter (0,1): V is a specified region or a threhold  The Query Sensitive Classifer is combination of classifer and the splitter: 
18	Training Algorithm Usage: Constructing an embedding and a query sensitive distance measure AdaBoost algortihm is used as the training algortihm AdaBoost assumes that we have a “weak learner” module, which we can call at each round to obtain a new weak classifier.  The goal is to construct a strong classifier that achieves much higher accuracy than the individual weak classifiers. At each step algorithm simply determines the appropriate weight for each weak classifier, and then adjusts the training weights At each round mistakes from previous rounds are corected by some amount Some changes  and additions are done on the to adapt AdaBoost  to this case 
19	Training Data Xtr is a subset of X (object space) We need to compute Dx for all pairs of points in Xtr 
20	Evaluation of Method Complexity Analysis Embedding application: filter and refine retrieval Experiments 
21	Complexity Analysis At each training round measure m classifiers with t training triples in order to choose the best weak classifier The computational time per training round is O(mt) Before we even start the training algorithm, we need to compute distances Dx from: Every object in C to every object in C Every object in C to every object in Xtr C  the set of objects that we use to form 1D embeddings Xtr  the set of objects from which we form training triples We also need all distances between pairs of objects in Xtr Computing all those distances can sometimes be the most computationally expensive part of the algorithm, depending on the complexity of computing Dx Fortunately all those costs are just onetime preprocessing costs 
22	Complexity Analysis (cont.) Computing the d-dimensional embedding of a query object takes O(d) time Also it requires O(d) evaluations of Dx  Result  Comparing the embedding of the query to the embeddings of n database objects takes time O(dn) For a fixed d, these costs are similar to FastMap, SparseMap, and MetricMap Incase of dynamic data change no performance penalty will be required until the underlying database structure changes (after too many inserts and deletions) 
23	Embedding application: filter and refine retrieval In applications where we are interested in retrieving the k nearest neighbors for a query object q, a d-dimensional embedding F can be used in a filter-and-refine framework Compute and store vector F(x) for every database object x (offline step) For any new query q perform following: Embedding step: compute F(q), by measuring the distances between q and the reference objects and/or pivot objects used to define F Filter step: Find the database objects whose associated vectors are the p most similar vectors to F(q) Refine step: sort those p candidates by evaluating the exact distance DX between q and each candidate The filter step discards most database objects by measuring distances between vectors The refine step applies Dx only to the top p candidates 
24	Experiments Comparison of  the proposed method to the original BoostMap method and FastMap will be done We used two different datasets: the MNIST and a time series dataset MNIST: Dataset of handwritten digits (from 0 to 9) Shape Context Distance as the exact distance measure A training set of 60,000 image Time Series: Dynamic Time Warping as the exact distance measure 32,768 various sequences are obtained Both Shape Context Distance and Dynamic Time Warping  are very hard to compute 
25	Experiments (Cont) Parameter “k” will be used in experiments as the number of k-neighbor.  Algorithms: FastMap Ra-QI  Original BoostMap Se-QS  Proposed Method Se-QI  Intermediate method (which incorporates our method of choosing training triples, but still constructs a query-insensitive embedding) We will try to achieve all k-neighbours with minimum number of comparisons 
26	Experiments (MNIST- %90 accuracy) 
27	Experiments (MNIST- %95 accuracy) 
28	Experiments (Time Serials- %90 accuracy) 
29	Experiments (Time Serials- %95 accuracy) 
30	Conclusion The main difference of the proposed method with respect to existing embedding methods is that it constructs a query sensitive distance measure Such a distance measure captures the fact that different embedding coordinates are important for different queries, and thus leads to improved retrieval accuracy and speed by using smaller number of comparisions Embeddings is a family of methods that is not domain specific and can be applied for efficient retrieval in non-metric spaces 
31	Thanks... 
