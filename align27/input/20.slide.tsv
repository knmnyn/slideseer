0	Summarizing and mining  inverse distributions on data streams via dynamic inverse sampling Graham Cormode cormode@bell-labs.com S. Muthukrishnan  muthu@cs.rutgers.edu Irina Rozenbaum  rozenbau@paul.rutgers.edu Presented by  
1	Outline Defining and motivating the Inverse Distribution Queries and challenges on the Inverse Distribution Dynamic Inverse Sampling to draw sample from Inverse Distribution Experimental Study 
2		Data Streams & DSMSs Numerous real world applications generate data streams: IP network monitoring	– financial transactions  click streams		– sensor networks Telecommunications	– text streams at application level, etc. Data streams are characterized by massive data volumes of transactions and measurements at high speeds. Query processing is difficult on data streams: We cannot store everything, and must process at line speed. Exact answers to many questions are impossible without storing everything We must use approximation and randomization with strong guarantees. Data Stream Management Systems (DSMS) summarize streams in small space (samples and sketches). 
3	DSMS Application: IP Network Monitoring Needed for: network traffic patterns identification intrusion detection reports generation, etc. IP traffic stream:  Massive data volumes of transactions and measurements:  over 50 billion flows/day in AT&T backbone. Records arrive at a fast rate: DDoS attacks  - up to 600,000 packets/sec Query examples: heavy hitters change detection quantiles Histogram summaries 
4		Forward and Inverse Views Problem A.  Which IP address sent the  most bytes?  That is , find i such that  ∑p|ip=i  sp is maximum. Forward distribution. Problem B.  What is the most common  volume of traffic sent by an  IP address?  That is , find traffic volume W  s.t |{i|W = ∑p|ip=i sp}|  is  maximum. Inverse distribution. Consider the IP traffic on a link as packet p representing (ip, sp) pairs where ip  is a source IP address and sp is a size of the packet. 
5		The Inverse Distribution If f is a discrete distribution over a large set X, then inverse distribution,  f-1(i), gives fraction of items from X with count i. Inverse distribution is f-1[0…N],                f-1(i)  = fraction of IP addresses which sent i bytes.	                 = |{ x : f(x) = i, i-¹≠0}|/|{x : f(x)-¹≠0}|    	              F-1(i)  = cumulative distribution of f-1	                          = ∑j > i f-1(j)   [sum of f-1(j) above i] Fraction of IP addresses which sent  < 1KB of data = 1 – F-1(1024)  Most frequent number of bytes sent = i s.t. f-1(i) is greatest Median number of bytes sent = i s.t. F-1(i) = 0.5 
6	Queries on the Inverse Distribution Particular queries proposed in networking map onto f-1,  f-1(1) (number of flows consisting of a single packet) indicative of network abnormalities / attack [Levchenko, Paturi, Varghese 04] Identify evolving attacks through shifts in Inverse Distribution [Geiger, Karamcheti, Kedem, Muthukrishnan 05] Better understand resource usage:  what is dbn. of customer traffic?  How many customers < 1MB bandwidth / day?  How many use 10 – 20MB per day?, etc. Histograms/ quantiles on inverse distribution. Track most common usage patterns, for analysis / charging requires heavy hitters on Inverse distribution Inverse distribution captures fundamental features of the distribution, has not been well-studied in data streaming.  
7	Forward and Inverse Views on IP streams Forward distribution: Work on f[0…U] where f(x) is the number of bytes sent by IP address x. Each new packet (ip, sp) results in f[ip] ←f[ip] + sp. Problems: f(i) = ? which f(i) is the largest? quantiles of f ? Inverse distribution: Work on f--1[0…K] Each new packet results in       f−1[f[ip]]←f−1[f[ip]] − 1 and       f−1[f[ip] + sp]← 		  f−1[f[ip] + sp]+1. Problems: f−1(i) = ? which  f−1(i) is the largest? quantiles of f−1 ? Consider the IP traffic on a link as packet p representing (ip, sp)  pairs where ip is a source IP address and sp is a size of the packet. 
8	Inverse Distribution on Streams: Challenges I If we have full space, it is easy to go between forward and inverse distribution.  But in small space it is much more difficult, and existing methods in small space don’t apply. Find f(192.168.1.1) in small space, with query give a priori – easy: just count how many times the address is seen. Find f-1(1024) – is provably hard (can’t find exactly how many IP addresses sent 1KB of data without keeping full space). 7/7 6/7 5/7 4/73/7 2/7 1/7 F-1(x) 1   2   3   4   5 f -1(x) i 1   2   3   4   5 3/7 2/7 1/7 i f(x) x 5 4 3 2 1 
9	Inverse Distribution on Streams: Challenges II, deletions How to maintain summary in presence of insertions and deletions?  Insertions onlyupdates sp > 0 Stream of arrivals Can sample original  distribution estimated  distribution ? ? Insertions and Deletions updates sp can be arbitrary original  distribution estimated  distribution Stream of arrivals and departures + - How to summarize? 
10	Our Approach: Dynamic Inverse Sampling Many queries on the forward distribution can be answered effectively by drawing a sample. Draw an x so probability of picking x is f(x) / ∑y f(y) Similarly, we want to draw a sample from the inverse distribution in the centralized setting.   draw (i,x) s.t. f(x)=i, i≠0 so probability of picking i is f-1(i) / ∑j f-1(j) and probability of picking x is uniform. Drawing from forward distribution is “easy”: just uniformly decide to sample each new item (IP address, size) seen Drawing from inverse distribution is more difficult, since probability of drawing (i,1) should be same as (j,1024) 
11	Dynamic Inverse Sampling: Outline Data structure split into levels For each update (ip, sp): compute hash l(ip) to a level in the data structure.  Update counts in level l(ip) with ip and sp x count unique x … … M Mr Mr2 Mr3 0 l(x) At query time:  probe the data structure to return (ip, S sp) where ip is sampled uniformly from all items with non-zero count Use the sample to answer the query on the inverse distribution.  
12	Hashing Technique Use hash function with exponentially decreasing distribution: Let h be the hash function and r is an appropriate const < 1           Pr[h(x) = 0] = (1-r) 	     Pr[h(x) = 1] = r (1-r)                        … 	     Pr[h(x) = l] = rl(1-r) Track the following information as updates are seen: x: Item with largest hash value seen so far unique: Is it the only distinct item seen with that hash value? count: Count of the item x Easy to keep (x, unique, count) up to date for insertions only x count unique x … … M Mr Mr2 Mr3 0 l(x) Challenge: How to maintain in presence of deletes? 
13	Collision Detection: inserts and deletes sum count  x … … M Mr Mr2 Mr3 0 l(x) coll. detection 16   8    4    2    1 0 1 update output insert 13 13 1 +1 +1 +1 +1 +1 13/1=13 insert 13 26 2 +2 +2 +2 +2 +2 26/2=13 insert 7 33 3 +3 +3 +3 +1 +1 collision delete 7 26/2=13 Level 0 Simple: Use approximate distinct element estimation routine. 
14	Outline of Analysis Analysis shows: if there’s unique item, it’s chosen uniformly from set of items with non-zero count. Can show whatever the distribution of items, the probability of a unique item at level l is at least constant Use properties of hash function: only limited, pairwise independence needed (easy to obtain) Theorem: With constant probability, for an arbitrary sequence of insertions and deletes, the procedure returns a uniform sample from the inverse distribution with constant probability.  Repeat the process independently with different hash functions to return larger sample, with high probability. Level l 
15	Application to Inverse Distribution Estimates Overall Procedure:  Obtain the distinct sample from the inverse distribution of size s; Evaluate the query on the sample and return the result. Median number of bytes sent: find median from sample The most common volume of traffic sent: find the most common from sample What fraction of items sent i bytes: find fraction from the sample Example:    Median is bigger than ½ and smaller than ½ the values.   Answer has some error: not ½, but (½ ± e) Theorem:  If sample size s = O(1/e2 log 1/d) then answer from the sample is  between (½-e) and (½+e) with probability at least 1-d. Proof follows from application of Hoeffding’s bound. 
16	Experimental Study Data sets: Large sets of network data drawn from HTTP log files from the 1998 World Cup Web Site (several million records each) Synthetic data set with 5 million randomly generated distinct items Used to build a dynamic transactions set with many insertions and deletions (DIS) Dynamic Inverse Sampling algorithms – extract at most one sample from each data structure (GDIS) Greedy version of Dynamic Inverse Sampling – greedily process every level, extract as many samples as possible from each data structure (Distinct) Distinct Sampling (Gibbons VLDB 2001) draws a sample based on a coin-tossing procedure using a pairwise-independent hash function on item values 
17	Sample Size vs. Fraction of Deletions Desired sample size is 1000. X = 80%  Distinct ≈ 12%   DIS ≈ 100% X = 99%   Distinct < 1%   DIS ≈ 100% 
18	Returned Sample Size  X = 100  Distinct – 45%   DIS – 99%   GDIS ≈ 5 items per     data structure X = 1000   Distinct – 30%   DIS – 95%   GDIS ≈ 5 items per     data structure Experiments were run on the client ID attribute of the HTTP log data. 50% of the inserted records were deleted. 
19	Sample Quality Inverse range query:  Compute the fraction of records with size greater than i=1024 and compare it to the exact value computed offline Inverse quantile query:  Estimate the median of the inverse distribution using the sample and measure how far was the position of the returned item i from 0.5. 
20	Related Work Distinct sampling under insert only: Gibbons: Distinct Sampling, VLDB 2002. Datar and Muthukrishnan: Rarity and similarity, ESA 2002. Distinct sampling under deletes also: Frahling, Indyk, Sohler: Dynamic geometric streams, STOC 2005. Ganguly, Garofalakis, Rastogi: Processing Set Expressions over Continuous Update Streams, SIGMOD 2003. Inverse distributions: Has recently informally appeared in networking papers. 
21	Conclusions We have formalized Inverse Distributions on data streams and introduced Dynamic Inverse Sampling method that draws uniform samples from the inverse distribution in presence of insertions and deletions. With a sample of size O(1/ε2), can answer many queries on the inverse distribution (including point and range queries, heavy hitters, quantiles)  up to additive approximation of ε. Experimental study shows that proposed methods can work at high rates and answer queries with high accuracy Future work:  Incorporate in data stream systems Can we also sample from forward dbn under inserts and deletes? 
22	
23	Future Work Incorporate Inverse Distribution into Data Stream Management System  Development of sampling techniques from forward distribution under inserts and deletes 
24	DIS (example of insertions) We consider the following example sequence of insertions of items: 		Input: 4, 7, 4, 1, 3, 4, 2, 6, 4, 2 Suppose these hash to levels in an instance of our data structure as follows: x l(x) 1   2   3   4   5   6   7   8 1   3   2   1   1   1   2   1 Time                   Level 1                       Level 2                           Level 3 Step             item count unique       item count unique          item count unique 1.                   4        1       T                0       0       T                   0       0      T 2.                   4        1       T                7       1       T                   0       0      T 3.                   4        2       T                7       1       T                   0       0      T 4.                   4        3       F                7       1       T                   0       0      T 5.                   4        3       F                7       2       F                   0       0      T 6.                   4        4       F                7       2       F                   0       0      T 7.                   4        4       F                7       2       F                   2       1      T 8.                   4        5       F                7       2       F                   2       1      T 9.                   4        6       F                7       2       F                   2       1      T 10.                 4        6       F                7       2       F                   2       2      T (count,item) (1,4) (1,4)(2,7) (1,4)(2,7) (2,7) (1,2) (1,2) (1,2) (2,2) 
25	Inverse Distribution on Streams: Challenges II, deletions Insertions onlyupdates sp > 0 Stream of arrivals Can sample ? ? Insertions and Deletions updates sp can be arbitrary original distribution estimated distribution original distribution estimated distribution Maintain summary in presence of insertions and deletions?  How to summarize? Stream of arrivals and departures + - 
26	Sampling Insight Each distinct item x contributes to one pair (i,x)  Need to sample uniformly from these pairs. Basic insight: sample uniformly from the items x and count how many times x is seen to give (i,x) pair that has correct i and is uniform.  How to pick x uniformly before seeing any x? Use a randomly chosen hash function on each x to decide whether to pick it (and reset count). f(x) x 5 4 3 2 1 f -1(x) i 1   2   3   4   5 3/7 2/7 1/7 
27	Hashing Analysis Theorem: If unique is true, then x is picked uniformly.            Probability of unique being true is at least a constant. (For right value of r, unique is almost always true in practice) Proof outline: Uniformity follows so long as hash function h is at least                         pairwise independent. Hard part is showing that unique is true with constant prob. Let D is number of distinct items.  Fix l so 1/r · Drl · 1/r2. In expectation, Drl items hash to level l or higher Variance is also bounded by Drl, and we ensure 1/r2 · 3/2. Analyzing, can show that there is constant probability that there are either 1 or 2 items hashing to level l or higher.  
28	13 26 33 Collision Detection: inserts and deletes sum count  x … … M Mr Mr2 Mr3 0 l(x) coll. detection 16      8       4      2      1 0 1 update output insert 13 1 +1 +1 +1 +1 +1 13/1=13 insert 13 2 +2 +2 +2 +2 +2 26/2=13 insert 7 3 +3 +3 +3 +1 +1 collision delete 7 26/2=13 Level 0 Simple: Use approximate distinct element estimation routine. 
29	Collision Detection: inserts and deletes sum count  x … … M Mr Mr2 Mr3 0 l(x) coll. detection 16   8    4    2    1 0 1 update output insert 13 13 1 +1 +1 +1 +1 +1 13/1=13 insert 13 26 2 +2 +2 +2 +2 +2 26/2=13 insert 7 33 3 +3 +3 +3 +1 +1 collision delete 7 26/2=13 Level 0 Simple: Use approximate distinct element estimation routine. 
30	Hashing Analysis If only one item at level l, then unique is true If two items at level l or higher, can go deeper into the analysis and show that (assuming there are two items) there is constant probability that they are both at same level. If not at same level, then unique is true, and we recover a uniform sample. Probability of failure is p = r(3+r)/(2(1+r)). Number of levels is O(log N / log 1/r) Need 1/r > 1 so this is bounded, and 1/r2 ¸ 3/2 for analysis to work End up choosing r = p(2/3), so p is < 1  Level l 
31	Collision Detection (cont’d) Deterministic (previous slide): Suppose |X| = m = 2b  so each x X is represented as a b bit integer. We can keep 2b counters c[j,k] indexed by j=1…b and k {0,1}. Probabilistic: Draw t hash functions, g1….gt, which map items uniformly onto {0,1}, and a set of t×2 counters c[j,k]. Heuristic: Compute q new hash functions gj[x] mapping items x onto 0…m, and take the summation of g(x) as sumg[j, l(x)]. 
32	Sample Size This process either draws a single pair (i,x), or may not return anything. In order to get a larger sample with high probability, repeat the same process in parallel over the input with different hash functions h1 … hs to draw up to s samples (ij,xj) Let e = p(2 log (1/d)/s).  By Chernoff bounds, if we keep S = (1+2e) s/(1 – p) copies of the data structure, then we  recover at least s samples with probability at least 1-d Repetitions are a little slow — for better performance, keeping the s items with the s smallest hash values is almost uniform, and faster to maintain.   
33	Evaluation Output process:        At each level: If level is not empty, check whether there was a collision at the level If no collision, extract item from the level 
34	Experimental Study 5 more slides 
35	Key features of existing sampling methods 
