0	 Boston University Computer Science Tech. Report No. 2005-010, March 21, 2005. To appear in Proceedings of ACM International Conference on Management of Data (SIGMOD), June 2005.  
1	 Query-Sensitive Embeddings  
2	 Vassilis Athitsos Marios Hadjieleftheriou George Kollios Stan Sclaroff  
3	 Computer Science Department Boston University 111 Cummington Street Boston, MA 02215, USA {athitsos,marioh,gkollios,sclaroff}@cs.bu.edu  
4	 ABSTRACT  
5	 A common problem in many types of databases is retrieving the most similar matches to a query object. Finding those matches in a large database can be too slow to be practical, especially in domains where objects are compared using computationally expensive similarity (or distance) measures. This paper proposes a novel method for approximate nearest neighbor retrieval in such spaces. Our method is embedding-based, meaning that it constructs a function that maps objects into a real vector space. The mapping preserves a large amount of the proximity structure of the original space, and it can be used to rapidly obtain a short list of likely matches to the query. The main novelty of our method is that it constructs, together with the embedding, a query-sensitive distance measure that should be used when measuring distances in the vector space. The term "querysensitive" means that the distance measure changes depending on the current query object. We report experiments with an image database of handwritten digits, and a time-series database. In both cases, the proposed method outperforms existing state-of-the-art embedding methods, meaning that it provides significantly better trade-offs between efficiency and retrieval accuracy.  
6	 1. INTRODUCTION  
7	 Many important applications require identifying, in a large database, the most similar matches to a query object. For example, a common way of estimating the properties of a biological sequence (like a protein, or DNA sequence) is by identifying its closest matches in a large database of known sequences. As another example, nearest neighbor classification is a widely used pattern recognition technique, in which we classify an object by assigning to it the class of its closest match in a database of training objects.  
8	 This work was supported by NSF grants IIS-0308213 and IIS-0133825, and by ONR grant N00014-03-1-0108.  
9	 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMOD 2005 June 14-16, 2005, Baltimore, Maryland, USA. Copyright 2005 ACM 1-59593-060-4/05/06 $ 5.00.  
10	 Although numerous indexing methods have been proposed for speeding up nearest-neighbor retrieval [6], such methods typically assume that we are operating in a Euclidean space, or a metric space, or a so-called "coordinate space," where each object is represented as a feature vector of fixed dimensions. In many actual applications these assumptions are not obeyed, because we need to use distance measures that are non-Euclidean and even non-metric (meaning that even general indexing methods for metric spaces, like vp-trees and variants [8, 18, 38, 40] cannot be applied), and because the objects are not of fixed dimensionality. Examples of computationally expensive non-Euclidean distance measures include the Kullback-Leibler distance for matching probability distributions, Dynamic Time Warping for matching time series, or the edit distance for matching strings and biological sequences. It is important to design efficient methods for nearest neighbor retrieval in such spaces.  
11	 This paper proposes a novel method for approximate nearest neighbor retrieval in such non-Euclidean spaces. Our method is embedding-based, meaning that it constructs a function that maps objects into a real vector space. This mapping preserves a large amount of the proximity structure of the original space, meaning that nearby objects tend to get mapped to nearby vectors. At the same time, measuring distances between vectors (using a weighted L 1 distance measure) can be orders-of-magnitude faster than comparing objects in the original space.  
12	 The main novelty of our method is that it constructs, together with the embedding, a "query-sensitive" distance measure that should be used when measuring distances in the vector space. The term "query-sensitive" means that the distance measure changes depending on the current query object. In particular, the weights used for the L 1 distance measure automatically adjust to each query. Using a querysensitive measure is a natural way to capture the fact that, as described in Sec. 4, given a query object, some coordinates of the embedding are more informative than other coordinates. In general, query-sensitive distance measures provide a solution to an important issue that arises when objects are represented as high-dimensional vectors: the need to identify, for any two objects, the coordinates that are really important for comparing those objects [1].  
13	 Our formulation uses a recent technique for constructing embeddings using machine learning, that was introduced in the BoostMap embedding algorithm [2]. The key novelty of the proposed method is that our algorithm produces an embedding and a query-sensitive distance measure, with a  
14	 well-defined mechanism for adjusting the distance measure to each query object. Existing embedding methods, including the original BoostMap algorithm, produce a global distance measure. In the datasets we have experimented with, a query-sensitive distance measure leads to much better retrieval performance.  
15	 A secondary contribution in this paper is a method for choosing training data for the learning algorithm. The original BoostMap algorithm uses as training data random triples of objects from the original space. We propose a more selective method for choosing training data, that leads to embeddings that are better optimized for retrieval accuracy. The method we propose is very simple, but it leads to significant improvement in the experimental results.  
16	 The proposed method is experimentally compared to the original BoostMap algorithm, as well as to FastMap [12], which is a well-known existing embedding method. Experiments are performed on two datasets: the MNISTdatabase of handwritten digits [22], with Shape Context Distance [4] as the underlying distance measure, and a time-series database [32] with constrained Dynamic Time Warping as the underlying distance measure. In both datasets, the algorithm described in this paper yields superior performance with respect to both the original BoostMap method and FastMap. For a fixed budget of exact distance computations per query, and for different integers k, the new method correctly retrieves all k nearest neighbors for a significantly higher fraction of queries.  
17	 2. RELATED WORK  
18	 Various methods have been employed for similarity indexing in multi-dimensional datasets, including hashing and tree structures [6, 8, 18, 36, 38]. However, the performance of such methods degrades in high dimensions. This phenomenon is one of the many aspects of the "curse of dimensionality". Another problem with tree-based methods is that they typically rely on Euclidean or metric properties, and those properties do not hold in non-metric spaces.  
19	 Approximate nearest neighbor methods have been proposed in [17] and scale better with the number of dimensions. However, those methods are available only for specific sets of metrics, and they are not applicable to arbitrary distance measures. In [13], a randomized procedure is used to create a locality sensitive hashing structure that can report a (1 + )-approximate nearest neighbor with a constant probability. In [40] M-trees are used for approximate similarity retrieval, while [23] proposes clustering the dataset and retrieving only a small number of clusters (which are stored sequentially on disk) to answer each query. In [9, 11, 19] dimensionality reduction techniques are used where lowerbounding rules are ignored when dismissing dimensions and the focus is only on preserving close approximations of distances. In [34] the authors used VA-files [35] to find nearest neighbors by omitting the refinement step of the original exact search algorithm and estimating approximate distances using only the lower and upper bounds computed by the filtering step. Finally, in [30] the authors partition the data space into clusters and then the representatives of each cluster are compressed using quantization techniques. Other similar approaches include [21, 26]. However, all these techniques can be employed mostly for distance functions defined using L p norms.  
20	 Various techniques appeared in the literature for robust evaluation of similarity queries on time-series databases when using non-metric distance functions [20, 32, 37]. These techniques use the filter-and-refine approach, where a computationally efficient approximation of the original distance is utilized in the filtering step. Query speedup is achieved by pruning a large part of the search space at the filter step. Then, the original, accurate but more expensive distance measure is applied to the few remaining candidates, during the refinement step. Usually, the distance approximation function is designed to be metric (even if the original distance is not), so that traditional indexing techniques can be applied to index the database in order to speed up the filtering stage as well. In our experimental evaluation we compare our approach with the technique presented in [32].  
21	 Also related to our setting is work on distance-based indexing for string similarity. In [25] special modifications to distance-based indices [8, 18, 38] are proposed for indexing distance functions that are almost metric. However, unlike our method, the technique of [25] cannot be applied to general distance functions.  
22	 In domains where the distance measure is computationally expensive, significant computational savings can be obtained by constructing a distance-approximating embedding, which maps objects into another space with a more efficient distance measure. A number of methods have been proposed for embedding arbitrary spaces into a Euclidean or pseudoEuclidean space [2, 7, 12, 16, 24, 28, 33, 39]. Some of these methods, in particular MDS [39], Bourgain embeddings [7, 15], LLE [24] and Isomap [28] are not targeted at speeding up online similarity retrieval, because they still need to evaluate exact distances between the query and most or all database objects. Online queries can be efficiently handled by Lipschitz embeddings [15], FastMap [12], MetricMap [33], SparseMap [16], and BoostMap [2].  
23	 Embedding methods designed for speeding up nearest neighbor retrieval [2, 12, 15, 16, 33] have two attractive properties: first, they can compute the embedding of a new query object by comparing that object to a relatively small subset of all database objects; second, they are formulated in a domainindependent way, and they can be applied to any space and distance measure (unlike techniques like [13, 25], for example, whose formulation cannot handle arbitrary spaces). At the same time, when applied to arbitrary spaces, there is no guarantee that these methods will attain some acceptable tradeoff between accuracy and efficiency.  
24	 The method proposed in this paper belongs to the same family of approaches as [2, 12, 15, 16, 33]; it tries to solve the same problem (efficient nearest neighbor retrieval), and it can be applied to arbitrary spaces and distance measures. The proposed method can be seen as an extension of BoostMap [2]. The main advantage of BoostMap is that it optimizes a measure of embedding quality that is directly related to how well the embedding preserves the similarity structure of the original space. A secondary contribution of this paper is that it shows how to reformulate this measure of embedding quality so that it is more tightly related to the task of nearest neighbor retrieval. The main contribution consists of showing how to extend the BoostMap algorithm so that it produces, together with the embedding, a query-sensitive distance measure. Given a query object, the query-sensitive distance measure assigns higher weights to the embedding coordinates that are important for that query.  
25	 Query-sensitive distance measures have been used in [10,  
26	 14] to improve the classification accuracy of nearest neighbor classifiers. In these methods, it is assumed that an initial global (query-insensitive) distance measure is available. Given a query object, the initial distance measure is iteratively refined. In contrast, in this paper we formulate a method that constructs an embedding and a query-sensitive distance measure for speeding up nearest neighbor retrieval. Given a query object, the query-sensitive distance measure is constructed in a non-iterative way, and no initial distance measure is given to our algorithm.  
27	 3. BACKGROUND  
28	 We use X to denote a set of objects, and D X (x 1 , x 2 ) to denote a distance measure between objects x 1 , x 2  X. For  
29	 example, X can be a set of images of handwritten digits (Fig. 3), and D X can be shape context matching as defined in [5]. However, any X and D X can be plugged into the formulations described in this paper.  
30	 First, we will define some simple embeddings, and then we will briefly describe the association between embeddings and classifiers that was introduced in [2].  
31	 3.1 Some Simple Embeddings  
32	 An embedding F : X  R d is a function that maps any object x  X into a d-dimensional vector F (x)  R d . Distances in R d are measured using the Euclidean (L 2 ) metric, or some other L p metric. It is assumed that measuring a single L p distance between two vectors is significantly faster than measuring a single distance D X between two objects of X. This assumption is obeyed in the example datasets we used in our experiments. For example, with our PC we can measure close to a million L 1 distances between highdimensional vectors in R 100 in one second, whereas only 15 shape context distances can be evaluated per second.  
33	 A simple way to define one-dimensional (1D) embeddings is using prototypes [15]. In particular, given an object r  X, we can define an embedding F r : X  R as follows:  
34	 F r (x) = D X (x, r) . (1)  
35	 The prototype r that is used to define F r is typically called a reference object or a vantage object [15]. The intuition behind embeddings of type F r is simple: if two objects x 1 and x 2 are very similar to each other, we expect their distances to r, i.e., D X (x 1 , r) and D X (x 2 , r) to also be similar. Therefore, F r is expected to map similar objects to nearby points on the real line.  
36	 Another family of simple, 1D embeddings is proposed in [12] and used as building blocks for FastMap. The idea is to choose two objects x 1 , x 2  X, called pivot objects, and  
37	 then, given an arbitrary x  X, to define the embedding F x 1 ,x 2 of x to be the projection of x onto the "line" x 1 x 2 :  
38	 F x 1 ,x 2 (x) = D X (x, x 1 ) 2 + D X (x 1 , x 2 ) 2 - D X (x, x 2 ) 2  
39	 2D X (x 1 , x 2 ) . (2)  
40	 The reader can find in [12] an intuitive geometric interpretation of this equation, based on the Pythagorean theorem.  
41	 These simple 1D embeddings can be used as building blocks for constructing higher-dimensional embeddings. Embeddings of type F x 1 ,x 2 are used to construct FastMap [12]. Embeddings of type F r can be combined to form Lipschitz embeddings [15]. 3.2 Associating Embeddings with Classifiers  
42	 Let F : X  R d be a d-dimensional embedding. F acts as a classifier for the following binary classification problem: given three objects q, a, b  X, is q closer to a or to b? If we know F , but we do not know the exact distances D X (q, a) and D X (q, b), we can provide an answer by simply checking if F (q) is closer to F (a) or to F (b). If that answer is wrong, we say that embedding F fails on triple (q, a, b). If the answer is correct, we say that embedding F succeeds on triple (q, a, b). If F succeeds on all triples, then F can be used to correctly identify the true nearest neighbors for all queries.  
43	 Simple, 1D embeddings, like the ones we defined above, are expected to act as weak classifiers [2, 27], i.e., they will probably have a high error rate, but at the same time they should provide answers that are, on average, more accurate than a random guess, which would have an error rate of 50%. In other words, we expect that a 1D embedding will fail on many triples (q, a, b), but it will succeed on more than half of all possible triples.  
44	 The key insight in [2] is that, by associating embeddings with classifiers, we can reduce the problem of embedding construction to the problem of combining many weak classifiers into a strong classifier. The latter problem of combining weak classifiers has been extensively studied in the machine learning community, and a well-known and widely used solution to that problem is the AdaBoost algorithm [27]. The BoostMap algorithm [2] essentially uses AdaBoost to construct a high-dimensional embedding out of 1D embeddings of type F r and F x 1 ,x 2 . The BoostMap algorithm uses a training set S of triples (q, a, b), picked randomly from the available training objects, with the constraint that q is closer to a than to b. The algorithm constructs an embedding F : X  R d in a way that minimizes the fraction of triples (q, a, b)  S on which the embedding F fails, i.e., triples (q, a, b) for which F maps q closer to b than to a.  
45	 4. MOTIVATION FOR QUERY-SENSITIVE  
46	 DISTANCE MEASURES  
47	 In the experiments reported in [2], it is shown that it is often beneficial to generate, using BoostMap, a highdimensional embedding, with over 100 dimensions. As pointed out in [1], finding nearest neighbors in a high-dimensional space raises the following issues:  
48	 · Lack of contrasting: Two high-dimensional objects  
49	 are unlikely to be very similar in all the dimensions.  
50	 · Statistical sensitivity: The data is rarely uniformly  
51	 distributed, and for a pair of objects there may be only relatively few coordinates that are statistically significant for comparing those objects.  
52	 Figure 1 illustrates the problem of statistical sensitivity. In that toy example, we define a three-dimensional embedding of the 2D plane using three reference objects. For some query objects, sometimes a single coordinate is sufficient for getting near-perfect retrieval results. In particular, if for a given query object q there is a reference object r really close to q, then using the 1D embedding F r by itself might give more accurate results than using the high-dimensional embedding. Figure 1 does not illustrate the problem of lack of contrasting, but that problem can also be present if the original distance measure D X is not metric: then, it is possible  
53	 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0 0.2 0.4 0.6 0.8 1  
54	 database points reference points query points 2 r 3 r  
55	 r 1 q 1 q 2 q 3  
56	 Figure 1: A toy example illustrating the use of querysensitive embeddings. Our space is the set of points in the unit square [0 , 1] × [0, 1]. There are twenty database objects, three of which (indicated as r 1 , r 2 , r 3 )are selected as reference objects. Using these reference objects, we define embedding F (x) = (F r 1 ( x), F r 2 ( x), F r 3 ( x)), and we use the L 1 distance to compare the embeddings of two objects. There are ten query objects, three of which are marked as q 1 , q 2 , q 3 . F fails on 23.5% of the 3800 triples  
57	 ( q, a, b) we can form by picking q from the query objects, and the pair a, b from the database objects. In con 
58	 trast, the 1D embeddings F r 1 , F r 2 , F r 3 fail respectively on 39 .2%, 36.4%, and 26.6% of the triples. However, if we restrict our attention to triples ( q, a, b) where q = q 1 , F r 1 does better than F : F r 1 fails on 5 .8% of those triples,  
59	 whereas F fails on 11.6% of those triples. Similarly, for q = q 2 and q = q 3 respectively, F r 2 and F r 3 are more accurate than F . Therefore, for query objects q 1 , q 2 , q 3 , it would be beneficial to use a query-sensitive weighted L 1 measure, that would respectively use only the first, second, and third coordinate of F .  
60	 for two objects x 1 and x 2 to be very close to each other, but have very different distances D X (x 1 , r) and D X (x 2 , r) to a reference object r.  
61	 To address these problems, we propose to construct, together with the embedding, a query-sensitive distance measure. By "query-sensitive" we mean that the weights used for the weighted L 1 distance (used to measure distances between embeddings of objects) will not be fixed; instead, they will depend on the query object. Figure 1 illustrates how using a query-sensitive distance measure can give better retrieval accuracy. Overall, a query-sensitive distance measure provides a principled way to address the problems described in [1], by putting more emphasis on coordinates that are more important for a particular query.  
62	 5. CONSTRUCTING AN EMBEDDING AND  
63	 A QUERY-SENSITIVE DISTANCE MEA 
64	 SURE  
65	 At a high level, our method constructs an embedding using the following steps: 1. We start by specifying a large family of 1D embeddings, using well-known definitions from prior embedding methods.  
66	 2. We use 1D embeddings to define binary classifiers, which estimate for object triples (q, a, b) if q is closer to a or to b. These classifiers are expected to be pretty inaccurate, but still better than a random classifier (which would just guess randomly all the time, and therefore would have a 50% error rate).  
67	 3. We run AdaBoost to combine many classifiers into a single classifier H, which we expect to be significantly more accurate than the simple classifiers associated with 1D embeddings.  
68	 4. We use H to define a d-dimensional embedding F out , and a query-sensitive weighted L 1 distance measure D out . It is shown that H is mathematically equivalent to the combination of F out and D out : if, for three objects q, a, b  X, H predicts that q is closer to a than it is to b, then, under distance measure D out , F out (q) is closer to F out (a) than it is to F out (b).  
69	 The main difference between the proposed approach and the original BoostMap algorithm is introduced in the second step. Using 1D embeddings, we will define binary classifiers of a different type than what was used in [2]. We will then show how using those classifiers as building blocks results in constructing a query-sensitive distance measure. These steps are explained in detail in the remainder of this section.  
70	 5.1 Defining Query-Sensitive Classifiers from 1D Embeddings  
71	 As described earlier, every embedding F corresponds to a classifier that classifies triples (q, a, b) of objects in X. Formally, we can say that a triple (q, a, b) is of type 1 if q is closer to a than to b, type 0 if q is equally close to a and b, and type -1 if q is closer to b than to a. Given embedding F , and a distance measure D for comparing vectors, we can define the classifier ~ F associated with embedding F as follows:  
72	 ~ F (q, a, b) = D(F (q), F (b)) - D(F (q), F (a)) . (3)  
73	 The sign of ~ F (q, a, b) is an estimate of whether triple (q, a, b) is of type 1, 0, or -1. We should note that, if F is a 1D embedding, then ~ F (q, a, b) = |F (q) - F (b)| - |F (q) - F (a)|.  
74	 Sometimes, ~ F may do a really good job on triples (q, a, b) when q is in a specific region, but at the same time it may be beneficial to ignore ~ F when q is outside that region. For example, suppose that we have an embedding F r defined using reference object r. If q = r, then ~ F r will classify correctly all triples (q, a, b), where a and b are any two objects of space X. If q = r, we still expect that, the closer q is to r, the more accurate ~ F r will be on triples (q, a, b). Figure 1 illustrates such cases.  
75	 In [2], the weak classifiers that are used by AdaBoost are of type ~ F , with F being a 1D embedding. We propose to use a different type of classifier, that can explicitly model the fact that any 1D embedding F is more useful in some regions of the space and less useful in other regions.  
76	 In particular, given a 1D embedding F , we need a function S(q) (which we call a splitter), that will estimate, given a query q, whether classifier ~ F is useful or not. More formally,  
77	 Given: (o 1 , y 1 ), . . . , (o t , y t ); o i  G, y i  {-1, 1}.  
78	 Initialize w i,1 = 1 t , for i = 1, . . . , t. For j = 1, . . . , J:  
79	 1. Train weak learner using training weights w i,j .  
80	 2. Get weak classifier h j : G  R.  
81	 3. Choose  j  R.  
82	 4. Set training weights w i,j+1 for the next round as follows:  
83	 w i,j+1 = w i,j exp( j y i h j (x i )) z j . (6)  
84	 where z j is a normalization factor (chosen so that P t i=1 w i,j+1 = 1).  
85	 Output the final classifier:  
86	 H(x) = J X  
87	 j=1  j h j (x). (7)  
88	 Figure 2: The AdaBoost algorithm. This description is largely copied from [27].  
89	 if X is the original space, we use the term splitter to denote any function mapping X to the binary set {0, 1}. We can readily define splitters using 1D embeddings. Given a 1D embedding F : X  R, and a subset V  R, we can define a splitter S F,V : X  {0, 1} as follows:  
90	 S F,V (q) =  1 if F (q)  V . 0 otherwise . (4)  
91	 Now, suppose we have a subset V  R and a 1D embedding F : X  R. We define a query-sensitive classifier ~ Q F,V : X 3  R, as follows:  
92	 ~ Q F,V (q, a, b) = S F,V (q) ~ F (q, a, b) . (5)  
93	 At an intuitive level, ~ F is by itself a classifier of triples (q, a, b). ~ Q F,V is a cropped version of ~ F , that gives 0 (i.e., a neutral result) whenever F (q) /  V . For example, if F =  
94	 F r for some reference object r, and V = [0,  ] for some positive threshold  , splitter S F,V (q) accepts object q if it is within distance  of reference object r. Therefore, the querysensitive classifier ~ Q F,V will apply ~ F only if q is sufficiently close to r. By choosing  in an appropriate way, we can capture the fact that ~ F should only be applied to objects within a specified distance from reference object r.  
95	 5.2 Overview of the Training Algorithm  
96	 The AdaBoost algorithm (taken, with minor modifications, from [27]) is shown in Figure 2. AdaBoost assumes that we have a "weak learner" module, which we can call at each round to obtain a new weak classifier. The goal is to construct a strong classifier that achieves much higher accuracy than the individual weak classifiers.  
97	 The AdaBoost algorithm simply determines the appropriate weight for each weak classifier, and then adjusts the training weights. The training weights are adjusted so that training objects that are misclassified by the chosen weak classifier h j get more weight for the next round. Because of the training weights, the weak learner is biased towards returning a classifier that tends to correct mistakes of previously chosen classifiers. Overall, weak classifiers are chosen and weighted so that they complement each other. The ability of AdaBoost to construct highly accurate classifiers by combining many relatively inaccurate weak classifiers has been demonstrated in numerous applications (for example, in [29, 31]).  
98	 In our case, the AdaBoost algorithm is adapted to the problem of constructing an embedding and a query sensitive distance measure. We adapt AdaBoost to this problem as follows:  
99	 · Each training object o i is a triple (q i , a i , b i ) of objects in X. Because of that, we refer to o i not as a training object, but as a training triple. T he set G from which  
100	 training triples are picked can be the entire X 3 (the set of all triples we can form by objects from X), or a more restricted subset of X 3 , as discussed in Sec. 6.  
101	 · The i-th training triple (q i , a i , b i ) is associated with a class label y i , which is 1 if q i is closer to a i and -1 if q i is closer to b i .  
102	 · Each weak classifier h j is a query-sensitive classifier ~ Q F,V , where F is a one-dimensional embedding and V is an interval of R.  
103	 Also, we pass to AdaBoost some additional arguments:  
104	 · A set C  X of candidate objects. Elements of C will  
105	 be used as reference objects and pivot objects to define 1D embeddings of type F r and F x 1 ,x 2 .  
106	 · A matrix of distances between any two objects in C,  
107	 and a matrix of distances from each c  C to each q i , a i and b i appearing in one of the training triples.  
108	 To fully specify the training algorithm, we need to specify what we do for steps 1, 2 and 3 of the algorithm shown in Figure 2. In simple terms, this is how those steps are implemented at each training round j:  
109	 · We construct a large set of classifiers ~ Q F,V by choosing randomly different 1D embeddings F and different ranges V  R.  
110	 · We choose, among that large set of classifiers, the one  
111	 that is the "best" at the current round, and we assign a weight to that classifier, using a method suggested in [27]. Evaluating how good a classifier is at a particular training round is related to how well that classifier performs on a training set of triples of objects.  
112	 In the next few paragraphs we will discuss how each of those operations is done, i.e., how we construct a large set of weak classifiers at each training round, and how we choose the best one out of them.  
113	 5.3 Forming Weak Classifiers  
114	 The weak classifiers considered by AdaBoost are classifiers ~ Q F,V as defined in Eq. 5, where F is some 1D embedding defined using reference objects or pivot objects from the set C of candidate objects. To pick a range V for ~ Q F,V , we simply compute the values F (x) for every object appearing in a training triple (q i , a i , b i ), and set V to be a random interval of R containing some of those values. We form many  
115	 such ranges V for each F , and for each range we measure the training error, i.e., the classification error of classifier ~ Q F,V , on the training triples. When we measure the training error, we weigh each training triple o i by the current weight w i,j of that triple in training round j. Therefore, the error of ~ Q F,V will be different at each training round.  
116	 At training round j we choose, randomly, a large number of 1D embeddings. For each selected 1D embedding F , we find the range V F,j that achieves the lowest training error at round j. The next classifier will be chosen among the classifiers ~ Q F,V F,j .  
117	 Now we are ready to specify how to implement steps 1 - 3  
118	 in Figure 2, for each training round j = 1, . . . , J. Step 1 consists of evaluating each ~ Q F,V F,j , so that we can choose the best weak classifier to add to the strong classifier that is being assembled. The function Z j ( ~ Q, ) gives a measure of how useful it would be to choose h j = ~ Q and  j =  at training round j:  
119	 Z j ( ~ Q, ) = t X  
120	 i=1 (w i,j exp( -y i ~ Q(q i , a i , b i ))) . (8)  
121	 The full details of the significance of Z j can be found in [27]. Here it suffices to say that if Z j ( ~ Q, ) &lt; 1 then choosing h j = ~ Q and  j =  is overall beneficial, and is expected to reduce the training error. Given the choice between two weighted classifiers h and  h , we should choose the weighted classifier that gives the lowest Z j value. Given h j , we should choose  j to be the  that minimizes Z j (h j , ).  
122	 Based on the above considerations, in step 1 we find the optimal  for each weak classifier ~ Q F,V F,j . Then, in steps 2 and 3 we set h j and  j respectively to be the weak classifier and weight that yielded the lowest overall value of Z j .  
123	 5.4 Training Output: Embedding and Distance  
124	 The output of the training stage is a classifier H of the following form:  
125	 H = J X  
126	 j=1  j ~ Q F j ,V j . (9)  
127	 Each ~ Q F j ,V j is associated with a 1D embedding F j . Clas 
128	 sifier H has been trained to estimate, for triples of objects (q, a, b), if q is closer to a or to b. However, our goal is to actually construct not just a classifier of triples of objects, but an embedding. Here we discuss how to define such an embedding F out , and an associated distance measure D out to be used to compare vectors.  
129	 A particular 1D embedding F can be equal to multiple F j 's occurring in the definition of classifier H. We construct the set F of all unique 1D embeddings used in H, as F = S J j=1 {F j }, and we denote the elements of F as F 1 , . . . , F d .  
130	 The embedding F out : X  R d is defined as F out (x) = (F 1 (x), . . . , F d (x)). Obviously, it is a d-dimensional embedding.  
131	 Before defining distance measure D out , we first need to define an auxiliary function A i (q), which assigns a weight to the i-th coordinate, for i = 1, . . . , d:  
132	 A i (q) = X  
133	 j:((j{1,...,J})(F i =F j )(F i (q)V j ))  j . (10)  
134	 In words, given object q, for coordinate i, we go through all weak classifiers ~ Q F j ,V j that make up H. For each such  
135	 classifier, we check if the splitter S F j ,V j accepts q (i.e., we  
136	 check if F j (q)  V j ), and we also check if F j = F i . If those conditions are satisfied, we add the weight  j to A i (q).  
137	 Let F out (q) = (q 1 , ..., q d ), and let x be some other object in X, with F out (x) = (x 1 , ..., x d ). We define distance D out as follows:  
138	 D out ((q 1 , ..., q d ), (x 1 , ..., x d )) = d X  
139	 i=1 (A i (q)|q i - x i |) . (11)  
140	 D out (v 1 , v 2 ) (where v 1 , v 2 are d-dimensional vectors) is like a weighted L 1 measure on R d , but the weights depend on v 1 . Therefore D out (v 1 , v 2 ) is not symmetric, and not a metric. We say that D out (v 1 , v 2 ) is a query-sensitive distance measure, since v 1 is typically the embedding of a query, and v 2 is the embedding of a database object that we want to compare to the query.  
141	 It is important to note that the way we defined F out and D out , if we apply Eq. 3 to obtain a classifier ~ F out from F out (with D set to D out ), then ~ F out = H. In words, the classifier corresponding to embedding F out is equal to the output of AdaBoost. Here are the main steps of the proof:  
142	 Proposition 1. ~ F out = H. Proof:  
143	 ~ F out (q, a, b) =  
144	 D out (F out (q), F out (b)) - D out (F out (q), F out (a)) =  
145	 d X  
146	 i=1 (A i (q)|F i (q) - F i (b)| - A i (q)|F i (q) - F i (a)|) =  
147	 d X  
148	 i=1 (A i (q)(|F i (q) - F i (b)| - |F i (q) - F i (a)|)) =  
149	 J X  
150	 j=1 ( j S F j ,V j (q)(|F j (q) - F j (b)| - |F j (q) - F j (a)|)) =  
151	 J X  
152	 j=1 ( j S F j ,V j (q) ~ F j (q, a, b)) =  
153	 J X  
154	 j=1 ( j ~ Q F j ,V j (q, a, b)) = H(q, a, b) .   
155	 This equivalence is important, because it shows that the quantity optimized by the training algorithm (i.e., classification error on triples of objects) is not only a property of the classifier H constructed by AdaBoost, but it is also a property of the embedding F out , when coupled with distance measure D out . We should emphasize that this equivalence between classifier H and embedding F out relies on the way we define D out . If, for example we had defined D out as a Euclidean (L 2 ) distance, or as a query-insensitive L 1 distance, then the equivalence would no longer hold.  
156	 6. CHOOSING TRAINING TRIPLES  
157	 In the original BoostMap algorithm [2], training triples are chosen at random. By using a random training set of triples, BoostMap tries to preserve the entire similarity structure of the original space X. This means that the resulting embedding is equally optimized for nearest neighbor queries, farthest neighbor queries, or median neighbor queries. In  
158	 cases where we only care about nearest neighbor queries, we would actually prefer an embedding that gave more accurate results for such queries, even if such an embedding did not preserve other aspects of the similarity structure of X, like farthest-neighbor information.  
159	 If we want to construct an embedding for the purpose of answering nearest neighbor queries, then we can construct training triples in a more selective manner. The main idea is that, given a training object q i , the types of triples (q i , a i , b i ) that are related to k-nearest neighbor retrieval accuracy are triples in which a i is one of the k nearest neighbors of q i , and b i is not one of the k nearest neighbors of q i . As long as the embedding does not fail on such triples, the embedding will correctly identify the set of k nearest neighbors of q i .  
160	 Based on the above considerations, given a parameter k 1 and given a set X tr of training objects (typically X tr is a subset of the set of database objects) we propose the following heuristic for choosing the i-th training triple (q i , a i , b i ):  
161	 1. Choose a random training object q i  X tr .  
162	 2. Choose a random integer k in 1, . . . , k 1 .  
163	 3. Choose a i to be the k -nearest neighbor of q i in X tr .  
164	 4. Reset k to a random integer between k 1 + 1 and |X tr |.  
165	 5. Choose b i to be the k -nearest neighbor of q i in X tr .  
166	 The value of parameter k 1 should be based on the maximum number k max of nearest neighbors that we may want to retrieve for an object. For example, if we want to retrieve up to 50 nearest neighbors per query (k max = 50), and if X tr contains about one tenth of the database, then we should set k 1 = 5, so that for every q i the corresponding a i is likely to be one of the 50 nearest neighbors of q i .  
167	 By choosing training triples this way, the training algorithm concentrates on building an embedding that, for any query object q, tends to map q closer to q's k max nearest neighbors than to objects not included in q's k max nearest neighbors. In practice, essentially the algorithm focuses on training triples (q, a, b) such that a is one of the nearest neighbors of q, that we would like to retrieve, and b is an object that is so far from q that we explicitly do not want to retrieve it as a match for q. An embedding that, given q, fails on many such triples (q, a, b), will fail to preserve the fact that a is one of the nearest neighbors of q. By using such triples for training, the learning algorithm will try to minimize the frequency with which the output embedding fails on such triples.  
168	 7. COMPLEXITY  
169	 At each training round we evaluate a number of weak classifiers by measuring their performance on t training triples, in order to choose the best weak classifier. If m weak classifiers are evaluated at each round, the computational time per training round is O(mt). In contrast, FastMap [12], SparseMap [16], and MetricMap [33] do not require training at all.  
170	 Before we even start the training algorithm, we need to compute distances D X from every object in C (the set of objects that we use to form 1D embeddings) to every object in C and to every object in X tr (the set of objects from which we form training triples). We also need all distances between pairs of objects in X tr . Computing all those distances can sometimes be the most computationally expensive part of the algorithm, depending on the complexity of computing D X .  
171	 If time and memory resources are not limited, then we can set both C and X tr equal to the entire database. Otherwise, we need to create C and X tr by sampling randomly from the database. If (as in our experiments) C and X tr have an equal number of elements, then the number of distances that we need to precompute is quadratic to |C|. In the experiments  
172	 we report some results using relatively small values for |C|.  
173	 We will see that, although larger values of |C| clearly im 
174	 prove embedding quality, we can get reasonable results (better than, say, using FastMap) even with a small |C|, thus  
175	 keeping the number of precomputed distances manageable.  
176	 We should emphasize that both the cost of precomputing distances and the cost of the training algorithm are onetime preprocessing costs. In many applications, spending the extra hours or days needed for this type of preprocessing is an acceptable cost, as long as it results in a higher-quality embedding, i.e., an embedding that leads to faster retrieval without sacrificing retrieval accuracy.  
177	 With respect to the online retrieval cost, computing the d-dimensional embedding of a query object takes O(d) time and requires O(d) evaluations of D X . Comparing the embedding of the query to the embeddings of n database objects takes time O(dn). For a fixed d, these costs are similar to those of FastMap [12], SparseMap [16], and MetricMap [33].  
178	 Compared to the original BoostMap algorithm, the proposed method has similar complexity both for the preprocessing steps and the online retrieval.  
179	 7.1 Dynamic Datasets  
180	 In our discussion so far we have assumed that the database is static. In some applications, however, we may need to add or remove objects online. As long as the underlying distribution of database objects is not altered, adding and removing objects is pretty straightforward. When adding an object x we need to compute its embedding F out (x). If F out is d-dimensional, computing F out (x) requires computing at most 2d distances D X between x and database objects.  
181	 If the underlying distribution of database objects changes significantly because of additions and removals, we may have to create a new embedding. A way to check whether the distribution of database objects has changed significantly is by measuring, at regular intervals, the error of the current embedding F out , i.e., the classification error of ~ F out on triples of objects picked (from the current database distribution) the same way we would choose training triples. When that error increases above some threshold, we can reuse our algorithm to construct a new embedding.  
182	 8. EMBEDDING APPLICATION: FILTER 
183	 AND-REFINE RETRIEVAL  
184	 In applications where we are interested in retrieving the k nearest neighbors for a query object q, a d-dimensional embedding F can be used in a filter-and-refine framework [15], as follows: first, we perform an offline preprocessing step, in which we compute and store vector F (x) for every database object x. Then, given a previously unseen query object q, we perform the following three steps:  
185	 · Embedding step: compute F (q), by measuring the  
186	 Figure 3: Some examples from the MNIST database of images of handwritten digits.  
187	 distances between q and the reference objects and/or pivot objects used to define F .  
188	 · Filter step: Find the database objects whose associ 
189	 ated vectors are the p most similar vectors to F (q).  
190	 · Refine step: sort those p candidates by evaluating the  
191	 exact distance D X between q and each candidate.  
192	 The assumption is that distance measure D X is computationally expensive and evaluating distances between vectors is much faster. The filter step discards most database objects by measuring distances between vectors. The refine step applies D X only to the top p candidates. This is much more efficient than brute-force retrieval, in which we compute D X between q and the entire database.  
193	 To optimize filter-and-refine retrieval, we have to choose p, and often we also need to choose d, which is the dimensionality of the embedding. As p increases, we are more likely to include the true k nearest neighbors in the top p candidates found at the filter step, but we also need to evaluate more distances D X at the refine step. Overall, we trade accuracy for efficiency. Similarly, as d (the dimensionality of the embedding) increases, computing the embedding for the query object becomes more expensive, but we may also get more accurate results in the filter step (since each additional dimension has been added by the training algorithm in order to improve the classification error of the embedding), and thus we may be able to decrease p. The best choice of p and d will depend on domain-specific parameters like k (i.e., how many of the nearest neighbors of an object we want to retrieve), the time it takes to compute the distance D X , the time it takes to compare d-dimensional vectors, and the desired retrieval accuracy (i.e., how often we are willing to miss some of the true k nearest neighbors).  
194	 We should also note that, as d increases, the filter step also becomes more expensive, because we need to compare vectors of increasingly high dimensionality. However, in our experiments so far, with embeddings of up to 1,000 dimensions, the filter step always takes negligible time; retrieval time is dominated by the few exact distance computations we need to perform at the embedding step and the refine step.  
195	 In cases (not encountered in our experiments) when the filter step takes up a significant part of retrieval time, one can apply indexing techniques [6, 17, 36] to speed up filtering. We should keep in mind that in the filter step we are finding nearest neighbors in a real vector space, and many indexing methods are applicable in such a setting. One of the advantages of using embeddings is exactly the fact that we map arbitrary spaces to well-known real vector spaces, for which many tools are available.  
196	 9. EXPERIMENTS  
197	 We compared the proposed method to the original BoostMap method [2] and FastMap [12]. We used two different datasets: the MNISTdataset of handwritten digits [22], with the Shape Context Distance [4] as the exact distance measure, and a time series database [32] with constrained Dynamic Time Warping [32] as the exact distance measure.  
198	 The MNIST dataset contains images of isolated handwritten digits (numbers from 0 to 9). It consists of a training set of 60,000 images, which we used as the database, and a test set (disjoint from the training set) of 10,000 images that we used as query objects. The subjects who produced the test images were not used in producing the training images. The Shape Context Distance is introduced in [4]. To compute that distance, 100 shape context features are extracted from each image. Two images are aligned by doing bipartite matching between their features (which involves the computationally expensive Hungarian algorithm). The final distance is a weighted sum of three terms: the cost of matching shape context features, the cost of the alignment, and the intensity-level differences between image subwindows centered at matching feature locations. A 3-nearest-neighbor classifier using Shape Context matching gave state-of-theart classification accuracy on the MNISTdatabase, with an error rate of only 0.63%.  
199	 The second dataset that we tried was the time-series dataset used in [32]. To generate that dataset, various real datasets were used as seeds for generating a large number of timeseries that are variations of the original sequences. Multiple copies of every real sequence were constructed by incorporating small variations in the original patterns as well as additions of random compression and decompression in time. The final dataset contains a "database" set of 32,768 sequences, and a "query" set of 50 sequences. Sequences are multi-dimensional, with an average size of 500 points each. The series were normalized by subtracting the average value in each dimension. Exact distances were measured using constrained Dynamic Time Warping, with a warping length  = 10% of the total length of the shortest sequence under comparison as described in [32].  
200	 To compare different embedding methods, we used each of those methods to build embeddings of various dimensions (the dimensionality ranged from 1 to 600). Then, for each embedding method, for each k and accuracy percentage B, we found the optimal parameters (i.e., number of dimensions of the embedding, and parameter p specifying the number of database objects to be retained after the filter step) under which we would successfully retrieve all k true nearest neighbors for a percentage of query objects equal to B, while minimizing the total number of exact distance computations per query object.  
201	 For the time series dataset, we performed an initial evaluation on the 50 queries used as a test set in [32]. Our method achieved a speed-up factor of 51.2, using filter-andrefine retrieval, with a 150-dimensional embedding and with parameter p set to 443. With those settings, the true nearest neighbor was retrieved correctly for each of the 50 queries. The indexing method in [32] reports a speed-up of approxi 
202	 mately a factor of 5, while retrieving correctly the true nearest neighbor for all 50 queries, and measured on the same set of 50 queries that we used.  
203	 However, to get a clearer picture of performance, we decided to use a larger set of queries. To achieve that, we merged the query set and the database, and from the merged set we chose (randomly) a new set of 1,000 queries, with the remaining 31,818 objects used as the database for those queries. We found that performance on the new set of queries was not as good as on the initial set of 50 queries; on the new set of queries, a speedup factor of 50 was obtained only if we allowed the true nearest neighbor to be missed for 10% of the query objects. At the same time, even on the new set of queries, our method achieved significant speedups for k-nearest neighbor retrieval with different accuracy percentages and different values of k. T he results reported in the remainder of this section for the time series dataset are with respect to the set of 1,000 queries.  
204	 In Figures 4 and 5, and in Table 1, we compare the proposed method (denoted as Se-QS) to the original BoostMap method (denoted as Ra-QI) and FastMap. We also show results for two intermediate methods, which incorporate only one of our two modifications to the original BoostMap algorithm. To denote each method, and its relation to the other methods, we use the following abbreviations:  
205	 Ra: Training triples are chosen entirely randomly from the  
206	 set of all possible triples, as in the original BoostMap method.  
207	 Se: Training triples are chosen selectively, from a restricted  
208	 set of possible triples, using the method we propose in Sec. 6.  
209	 QI: A query-insensitive distance measure D out is used at the filter step, as in the original BoostMap method.  
210	 QS: A query-sensitive distance measure D out is used at the filter step, as proposed in this paper.  
211	 Based on these abbreviations, Ra-QI denotes the original BoostMap algorithm, and Se-QS denotes the modified BoostMap algorithm we propose in this paper. Ra-QS and Se-QI add to the original BoostMap only one of the two changes that we propose in this paper (either the method for building a query-sensitive distance measure or the method for choosing training triples).  
212	 The optimal number of exact distance computations (i.e., corresponding to optimal settings for the dimensionality of the embedding and the parameter p) is shown for different values of k, from 1 to 50, and different percentages of accuracy (i.e., 90%, 95%, and 99%), in Figure 4 for the MNISTdataset and Figure 5 for the time series dataset. To avoid cluttering the figures, we omit from them the Ra-QS method, which, overall, gave pretty similar performance to the Se-QI version. In Table 1 we show, for selected accuracy percentages and values of k, the number of exact distance computations required by FastMap, the original BoostMap method, the proposed method, and both intermediate methods Se-QI and Ra-QS.  
213	 In all cases (except for results on 100% accuracy, which are dominated by the single query giving the worst results), query-sensitive embeddings lead to better performance than embeddings using a global L 1 distance measure. In some cases, query-sensitive embeddings achieve performance that 0 10 20 30 40 50 1024 2048 4096 8192 16384 32768 60000  
214	 k # distances for 90% accuracy FastMap Ra-QI   Se-QI   Se-QS    
215	 0 10 20 30 40 50 2048 4096 8192 16384 32768 60000  
216	 k # distances for 95% accuracy FastMap Ra-QI   Se-QI   Se-QS    
217	 0 10 20 30 40 50 4096 8192 16384 32768 60000  
218	 k # distances for 99% accuracy FastMap Ra-QI   Se-QI   Se-QS    
219	 Figure 4: Comparing FastMap, the original BoostMap method (denoted as Ra-QI), the proposed method (denoted as Se-QS), and intermediate method Se-QI (which incorporates our method of choosing training triples, but still constructs a query-insensitive embedding)on the MNIST database of handwritten digits, using the Shape Context Distance as the exact distance measure. We show the number of exact distance computations needed by each method to achieve retrieval of all k nearest neighbors ( k ranging from 1 to 50)for 90%, 95%, and 99% of  
220	 the 10,000 query objects that make up the test set of the MNIST database.  
221	 0 10 20 30 40 50 512 1024 2048 4096 8192 16384 31818  
222	 k # distances for 90% accuracy FastMap Ra-QI   Se-QI   Se-QS    
223	 0 10 20 30 40 50 2048 4096 8192 16384 31818  
224	 k # distances for 95% accuracy FastMap Ra-QI   Se-QI   Se-QS    
225	 0 10 20 30 40 50 4096 8192 16384 31818  
226	 k # distances for 99% accuracy FastMap Ra-QI   Se-QI   Se-QS    
227	 Figure 5: Comparing FastMap, the original BoostMap method (denoted as Ra-QI), the proposed method (denoted as Se-QS), and intermediate method Se-QI (which incorporates our method of choosing training triples, but still constructs a query-insensitive embedding)on the time series database, using constrained Dynamic Time Warping as the exact distance measure. We show the number of exact distance computations needed by each method to achieve retrieval of all k nearest neighbors (k  
228	 ranging from 1 to 50)for 90%, 95%, and 99% of the 1,000 query objects that we use as a test set. MNIST Database with Shape Context k pct FastMap Ra-QI Ra-QS Se-QI Se-QS 1 90 20059 1930 1824 1296 1223 1 95 33858 3161 2789 2190 2135 1 99 56619 6315 5141 4577 4329 1 100 59996 55019 40479 40946 13406 10 90 53852 6280 5233 4631 3866 10 95 58009 9059 6584 5988 5072 10 99 59800 22266 11802 13932 7642 10 100 60000 55019 58677 56936 52066 50 90 59102 14232 9134 9856 6139 50 95 59644 21085 12767 14848 7477 50 99 59980 39311 25878 31176 18510 50 100 60000 59840 59974 59735 59941  
229	 Time Series Dataset with Constrained DTW k pct FastMap Ra-QI Ra-QS Se-QI Se-QS 1 90 8357 1018 898 649 580 1 95 20176 12851 6484 5691 1995 1 99 27082 16236 9743 9072 4269 1 100 27547 16426 13922 9562 6965 10 90 19613 13364 6521 5721 2582 10 95 24888 16270 9346 8262 4251 10 99 27531 24052 13070 9448 6260 10 100 27623 31818 24730 27267 17627 50 90 23289 18821 9757 9043 4997 50 95 27041 26985 12821 9571 6504 50 99 27564 31818 19357 24672 16265 50 100 27742 31818 26748 27267 26883  
230	 Table 1: Comparison of FastMap, the original BoostMap (denoted as Ra-QI), the proposed method (denoted as Se-QS), and the two intermediate methods Ra-QS and Se-QI, on the MNIST dataset based on 10,000 query objects and the time series dataset based on 1,000 query objects. For different values of k, and different percentages of accuracy (shown in the "pct" column), we show the number of exact distance computations required by each embedding method, assuming that we have set the two parameters of that method (dimensionality of embedding and number p of matches to keep after the filter  
231	 step)to the optimal values that minimize the number of exact distance computations. For comparison, brute force search would require 60000 exact distance computations in the MNIST dataset and 31818 exact distance computations in the time series dataset.  
232	 is two or three times as fast for a fixed error rate. Similarly, in all cases, except results on 100% accuracy, the method we introduced in this paper for choosing training triples leads to better performance than training an embedding with randomly chosen triples. Overall, the proposed method, which combines both a query-sensitive distance measure and the new way of choosing training triples, significantly outperforms both FastMap and the original BoostMap method.  
233	 To train embeddings, for the original BoostMap, the proposed method, and intermediate methods Ra-QS and Se-QI, we always used a training set of 300,000 triples, generated from a set X tr of 5,000 database objects. The set C of candidate objects also consisted of 5,000 database objects. Query objects from the test set were not used in any part of the training algorithm. Parameter m, the number of weak classifiers to evaluate at each training round, was set to 2,000. Parameter k 1 , used in choosing training triples, was set to 5 for the MNISTdataset and to 9 for the time series dataset,  
234	 5 10 15 20 25 30 35 40 45 50 1024 2048 4096 8192 16384 32768 60000  
235	 k # distances for 95% accuracy FastMap       Quick Se-QS   Regular Se-QS  
236	 Figure 6: Results using an embedding produced using the Se-QS method, but with sets C and X tr including only 200 objects, and using only 10,000 training triples, so that the preprocessing cost of the Se-QS method becomes very small. We compare those results (denoted as "Quick Se-QS")to the results for FastMap and method "Se-QS" as shown in Fig. 4. "Regular Se-QS" denotes the results using the Se-QS method with |C| and |X tr | equal to 5,000, and with 300,000 training triples. For different values of k, the figure shows the number of ex 
237	 act distance computations required by each embedding method, in order to retrieve the true k nearest neighbors  
238	 for 95% of the 10,000 queries, for the MNIST dataset.  
239	 following the guidelines suggested in Sec. 6 for k max = 50. This way, embeddings were optimized for retrieval of up to 50 nearest neighbors per query. In each dataset, we constructed a FastMap embedding by running the FastMap algorithm on a subset of the database, containing 5,000 objects.  
240	 We also ran an experiment on the MNISTdataset, in which we used the proposed method, i.e., method Se-QS, but with relatively small sizes for sets C and X tr used in the training algorithm, and with fewer training triples. Both |C| and |X tr | were equal to 200 in this experiment, and we  
241	 only used 10,000 training triples. Using these settings, the preprocessing cost of the algorithm becomes much smaller: both the number of distances D X that we need to precompute is smaller (80,000 distances as opposed to 50,000,000 distances in the previous experiments) and the running time for the learning algorithm was much shorter (about 20 minutes, as opposed to about 10 hours using 300,000 triples). Fig. 6 shows the results we obtain with these settings, for 95% retrieval accuracy, and compares those results to what we get using FastMap and using the Se-QS method with |C|  
242	 and X tr equal to 5,000, and with 300,000 training triples. We see that we still get results that are better than FastMap, and that are useful overall results. Although spending more time on preprocessing improves retrieval efficiency (for fixed accuracy), we can construct useful embeddings even when the time available for preprocessing is limited.  
243	 On average, computing exact Shape Context distances can be done at the rate of 15 distances per second, and computing constrained Dynamic Time Warping distances can be done at the rate of about 60 distances per second, on an Opteron 2.2GHz processor. To obtain the corresponding processing times per query for each setting shown in Figs. 4, 5, 6 and Table 1, one simply needs to divide the number of exact distance computations by 15 for Shape Context and by 60 for Dynamic Time Warping. Exact distance computations almost completely determined the processing time per query; the rest of the calculations took a fraction of a second for each query.  
244	 10. DISCUSSION  
245	 The experimental results reported in this paper provide a quantitative comparison of the proposed algorithm to the original BoostMap method [2] and FastMap [12], on two different datasets that use non-Euclidean, non-metric distance measures: the MNISTdataset of handwritten digits using the Shape Context distance as the underlying distance measure, and a time series dataset using constrained Dynamic Time Warping as the underlying distance measure. The experiments demonstrate that the proposed method gives, for most settings, significantly better results than the original BoostMap method or FastMap.  
246	 The main difference of the proposed method with respect to existing embedding methods is that it constructs a querysensitive distance measure. Such a distance measure captures the fact that different embedding coordinates are important for different queries, and thus leads to improved retrieval accuracy at the filter step of filter-and-refine retrieval.  
247	 We should stress that both the Shape Context distance measure and Dynamic Time Warping are non-metric, because they do not obey the triangle inequality. This means that even general indexing tools like M-trees [40], designed for metric spaces, cannot be applied in these datasets. Many other commonly used distance measures, like the KullbackLeibler distance, or the chamfer distance [3] are also nonmetric. Embeddings are the only family of methods that we are aware of that is not domain-specific and that can be applied for efficient retrieval in such spaces. Like other embedding methods, our method is general, and can be applied to arbitrary spaces.  
248	 We believe that query-sensitive distance measures may prove useful in other settings, in addition to embeddingbased nearest neighbor retrieval. A common problem in data mining, clustering, and pattern recognition applications, is how to construct a meaningful distance measure for comparing high-dimensional vectors. We are interested in exploring whether our algorithm for learning a query-sensitive distance measure can offer advantages in such applications.  
249	 11. REFERENCES  
250	 [1] C. C. Aggarwal. Re-designing distance functions and distance-based applications for high dimensional data. SIGMOD Record, 30(1):13­18, 2001. [2] V. Athitsos, J. Alon, S. Sclaroff, and G. Kollios. BoostMap: A method for efficient approximate similarity rankings. In CVPR, 2004. [3] H. Barrow, J. Tenenbaum, R. Bolles, and H. Wolf. Parametric correspondence and chamfer matching: Two new techniques for image matching. In IJCAI, pages 659­663, 1977. [4] S. Belongie, J. Malik, and J. Puzicha. Matching shapes. In ICCV, volume 1, pages 454­461, 2001. [5] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. PAMI, 24(4):509­522, 2002.  
