0	 FARMER: Finding Interesting Rule Groups in Microarray  
1	 Datasets  
2	 Gao Cong, Anthony K. H. Tung  ,  
3	 Xin Xu, Feng Pan  
4	 Dept. of Computer Science Natl. University of Singapore  
5	 {conggao,atung,xuxin,panfeng}@comp.nus.edu.sg Jiong Yang  
6	 Dept. of Computer Science, University of Illinois, Urbana Champaign  
7	 jioyang@cs.uiuc.edu  
8	 ABSTRACT  
9	 Microarray datasets typically contain large number of columns but small number of rows. Association rules have been proved to be useful in analyzing such datasets. However, most existing association rule mining algorithms are unable to efficiently handle datasets with large number of columns. Moreover, the number of association rules generated from such datasets is enormous due to the large number of possible column combinations.  
10	 In this paper, we describe a new algorithm called FARMER that is specially designed to discover association rules from microarray datasets. Instead of finding individual association rules, FARMER finds interesting rule groups which are essentially a set of rules that are generated from the same set of rows. Unlike conventional rule mining algorithms, FARMER searches for interesting rules in the row enumeration space and exploits all user-specified constraints including minimum support, confidence and chi-square to support efficient pruning. Several experiments on real bioinformatics datasets show that FARMER is orders of magnitude faster than previous association rule mining algorithms.  
11	 1. INTRODUCTION  
12	 With recent advances in DNA chip-based technologies, we can now measure the expression levels of thousands of genes in cell simultaneously resulting in a large amount of highdimension data. These microarray datasets typically have a large number of columns but a small number of rows. For example, many gene expression datasets may contain up to 10,000-100,000 columns but only 100-1000 rows.  
13	 Recent studies have shown that association rules are very useful in the analysis of microarray data. Due to their relative simplicity, they are more easily interpreted by biologists. Association rules can be applied in the following two  
14	  Contact Author  This work was supported in part by NUS ARF grant R252000-121-112 and R252-000-142-112.  
15	 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMOD 2004 June 13-18, 2004, Paris, France. Copyright 2004 ACM 1-58113-859-8/04/06 . . . $ 5.00. scenarios: (1) it is shown in [9, 13] that classifiers built from association rules are rather accurate in identifying cancerous cell; (2) it is suggested in [7] that association rules can be used to build gene networks since they can capture the associations among genes.  
16	 In this paper, we focus on a special type of association rule which takes the form of LHS  C, where LHS is a set of items and C is a class label. We use the term "support of A" to refer to the number of rows containing A in the database and denote this number as sup(A). The probability of the rule being true is referred to as "the confidence of the rule" and is computed as sup(LHS  C)/sup(LHS). The number of rows in the database that match the rule is defined as "the support of the rule". User-specified constraints such as minimum support (a statement of generality) and minimum confidence (a statement of predictive ability) are often imposed on mining such association rules.  
17	 Microarray datasets pose a great challenge for existing rule mining algorithms in both runtime and the number of discovered rules. While there are a large number of algorithms that have been developed for association rule mining [1, 11, 18, 23], their basic approaches are all column enumeration in which combinations of columns are tested systematically to search for association rules. Such an approach is unsuitable for microarray datasets. This is because if i is the maximum length of a row in a dataset, the search space based on column enumeration could be as large as 2 i . Previous column enumeration methods work well for datasets with small average row length (usually i &lt; 100). However, for micorarray datasets, i can be in the range of tens of thousands. These high-dimension bioinformatics datasets with thousands of columns render most of the existing algorithms impractical.  
18	 On the other hand, the number of rows in such datasets is typically in the order of hundreds to a thousand. If m is the number of rows, the size of the row enumeration space will be 2 m . In our application domain (e.g., microarray datasets), the size of the row enumeration space is much less than the size of the column enumeration space. Therefore, it seems reasonable to devise the algorithm that does not perform column enumeration but row enumeration. To the best of our knowledge, none of existing studies investigate the possibilities of discovering rules by row enumeration.  
19	 A large number of long frequent itemsets may be discovered from microarray datasets due to their large number of columns. As a result, large number of association rules may be generated due to the combinatorial explosion of frequent itemsets [3]. For example, given a dataset with one row, five columns and one class label: {a, b, c, d, e, Cancer}, we could have 31 rules of the form LHS  Cancer since any combination of a, b, c, d and e could be the LHS for the rule. These 31 rules all cover the same row and have the same confidence (100%). Such a large set of rules contains a lot of redundancy and is difficult to interpret. Instead of generating all 31 rules, we propose to discover these rules as a rule group whose consequent is Cancer, and which can be identified by a unique upper bound plus a set of lower bounds. The upper bound of a rule group is the rule with the most specific LHS among the rules. In our example, the upper bound rule is abcde  Cancer. The lower bounds of the rule group are the rules with the most general LHS in the rule group. For our example, the rule group has 5 lower bounds (a  Cancer, b  Cancer, c  Cancer, d  Cancer, and e  Cancer). Given the upper bound and the lower bounds of the rule group, other rules within the group can be easily derived.  
20	 We further reduce the number of rules by finding interesting rule groups only. Consider two rules abcd  Cancer with confidence 90% and ab  Cancer with confidence 95%, it is obvious that ab is a better indicator of Cancer since ab  Cancer has a higher confidence and all rows covered by abcd  Cancer must be covered by ab  Cancer. With ab  Cancer, rule abcd  Cancer is not interesting 1 .  
21	 In this paper, we describe a novel algorithm FARMER 2 , that is specially designed to mine interesting rule groups from microarray datasets. FARMER discovers upper bounds of interesting rule groups by performing depth-first rowwise enumeration instead of the usual column-wise approach taken by existing rule mining algorithms. This basic idea is combined with efficient search pruning strategies based on user-specified thresholds (minimum support, minimum confidence and minimum chi-square value), yielding a highly optimized algorithm. We also describe an efficient algorithm for computing the lower bounds. Our experiments show that FARMER substantially outperforms other rule mining algorithms described in [2], [23](CHARM) and [21](CLOSET+) To further illustrate the usefulness of the discovered interesting rule groups in biology, we build a simple classifier based on these interesting rule groups, which outperforms the wellknown CBA [14] and SVM [12] on 5 real-life datasets.  
22	 The rest of this paper is organized as follows: In the next section, we will introduce some preliminaries and give our problem definitions. The FARMER algorithm will be explained in Section 3. Experimental results will be given in Section 4 on real-life microarray datasets. Section 5 introduces some of the related work for this paper. We will conclude our discussion in Section 6.  
23	 2. PRELIMINARY  
24	 In this section, we introduce some basic notations and concepts that are useful for further discussion.  
25	 2.1 The Basics  
26	 Dataset: the dataset (or table) D consists of a set of rows, R={r 1 , ..., r n }. Let I={i 1 , i 2 , ..., i m } be the complete set of  
27	 1 Rules like abcd  Cancer are simply pruned off in methods like CBA [14] when they are building classifier with association rules. 2 FARMER stands for Finding Interesting Association Rule Groups by Enumeration of Rows. items of D, and C = {C 1 , C 2 , ..., C k } be the complete set of class labels of D, then each row r i  R consists of one or more items from I and a class label from C.  
28	 As an example, Figure 1(a) shows a dataset where items are represented with alphabets from `a' to `t'. There are altogether 5 rows, r 1 ,...,r 5 , in the dataset, the first three of which are labeled C while the other two are labeled ¬C. To simplify the notation, we use the row id set to represent a set of rows and the item id set to represent a set of items. For instance, "234" denotes the row set {r 2 , r 3 , r 4 }, and "acf " denotes the itemset {a, c, f }.  
29	 Given a set of items I  I, we define the row support set, denoted R(I )  R, as the largest set of rows that contain I . Likewise, given a set of rows R  R, we define item support set, denoted I(R )  I, as the largest set of items that are common among the rows in R .  
30	 Example 1. R(I ) and I(R ) Consider again the table in Figure 1(a). Let I be the itemset {a, e, h}, then R(I ) = {r 2 , r 3 , r 4 }. Let R be the row set {r 2 , r 3 }, then I(R )={a, e, h} since this is the largest itemset that occurs in both r 2 and r 3 . 2  
31	 Association Rule: an association rule , or just rule for short, from dataset D takes the form of A  C, where A  I is the antecedent and C is the consequent (here, it is a class label). The support of  is defined as the |R(A  C)|, and its confidence is |R(A  C)|/|R(A)|. We denote the antecedent of  as .A, the consequent as .C, the support as .sup, the confidence as .conf and the chi-square value is .chi.  
32	 As discussed in the introduction, in real biological applications, people are often interested in rules with a specified consequent C that meet specified thresholds, like minimum support and minimum confidence.  
33	 2.2 Interesting Rule Groups (IRGs)  
34	 The interesting rule group is a concept which helps to reduce the number of rules discovered by identifying rules that come from the same set of rows and clustering them conceptually into one entity.  
35	 Definition 2.1. Rule Group Let D be a dataset with itemset I and C be a specified class label. G = {A i  C|A i  I} is a rule group with antecedent support set R and consequent C, iff (1) A i  C  G, R(A i ) = R, and (2) R(A i ) = R, A i  C  G. Rule  u  G ( u : A u  C) is an upper bound of G iff there exists no   G ( :A  C) such that A  A u . Rule  l  G ( l : A l  C) is a lower bound of G iff there exists no   G ( : A  C) such that A  A l . 2  
36	 Lemma 2.1. Given a rule group G with the consequent C and the antecedent support set R, it has a unique upper bound  (: A  C). Proof: Assume there exists another upper bound  (A  C)  G such that A = A and A  A. Let A = A  A . Because of R(A ) = R(A) = R, we get R(A ) = R, and then A  C  G and A  A. Therefore, (A  C) cannot be an upper bound of G. So the upper bound of a rule group must be unique. 2  
37	 Based on lemma 2.1, a rule group G can be represented with its unique upper bound  u . i r i class 1 a,b,c,l,o,s C 2 a,d,e,h,p,l,r C 3 a,c,e,h,o,q,t C 4 a,e,f,h,p,r ¬C 5 b,d,f,g,l,q,s,t ¬C (a) Example Table i j R(i j ) C ¬C a 1,2,3 4 b 1 5 c 1,3 d 2 5 e 2,3 4 f 4,5 g 5 h 2,3 4 l 1,2 5 o 1,3 p 2 4 q 3 5 r 2 4 s 1 5 t 3 5 (b) Transposed Table, T T  
38	 Figure 1: Running Example i j R(i j ) C ¬C a 1,2,3 4 e 2,3 4 h 2,3 4  
39	 Figure 2: T T | {2,3}  
40	 Example 2. Rule Group A running example is shown in Figure 2 in which R({e}) = R({h}) = R({ae}) = R({ah}) = R({eh}) = R({aeh}) = {r  
41	 2 , r 3 , r 4 }. They make up a rule group {e  C, h  C, ..., aeh  C} of consequent C, with the upper bound aeh  C and the lower bounds e  C and h  C. 2  
42	 It is obvious that all rules in the same rule group have the same support, confidence and chi-square value since they are essentially derived from the same subset of rows. Based on the upper bound and all the lower bounds of a rule group, we can identify its remaining members according to the lemma below.  
43	 Lemma 2.2. Suppose rule group G with the consequent C and antecedent support set R has an upper bound A u  C and a lower bound A l  C. Rule (A  C), where A  A u and A  A l , must be a member of G. Proof: Since A  A u , R(A)  R(A u ). Likewise, R(A)  R(A l ). Since R(A l ) = R(A u ) = R, R(A) = R. So (A  C) belongs to G. 2  
44	 Definition 2.2. Interesting Rule Group (IRG) A rule group G with upper bound  u is an interesting rule group iff for any rule group with upper bound  u   u ,  u .conf &lt;  u .conf . For brevity, we will use the abbreviation IRG to refer to interesting rule group. 2  
45	 Our algorithm FARMER is designed to find IRGs that satisfy user-specified constraints including minimum support, minimum confidence and minimum chi-square value 3 . FARMER finds the upper bounds of all IRGs first, and then gathers their lower bounds. This makes it possible for users to recognize all the rule group members as and when they want to.  
46	 3. THE FARMER ALGORITHM  
47	 3 Other constraints such as lift, conviction, entropy gain, gini and correlation coefficient can be handled similarly  
48	 To illustrate our algorithm, we first give a running example (Figure 1). Table T T (Figure 1(b)) is a transposed version of the example table (Figure 1(a)). In T T , the items become the row ids while the row ids become the items. A row id r m in the original table will appear in tuple i n of T T if and only if the item i n occurs in the row r m of the original table. For instance, since item d occurs in row r 2 and r 5 of the original table, row ids "2" and "5" occur in tuple d of T T . To avoid confusion, we hereafter refer to the rows in the transposed table as tuples while referring to those in the original table as rows.  
49	 We provide a conceptual explanation of FARMER algorithm to discover upper bounds of interesting rule groups in Section 3.1, the pruning strategies in Section 3.2, and the implementation details in Section 3.3. In Section 3.4, we describe subroutine MineLB of FARMER to discover the lower bounds of interesting rule groups.  
50	 3.1 Enumeration  
51	 Unlike existing column-wise rule mining algorithms which perform their search by enumeration of columns [18], FARMER performs search by enumeration of row sets to find interesting rule groups with consequent C. Figure 3 illustrates the enumeration tree which represents the search of FARMER conceptually for the interesting rule groups in the absence of any pruning strategies. Each node X of the enumeration tree corresponds to a combination of rows R and is labeled with I(R ) that is the antecedent of the upper bound of a rule group identified at this node. For example, node "12" corresponds to the row combination {r 1 , r 2 } and "al" indicates that I({r 1 , r 2 }) = {a, l}. An upper bound al  C can be discovered at node "12". This is correct because of the following lemma.  
52	 Lemma 3.1. Let X be a subset of rows from the original table, then I(X)  C must be the upper bound of the rule group G whose antecedent support set is R(I(X)) and consequent is C. Proof: First, according to Definition 2.1, I(X)  C belongs to rule group G with antecedent support set R(I(X)) and consequent C. Second, assume that I(X)  C is not the upper bound of G, then there must exist an item i such that i /  I(X), and I(X)  {i}  C belongs to G. So we get R(I(X)) = R(I(X)  {i}). Since rows in X contain all items of I(X), we get X  R(I(X)), and then X  R(I(X)  {i}). This means that i is also found in every row of X, which contradicts the definition that I(X) is the largest set of items that are found in every row of X. So I(X)  C is the upper bound of the rule group with antecedent support set R(I(X)). 2  
53	 FARMER performs a depth-first search on the enumeration tree by moving along the edges of the tree. By imposing an order ORD, in which the rows with consequent C are ordered BEFORE the rows without consequent C(this is done to support efficient pruning which will be explained later), we are able to perform a systematic search by enumerating the combinations of rows based on the order ORD. For example, let "1 2 3 4 5" according to ORD, the order of search in Figure 3 will be {"1", "12", "123", "1234", "12345", "1235",...,"45", "5"} in absence of any optimization and pruning strategies. Note that the order also serves for confidence pruning purpose (explained in section 3.2.3). {bls} 15 {l} 125 {a} 124 {a} 123  
54	 {al}  
55	 {aco} {abclos}  
56	 {} 13 12  
57	 1 134  
58	 {f} {}  
59	 {} {} {} {a}  
60	 1245  
61	 1345 1235 1234  
62	 {} 345 12345  
63	 {aeh} 34  
64	 35  
65	 45 {q} {acehoqt} 3 {} 245 {a}  
66	 2345  
67	 {bdfglqst} 5 {aefhpr} 4 {adehplr} 2  
68	 {dl} 25 {aehpr} 24 {aeh} 23  
69	 {} 235 {aeh} 234 {} 135  
70	 {} 145 {a} 14  
71	 {}  
72	 Figure 3: The Row Enumeration Tree.  
73	 Next, we prove that the complete rule groups can be discovered by a complete row enumeration.  
74	 Lemma 3.2. By enumerating all possible row combinations on the row enumeration tree, we can obtain the complete set of upper bounds and the corresponding complete set of rule groups in the dataset. Proof: With Lemma 2.1, we know that each rule group can be represented by a unique upper bound. Based on the definition of rule group (Definition 2.1), all possible antecedent support sets of rule groups can be obtained by enumerating all possible row combinations. Each antecedent support set X corresponds to a rule group with upper bound "I(X)  C". Hence the proof. 2  
75	 It is obvious that a complete traversal of the row enumeration tree is not efficient. Various pruning techniques will be introduced to prune off unnecessary searches in the next section. We will next introduce the framework of our algorithm for discovering the upper bounds of rule groups. We first introduce two concepts.  
76	 Definition 3.1. Conditional Transposed Table (T T | X ) Given the transposed table T T (used at the root of the enumeration tree), a X-conditional transposed table (T T | X ) at node X (X is the row combination at this node) is a subset of tuples from T T such that for each tuple t of T T that t  X, there exists a tuple t = t in T T | X . 2  
77	 Example 3. Let T T be the transposed table in Figure 1(b) and let X = {2, 3}. The X-conditional transposed table, T T | X is shown in Figure 2. 2  
78	 Definition 3.2. Enumeration Candidate List (T T | X .E) Let T T | X be the X-conditional transposed table and r min  X be the row id with the lowest ORD order in row combination X. Let E P = {r|r ORD r min  r  R(C)} (all rows of consequent C ordered after r min ), and E N = {r|r ORD r min  r  R(¬C)} (all rows with class C ordered after r min ). The enumeration candidate list for T T | X , denoted as T T | X .E, is defined to be E P  E N . 2 Notation Description T T | X .E enumeration candidates; T T | X .E P enumeration candidates with label C; T T | X .E N enumeration candidates without label C; T T | X .Y enumeration candidates that occur in each tuple of T T | X .  
79	 Figure 4: Notations for Conditional Transposed Table  
80	 In the rest of this paper, we will use the notations in Figure 3.1 to describe various operations on the conditional transposed table, T T | X .  
81	 Our formal algorithm is shown in Figure 5. FARMER involves recursive computations of conditional transposed tables by performing a depth-first traversal of the row enumeration tree. Each computed conditional table represents a node in the enumeration tree of Figure 3. For example, the {2, 3}-conditional table is computed at node "23". After initialization, FARMER calls the subroutine M ineIRGs to recursively generate X-conditional tables.  
82	 The subroutine M ineIRGs takes in four parameters at node X: T T | X , sup p , sup n and IRG. T T | X is the Xconditional transposed table at node X with enumeration candidates T T | X .E P and T T | X .E N . sup p is the number of identified rows that contain I(X)  C while sup n is the number of identified rows that contain I(X)  ¬C before scanning T T | X . IRG stores the upper bounds of interesting rule groups discovered so far.  
83	 Steps 1, 2, 4 and 5 in the subroutine M ineIRGs perform the pruning. They are extremely important for the efficiency of FARMER Algorithm and will be explained in the next subsection. Step 3 scans the table T T | X . Step 6 moves on into the next level enumerations in the search tree. Step 7 checks whether I(X)  C is the upper bound of an IRG that satisfies the user-specified constraints before inserting it into IRG. Note that step 7 must be performed after step 6 (the reason will be clear later). We first prove the correctness of the two steps by two lemmas as follows:  
84	 Lemma 3.3. T T | X | r i = T T | X+r i , r i  T T | X .E. 2  
85	 Lemma 3.3 is useful for explaining Step 6. It simply states that a X + r i conditional transposed table can be computed from a X conditional transposed table T T | X in the next level search after node X.  
86	 Lemma 3.1 ensures that at Step 7 only upper bounds of rule groups are possibly inserted into IRG. To determine whether an upper bound  discovered at node X represents an interesting rule group satisfying user-specified constraints, we need to compare .conf with all  .conf , where  .A  .A and  satisfies user specified constraints. FARMER ensures that all such  have already been discovered and kept in IRG at Step 7 by lemma 3.4 below.  
87	 Lemma 3.4. Let  : I(X)  C be the upper bound rule discovered at node X. The rule group with upper bound  : A  C such that A  I(X) can always be discovered at the descendent nodes of node X or in an earlier enumeration. Proof: Since A  I(X), and  and  are the upper bounds of two different rule groups, we see R(A )  R(I(X))  X. Let RS = {r|r  R(A )  r /  X} and r min  X be the row with the lowest ORD rank in row set X. If r  RS such that r r min , then node R(A ) is traversed before node X; otherwise node R(A ) is traversed at a descendent node of node X. 2 Algorithm FARMER Input: table D, specified consequent C, minsup, minconf , and minchi. Output: interesting rule groups with consequent C satisfying minimum measure thresholds. Method:  
88	 1. Initialization: Let T T be the transposed table of ORD ordered D; IRG = .  
89	 2. Mine Interesting Rule Groups: MineIRGs(T T |  , 0, 0, IRG).  
90	 3. Mine Lower Bounds of Interesting Rule Groups: Optional.  
91	 Subroutine: MineIRGs(T T | X , sup p , sup n , IRG). Parameters:  
92	 T T | X : a X-conditional transposed table;  
93	 sup p and sup n : support parameters;  
94	 IRG: the set of discovered interesting rule groups;  
95	 Method:  
96	 1. Apply Pruning 2: If I(X)  C is already identified, then return.  
97	 2. Apply Pruning 3: If prunable with the loose upper bounds of support or confidence, then return.  
98	 3. Scan T T | X and count the frequency of occurrences for each enumeration candidate, r i  T T | X .E, Let U p  T T | X .E P be the set of rows from T T | X .E P which occur in at least one tuple of T T | X ; Let U n  T T | X .E N be the set of rows from T T | X .E N which occur in at least one tuple T T | X ; Let Y p  T T | X .E P be the set of rows from T T | X .E P found in every tuple of T T | X ; Let Y n  T T | X .E N be the set of rows from T T | X .E N found in every tuple of T T | X ; sup p = sup p + |Y P | (|R(I(X)  C)|); sup n = sup n + |Y N | (|R(I(X)  ¬C)|);  
99	 4. Apply Pruning 3: If prunable with one of the three tight upper bounds, then return.  
100	 5. Apply Pruning 1: Update enumeration candidate list, T T | X .E P = U P - Y P , T T | X .E N = U N - Y N .  
101	 6. for each r i  T T | X .E do if r i  R(C) then T T | X | r i .E P = {r j |r j  T T | X .E P  r j ORD r i }; T T | X | r i .E N = T T | X .E N ; a = sup p + 1; b = sup n ; else T T | X | r i .E P = ; T T | X | r i .E N = {r j |r j  T T | X .E N  r j ORD r i }; a = sup p ; b = sup n + 1; M ineIRGs(T T | X | r i , a, b, IRG);  
102	 7. Let conf = (sup p )/(sup p + sup n ); If (sup p  minsup)  (conf  minconf ) (chi(sup p , sup p + sup n )  minchi) then if , (  IRG)  (.A  I(X))  (conf &gt; .conf ) then add upper bound rule I(X)  C into IRG.  
103	 Figure 5: The FARMER Algorithm  
104	 Step 7 is done after Step 6 to ensure that all descendant nodes of X are explored before determining whether the upper bound rule  at X is an IRG. Together with Lemma 3.2, we know that the complete and correct set of interesting rule groups will be in IRG.  
105	 Note that Step 6 implicitly does some pruning since it is possible that the enumeration candidate list is empty, i.e. T T | X .E = . It can be observed from the enumeration tree that there exist some combinations of rows, X, such that I(X) =  (an example is node "134"). This implies that there is no item existing in all the rows in X. When this happens, T T | X .E is empty and no further enumeration will be performed.  
106	 3.2 Pruning Strategies  
107	 We next look at the pruning techniques that are used in FARMER, which are essential for the efficiency. Our emphasis here is to show that our pruning steps do not prune off any interesting rule groups while preventing unnecessary traversals of the enumeration tree. Combining this with our earlier explanations on how all interesting rule groups are enumerated in FARMER without the pruning steps, the correctness of our algorithm will be obvious.  
108	 3.2.1 Pruning Strategy 1  
109	 Pruning strategy 1 is implemented at Step 5 of MineIRGs by pruning T T | X .Y , the set of enumeration candidate rows that occur in all tuples of the T T | X . We partition T T | X .Y to two subsets: Y p with consequent C and Y n without. The intuitive reason for the pruning is that we obtain the same set of upper bound rules along the branch X WITHOUT such rows. The correctness of such a pruning strategy is due to the following lemma.  
110	 Lemma 3.5. Let T T | X be a X-conditional transposed table. Given any subset R , R  T T | X .E, we have I(X  R ) = I(X  T T | X .Y  R ). Proof: By definition, I(X  R ) contains a set of items which occur in every row of (X  R ). Suppose candidate y  T T | X .Y (y occurs in every tuple of T T | X ), then either y  X  R (if y  R ) or y occurs in every tuple of the T T | XR (if y /  R ). In either case, I(X  R ) = I(X R {y}). Thus, I(X R ) = I(X T T | X .Y R ). 2  
111	 With Lemma 3.5, we can safely delete the rows in T T | X .Y from the enumeration candidate list T T | X .E.  
112	 Example 4. Consider T T | {2,3} , the conditional transposed table in Figure 2. Since enumeration candidate row 4 occurs in every tuples of T T | {2,3} , we can conclude that I({2, 3}) = I({2, 3, 4}) = {a, e, h}. Thus, we need not traverse node "234" and create T T | {2,3,4} . Row 4 can be safely deleted from T T | {2,3} .E. 2  
113	 Since I({2, 3, 4}) = I({2, 3}), the upper bound rule is identified at node "23" and node "234" is redundant. We say that node "234" is compressed to node "23".  
114	 We argue here that Lemma 3.4 still holds after applying pruning strategy 1. Without applying pruning strategy 1, for each node X, A  C, where A  I(X), is identified at a node X , which is traversed before node X or is a descendent node of node X. With pruning strategy 1, X might be compressed to a node X (X  X and I(X ) = I(X ) = A ), and we can see node X is either traversed before the subtree rooted at node X, or inside this subtree.  
115	 3.2.2 Pruning Strategy 2  
116	 This pruning strategy is implemented at Step 1 of MineIRGs. It will stop searching the subtree rooted at node X if the upper bound rule I(X)  C was already discovered previously in the enumeration tree because this implies that any rules to be discovered at the descendants of node X would have been discovered too. Lemma 3.6. Suppose pruning strategy 1 is utilized in the search. Let T T | X be the conditional transposed table of the current node X. All upper bounds to be discovered in the subtree rooted at node X must have already been discovered if there exists such a row r that satisfies the following conditions: (1)r /  X; (2)r /  T T | X .E; (3)for any ancestor node X i of node X, r /  T T | X i .Y (pruned by strategy 1); and (4)r occurs in each tuple of T T | X . Proof: Let X = {r 1 , r 2 , ..., r m }, where r 1 ORD r 2 ORD ... ORD r m . Suppose that there is a node X (X = X  {r }), we can have the following properties: (1) I(X) = I(X ); (2) r ORD r m , since r /  T T | X .E and r /  X; (3) T T | X .E = T T | X .E.  
117	 X is either enumerated or compressed to a node X C , where I(X C ) = I(X ) and T T | X .E  T T | X C .E. We can prove that either node X or node X C is traversed before node X by considering the following two cases: (1) If r ORD r 1 , node X or node X C falls in the subtree rooted at node {r }, which is traversed before node X. (2) If row ids in X follow the order r 1 ORD r 2 ORD ... ORD r t ORD r ORD r t+1 ORD ... ORD r m , node X or node X C falls in the subtree rooted at node X = {r 1 , ..., r t , r }, which is also traversed before node X. Because T T | X .E = T T | X .E and T T | X .E  T T | X C .E, we can conclude that all upper bounds to be discovered in the subtree rooted at node X must have already been discovered earlier in the subtree rooted at node X or node X C . 2  
118	 In the implementation of pruning strategy 2, the existence of such a r can be efficiently detected by a process called back counting without scanning the whole T T | X . Details are explained in section 3.3.  
119	 Example 5. Consider node "23" in Figure 3 where the upper bound rule {a, e, h}  C is identified for the first time. When it comes to node "34", we notice that row "2" occurs in every tuple of T T | {3,4} , "2" /  T T | {3,4} .E, and "2" /  T T | {3} .Y . So we conclude that all upper bounds to be discovered down node "34" have already been discovered before (I({3, 4}) = I({2, 3}) = {a, e, h}. I({3, 4, 5}) = ). We can prune the search down node "34". 2  
120	 3.2.3 Pruning Strategy 3  
121	 Pruning strategy 3 performs pruning by utilizing the userspecified thresholds, minsup, minconf and minchi. We estimate the upper bounds of the measures for the subtree rooted at the current node X. If the estimated upper bound at X is below the user-specified threshold, we stop searching down node X. A important thing to note here is that our pruning strategy is highly dependent on the order ORD which rank all rows with consequent C before rows with consequent ¬C.  
122	 Pruning strategy 3 consists of 3 parts: pruning using confidence upper bound, pruning using support upper bound and pruning using chi-square upper bound. This strategy is executed separately at Step 2 and Step 4 (Figure 5). At Step 2, we will perform pruning using the two loose upper bounds of support and confidence that can be calculated BEFORE scanning T T | X . At Step 4 we calculate the three tight upper bounds of support, confidence and chi-square value AFTER scanning T T | X .  
123	 For clarity, we will use the notations in Figure 3.2.3 to explain our pruning strategy here. Notation Description X the current enumeration node;  the upper bound rule I(X)  C at node X; X the immediate parent node of X;  the upper bound rule I(X )  C at node X ; r m a row id such that T T | X = T T | X | r m ;  
124	 Figure 6: Notations for Search Pruning  
125	 Pruning Using Support Upper Bound We have two support upper bounds for the rule groups identified at the subtree rooted at node X: the tight support upper bound U s1 (after scanning T T | X ) and the loose support upper bound U s2 (before scanning T T | X ). If the estimated upper bound is less than the minimum support minsup, the subtree can be pruned.  
126	 If r m has consequent C:  
127	 U s1 =  .sup + 1 + M AX(|T T | X .E P  t|), t  T T | X ;  
128	 U s2 =  .sup + 1 + |T T | X .E P |;  
129	 If r m has consequent ¬C then U s1 = U s2 =  .sup;  
130	 Lemma 3.7. U s1 and U s2 are the support upper bounds for the upper bound rules discovered in subtree rooted at node X. Proof: Because of the ORD order (Definition 3.2), if the consequent of r m is ¬C, the enumeration candidates of nodes down node X will also have consequent ¬C. The support can not increase down node X, so the support of upper bounds discovered in the subtree rooted at node X is less than  .sup. If r m has consequent C, for node X and its descendent nodes, the maximum increase of support from  .sup must come from the number of enumeration candidates with consequent C (|T T | X .E P |) at node X plus 1 (1 for r m )(U s2 ), or more strictly, from the maximum number of enumeration candidates with consequent C within a tuple of T T | X (M AX(|T T | X .E P  t|), t  T T | X ) plus 1 (U s1 ). 2  
131	 Note that we need to scan T T | X to get U s1 while U s2 can be obtained directly from the parameters sup p and X passed by the parent node. Pruning Using Confidence Upper Bound Similarly, we estimate two confidence upper bounds for the subtree rooted at node X, the tight confidence upper bound U c1 and the loose confidence upper bound U c2 . If the estimated upper bound is less than minimum confidence minconf , the subtree rooted at node X can be pruned.  
132	 Given U s1 and U s2 , the two confidence upper bounds of subtree rooted at node X, U c1 (tight) and U c2 (loose), are:  
133	 U c1 = U s1 /(U s1 + |R(.A  ¬C)|);  
134	 U c2 = U s2 /(U s2 + |R( .A  ¬C)|) (r m has consequent C);  
135	 U c2 = U s2 /(U s2 + |R( .A  ¬C)| + 1) (r m has consequent ¬C).  
136	 Lemma 3.8. U c1 and U c2 are the confidence upper bounds for the rules discovered in the subtree rooted at node X. Proof: For a rule  discovered in subtree rooted at node X, its confidence is computed as |R( .A  C)|/(|R( .A  C)| + |R( .A  ¬C)|). This expression can be simplified as x/(x+y), where x = |R( .AC)| and y = |R( .A¬C)|. This value is maximized by choosing the maximum value for x (U s1 and U s2 ) and minimum value for y. Suppose rule  is discovered at node X. For any rule  discovered under the enumeration tree under node X,  .A  .A because of pruning strategy 1, so we can see |R( .A¬C)|  |R(.A Figure 7: The Possible Chi-square Variables  
137	 ¬C)|. Thus the minimum value for y is |R(.A  ¬C)| or loosely at |R( .A  ¬C)| + 1(if r m has no consequent C) and |R( .A  ¬C)| (if r m has consequent C). 2  
138	 Example 6. Suppose minimum confidence minconf = 95%. At node "134", the discovered upper bound rule is "a  C" with confidence 0.75 &lt; 0.95. Since row 4 has no consequent C, any descendent enumeration will only reduce the confidence. Thus we can stop next level searching.  
139	 Pruning Using Chi-Square Upper Bound The chi-square value of an association rule is the normalized deviation of the observed values from the expected values.  
140	 Let  be a rule in the form of A  C of dataset D, n be the number of rows in D, and m be the number of instances with consequent C in D. The four observed values for chi-square value computation are listed in the following table. For example, O A¬C represents the number of rows that contain A but do not contain C. Let x = O A and y = O AC . Since m and n are constants, the chi-square value is determined by x and y only and we get chi-square function chi(x, y).  
141	 C ¬C Total A O AC = y O A¬C O A = x ¬A O ¬AC O ¬A¬C O ¬A = n - x Total O C = m O ¬C = n - m n  
142	 The following lemma gives an estimation of upper bound of chi square value for rules down the node X.  
143	 Lemma 3.9. Suppose rule  is discovered at enumeration node X. The chi-square upper bound for the upper bound rules discovered at the subtree rooted as node X is: max{ chi(x()-y()+m, m), chi(y()+n-m, y()), chi(x(), y())}. Proof: Suppose rule  (A  C) is identified in the subtree rooted at node X, x = O A and y = O A C . Since O(A) = |R(A)| and A  A. The followings are satisfied.  
144	 1) x  x  n (|R(A)|  |R(A )|)  
145	 2) y  y  m (|R(A  C)|  |R(A  C)|)  
146	 3) y  x (|R(A  C)|  |R(A )|)  
147	 4) n - m  x - y  x - y (|R(A  ¬C)|  |R(A  ¬C)|) The value pair (x ( ), y ( )) falls in the gray parallelogram (x(), y()), (x()-y()+m, m), (n, m), (y()+n-m, y() (Figure 7). Since the chi-square function chi(x, y) is a convex function [15], which is maximized at one of its vertexes, and chi(n, m) = 0 (please refer to [15]), we only need to consider the remaining three vertexes. 2 3.3 Implementation  
148	 In the implementation of FARMER, we use memory pointers [4] to point at the relevant tuples in the in-memory transposed table to simulate the conditional transposed table. Our implementation assumes that despite the high dimensionality, the microarray datasets that we are trying to handle are still sufficiently small to be loaded completely into the main memory. This is true for many gene expression datasets which have small number of rows.  
149	 Following is the running example. Suppose the current node is node "1" (Figure 8(a)), and minsup = 1. The inmemory transposed table is shown on the right hand side of the figure. Memory pointers are organized into conditional pointer lists.  
150	 In Figure 8(a), the "1"-conditional pointer list (at the top left corner of the figure) has 6 entries in the form of &lt; f i , P os &gt; which indicates the tuple (f i ) that contains r 1 and the position of r 1 within the tuple (P os). For example, the entry &lt; a, 1 &gt; indicates that row r 1 is contained in the tuple `a' at position 1. We can extend the "1"-conditional transposed table T T | {1} by following the P os. During one full scan of the transposed table, FARMER also generates the conditional pointer lists for other rows (i.e. r 2 , r 3 , r 4 and r 5 ). However, the generated "2"-conditional pointer list is slightly different in that it contains an entry for each tuple that contains r 2 BUT NOT r 1 . For example, although the tuple `a' contains r 2 , it does not appear in the "2"conditional pointer list. It will be inserted subsequently as we will see later.  
151	 A further scan through the "1"-conditional pointer list will allow us to generate the "12", "13", "14" and "15" conditional pointer lists. Figure 8(b) shows the state of memory pointers when we are processing node {1, 2}.  
152	 Finally, we show the state of conditional pointer lists after node {1} and all its descendants have been processed (Figure 8(c)). Since all enumerations involving row r 1 have been either processed or pruned off, the entries in the "1"conditional pointer list are moved into the remaining conditional pointer lists. The entries in the "2"-conditional pointer list will be moved to the other conditional pointer lists after node {2} and its descendants are processed, and so on.  
153	 Throughout all the enumerations described above, we need to implement our three pruning strategies. The implementation of strategies 1 and 3 is straightforward. For pruning strategy 2, we do a back scan through the conditional list to see whether there exists some row that satisfies the condition of Lemma 3.6. For example at node "2" in Figure 8(c), we scan from the position of each pointer to the head of each tuple, instead of scanning the transposed table from the position of each pointer to the end of each tuple. In this example, there is no row that satisfies the pruning condition of Lemma 3.6. Such an implementation is proven to be efficient for our purpose as shown in our experiments.  
154	 3.4 Finding Lower Bounds  
155	 In this section, we describe the algorithm, MineLB, which is designed to find the lower bounds of a rule group. Since a rule group has a unique upper bound and the consequent of a rule group is fixed, the problem can be regarded as generating the lower bounds for the antecedent of the upper bound rule. This antecedent could be regarded as a closed set (Definition 3.3) and the problem can be solved as long 2 5 3 2 4 3 5 2 4 5 a b c d e f g h 4 2 3 4 5 3 2 5 2 3 4 4 5 5  
156	 1 1 1 2 3 l  
157	 i 1 1 1 c b a 1 o  
158	 3 1 1 1 l s fi 1  
159	 f o p q r s t 5 Pos 1 1 1 1 d e h p r Pos 1  
160	 1-conditional  
161	 2-conditional  
162	 3-conditional  
163	 4-conditional  
164	 5-conditional 1  
165	 (a) Node {1} 1 2 3 4 5 3 2 5 2 3 4 4 5 5  
166	 1 1 1 2 1  
167	 12-conditional  
168	 13-conditional 1-conditional  
169	 15-conditional 14-conditional 3 l o p q r s t a 2 2 Pos l  
170	 Pos c 2 2 o  
171	 5 h 4 2 5 3 2 4 3 5 2 4 5 a b c d e f g l o s fi Pos  
172	 2-conditional  
173	 3-conditional 4-conditional  
174	 5-conditional 1  
175	 1 1 1 1 1 1 a b c  
176	 3 fi  
177	 fi  
178	 (b) Node {1,2} 3 4 5 3 2 5 2 3 4 4 5 5  
179	 1 1 1 2 3 4 2 1-conditional  
180	 3-conditional  
181	 4-conditional  
182	 5-conditional 2-conditional 1 1  
183	 2  
184	 q r s t 5 a 2 2 Pos 1 1 1 1 d e h l p r  
185	 p 5 3 2 4 3 5 2 4 5 a b c d e f g h l o o s fi Pos  
186	 1  
187	 3 fi 1 l 1 1 1 1 1 1 a b c  
188	 (c) Node {2}  
189	 Figure 8: Conditional Pointer Lists  
190	 as we can generate the lower bounds of a closed set.  
191	 Definition 3.3. Closed Set Let D be the dataset with itemset I and row set R. A (A  I) is a closed set of dataset D, iff there is no A  A such that R(A) = R(A ). A l , A l  A, is a lower bound of closed set A, iff R(A l ) = R(A) and there is no A  A l such that R(A ) = R(A). 2  
192	 MineLB is an incremental algorithm that is initialized with one closed set A, the antecedent of an upper bound rule, A  C for a rule group. It then updates the lower bounds of A incrementally whenever a new closed set A is added, where A  A and A is the antecedent of the newly added upper bound A  C. In this way, MineLB keeps track of the latest lower bounds of A. MineLB is based on the following lemma.  
193	 Lemma 3.10. Let A be the closed set whose lower bounds will be updated recursively and be the set of closed sets that are already added. Let A. be the current collection of lower bounds for A. When a new closed set A  A is added, A. is divided into two groups, A.1 and A.2, where A.1 = {l i |l i  A.  l i  A }, A.2 = A. - A.1. Then the newly generated lower bounds of A must be in the form of l 1  {i}, where l 1  A.1, i  A - A . Proof: Suppose l is a newly generated lower bound of A. (1) we prove l  l 1 . Since R(l) = R(A) (Definition 2.1) before A is added, there must exist a l i  A. such that l i  l  A. If l i  A.2, l can not be a new lower bound, since l i  A.2 is still a lower bound of A after A is added. So l  l 1 , l 1  A.1. (2) Obviously, the newly generated lower bound must contain at least one item from the set (A - A ). (3) l = l 1  {i} is a bound for A after adding A , where l 1  A.1, i  A and i /  A . Before A is added, l = l 1 {i} is a bound, so for any X  , l X. After A is added, l A because i /  A . So, l = l 1  {i} is a new bound for A after adding A . Based on (1), (2) and (3), we come to the conclusion that the newly generated lower bound for A after inserting A takes the form of l 1  {i}, where l 1  A.1 and i  (A - A ). 2  
194	 Itemset l 1  {i} described in Lemma 3.10 is a candidate lower bound of A after A is added. If l 1  {i} does not cover any l 2  A.2 and other candidates, l 1  {i} is a new lower bound of A after A is added. MineLB adopts bit vector for the above computation. Thus A. can be updated efficiently. The detailed algorithm is illustrated in Figure 9.  
195	 We can ensure that the closed sets (those that cover all the longest closed set A  A) obtained at Step 2 are sufficient for the correctness of MineLB because of Lemma 3.11  
196	 Lemma 3.11. If a closed set A1  A is already added and the collection of A's lower bounds, A., is already updated, A. will not change after adding closed set A2, A2  A1. Proof: After A1  A is added, A. is updated so that no l i  A. can satisfy l i  A1. So no l i  A. can satisfy l i  A2, A2  A1. Since A2 will not cover any l i  A., A. will not change, according to Lemma 3.10. 2  
197	 Example 7. Finding Lower Bound Given an upper bound rule with antecedent A = abcde and two rows, r 1 : abcf and r 2 : cdeg, the lower bounds A. of A can be determined as follows:  
198	 1)Initialize the set of lower bounds A. = {a, b, c, d, e};  
199	 2)add "abc" (= I(r 1 )  A): We get A.1 = {a, b, c} and A.2 = {d, e}. Since all the candidate lower bounds, "ad", "ae", "bd", "be", "cd", "ce" cover a lower bound from A.2, no new lower bounds are generated. So A. = {d, e};  
200	 3)add "cde" (= I(r 2 )  A): We get A.1 = {d, e} and A.2 = . The candidate lower bounds are "ad", "bd", "ae" and "be". Because none of them is covered by another candidate and A.2 = , A. = {ad, bd, ae, be}. 2  
201	 4. PERFORMANCE STUDIES  
202	 In this section, we will look at both the efficiency of FARMER and the usefulness of the discovered IRGs. All our experiments were performed on a PC with a Pentium IV 2.4 Ghz CPU, 1GB RAM and a 80GB hard disk. Algorithms were coded in Standard C. Datasets: The 5 datasets are the clinical data on lung cancer (LC) 4 , breast cancer (BC) 5 , prostate cancer (PC) 6 ,  
203	 4 http://www.chestsurg.org 5 http://www.rii.com/publications/default.htm 6 http://www-genome.wi.mit.edu/mpr/prostate Subroutine: MineLB(Table:D, upper bound rule: ).  
204	 1. A = .A; A. = {i|i  A};  = ;  
205	 2. for each row r id of D that r id /  R(A): if (I(r id )  A)  A then add (I(r id )  A) to ;  
206	 3. for each closed set A  : A.1 = A.2 = ; for each lower bound l i  A.: if l i  A then add l i to A.1; else add l i to A.2; CandiSet = ; for each l i  A.1 and each i  A &amp;&amp; i /  A : add candidate l i  {i} to CandiSet; A. = A.2; for each candidate c i  CandiSet if c i does not cover any l i  A.2 and c i does not cover any other c j  CandiSet then add c i to A.  
207	 4. Output A.. Figure 9: MineLB  
208	 ALL-AML leukemia (ALL) 7 , and colon tumor (CT) 8 . In such datasets, the rows represent clinical samples while the columns represent the activity levels of genes/proteins in the samples. There are two categories of samples in these datasets.  
209	 dataset # row # col class 1 class 0 #row of class 1 BC 97 24481 relapse nonrelapse 46 LC 181 12533 MPM ADCA 31 CT 62 2000 negative positive 40 PC 136 12600 tumor normal 52 ALL 72 7129 ALL AML 47  
210	 Table 1: Microarray Datasets  
211	 Table 1 shows the characteristics of these 5 datasets: the number of rows (# row), the number of columns (# col), the two class labels (class 1 and class 0), and the number of rows for class 1 (# class 1). All experiments presented here use the class 1 as the consequent; we have found that using the other consequent consistently yields qualitatively similar results.  
212	 To discretize the datasets, we use two methods. One is the entropy-minimized partition (for CBA and IRG classifier) 9  
213	 and the other is the equal-depth partition with 10 buckets. Ideally, we would like to use only the entropy discretized datasets for all experiments since we want to look at the classification performance of IRGs. Unfortunately, the two rule mining algorithms that we want to compare against are unable to run to completion within reasonable time (we ran them for several days without results) on the entropy discretized datasets, although FARMER is still efficient. As a result, we will report our efficiency results based on the equal-depth partitioned data while our classifier is built using the entropy-discretized datasets.  
214	 4.1 Efficiency of FARMER  
215	 The efficiency of FARMER will first be evaluated. We compare FARMER with the interesting rule mining algorithm in [2]. The algorithm in [2] is the one most related to FARMER in terms of interesting rule definition (but not the same, see related work). To our best knowledge, it is also  
216	 7 http://www-genome.wi.mit.edu/cgi-bin/cancer 8 http://microarray.princetion.edu/oncology/affydata 9 the code is available at http://www.sgi.com/tech/mlc/ the most efficient algorithm that exists with the purpose of mining interesting rules of our kind. We denote this algorithm as ColumnE since it also adopts column enumeration like most existing rule mining algorithms. We also compare FARMER with the closed set discovery algorithms CHARM [23] and CLOSET+ [21], which are shown to be more efficient than other association rule mining algorithms in many cases. We found that CHARM is always orders of magnitude faster than CLOSET+ on the microarray datasets and thus we do not report the CLOSET+ results here. Note that the runtime of FARMER includes the time for computing both the upper bound and lower bounds of each interesting rule group. Compared with CHARM, FARMER does extra work in: 1)computing the lower bounds of IRGs and 2) identifying the IRGs from all rule groups. Unlike FARMER that discovers both upper bound and lower bounds for each IRG, ColumnE only gets one rule for each IRG.  
217	 4.1.1 Varying Minimum Support  
218	 The first set of experiments (Figure 10) shows the effect of varying minimum support threshold minsup. The graphs plot runtime for the three algorithms at various settings of minimum support. Note that the y-axes in Figure 10 are in logarithmic scale. We set both minconf and minchi as ZERO, which disables the pruning with confidence upper bound and the pruning with the chi-square upper bound of FARMER.  
219	 For CHARM, minsup represents the least number of rows that the closed sets must match. The runtime of CHARM is not shown in Figures 10(a) and 10(b) because CHARM runs out of memory even at the highest support in Figure 10 on datasets BC and LC.  
220	 Figure 10 shows that FARMER is usually 2 to 3 orders of magnitude faster than ColumnE and CHARM (if it can be run). Especially at low minimum support, FARMER outperforms ColumnE and CHARM greatly. This is because the candidate search space for ColumnE and CHARM, dependent on the possible number of column combinations after removing the infrequent items, is orders of magnitude greater than the search space of FARMER, dependent on the possible number of row combinations, on microarray datasets.  
221	 As shown in Figure 10(f), the number of interesting rule groups discovered at a low minsup is much larger than that at a high minsup. Besides the size of row enumeration space, the number of IRGs also affects the efficiency of FARMER due to two reasons. First, since FARMER discovers IRGs by comparison (see algorithm section, step 7), more time will be spend when the number of IRGs to be compared against increase. Second, the time complexity of computing lower bounds in FARMER is O(n), where n is the number of IRGs. We observe that at high minsup, the time used to compute lower bounds takes 5% to 10% of the runtime of FARMER while the time taken at low minsup can be up to 20%. ColumnE also does the comparison to get interesting rules while all the runtime of CHARM is used to discover closed sets.  
222	 We choose our minimum support such that the runtime of FARMER is around 10 seconds. Although ColumnE and CHARM could perform better that FARMER for higher minsup, the absolute time difference however will be less (a) Lung Cancer (b) Breast Cancer (c) Prostate Cancer  
223	 (d) ALL-AML leukemia (e) Colon Tumor (f) Number of IRGs vs minsup (minchi=0)  
224	 Figure 10: Varying minsup  
225	 (a) Lung Cancer (b) Breast Cancer (c) Prostate Cancer  
226	 (d) ALL-AML leukemia (e) Colon Tumor (f) Number of IRGs vs minconf (minchi=0)  
227	 Figure 11: Varying minconf  
228	 than 10 seconds and thus is not interesting for comparison. This is negligible compared to the difference in runtime at low minsup.  
229	 4.1.2 Varying Minimum Confidence  
230	 The next set of experiments (Figure 11) shows the effect of varying minconf when minsup is fixed. The minchi pruning is still disabled by setting it to ZERO. For all the parameter settings in Figure 11, CHARM is unable to finish because of insufficient memory after several hours while ColumnE always has a runtime of more than 1 day. This is because we adopt a relative low minsup to study the effectiveness of confidence pruning in the experiment. To show the effect of various minconf clearly, we do not give the runtime of ColumnE. We will first ignore the lines marked with "minchi=10" here. We set minsup = 1, which means that minimum support pruning is almost disabled.  
231	 Figure 11 shows that the runtime of FARMER decreases when increasing minconf on all the 5 datasets (Figure 11(f) lists the number of IRGs). This shows that it is effective to exploit the confidence constraint for pruning. There is only a slight decrease in runtime of FARMER when the minconf increases from 85% to 99% since there are few upper bound rules with confidence between 85% and 99%. We observe that nearly all IRGs discovered at confidence 85% on these 5 datasets have a 100% confidence. As a result, FARMER does no additional pruning when minconf increases from 85% to 99%.  
232	 The result that many discovered IRGs have a 100% confidence is interesting and promising. It means that the IRGs are decisive and have good predictability.  
233	 4.1.3 Varying Minimum Chi-Square Value  
234	 The last set of experiments was performed to study the effectiveness of the chi-square pruning. Minimum chi-square constraint is usually treated as a supplementary constraint of minimum support and minimum confidence. We set minchi = 10 and plot the runtime vs various minconf in Figure 11 due to the space limitation, where minconf is set the same as in section 4.1.2.  
235	 The pruning exploited by constraint minchi = 10 is shown to be very effective on datasets BC, PC, CT and ALL. In some cases, the saving can be more than an order of magnitude. The pruning effect is not so obvious on dataset LC. By checking the identified IRGs, we found that discovered IRGs from LC usually have higher chi-square value. If we impose a tighter chi-square constraint by increasing minchi, the minchi pruning will be more obvious. Due to space constraint, we do not discuss this further.  
236	 As can be seen, in all the experiments we conducted, FARMER outperforms ColumnE and CHARM. Moreover, the prunings based on minsup, minconf and minchi are effective. In general, the runtime of FARMER correlates strongly with the number of interesting rule groups that satisfy all of the specified constraints. Our experimental results demonstrate that FARMER is extremely efficient in finding IRGs on datasets with small number of rows and large number of columns.  
237	 In additional to these experiments, we also look at how the performance of FARMER varies as the number of rows increase. This is done by replicating each dataset a number of times to generate a new dataset. It is observed that the performance of FARMER still outperform other algorithms even when the datasets are replicated for 5-10 times. Due to lack of space, we refer readers to [6] for these additional experiments.  
238	 4.2 Usefulness of IRGs  
239	 In order to show the usefulness of the discovered IRGs, we build a simple classifier called IRG classifier based on those IRGs that we discovered. Note that our emphasis here is not to build a new classifier but to provide some evidence that the discovery of IRGs is at least useful for such purpose.  
240	 We will compare our IRG classifier with two well-known classifiers CBA [14] and SVM [12], both available through the Internet. The open-source CBA algorithm (and all competitors we look at in the earlier section) failed to finish running in one week. To go around this problem, we build the CBA classifier by obtaining the frequent rules based on the upper bounds and lower bounds generated by FARMER. Our IRG classifier is similar to CBA but we use IRGs to build classifiers instead of all rules. Due to space limitation, we do not explain the details of the IRG classifier.  
241	 For CBA, we set the minimum support threshold as 0.7 number of training data of class C for each class C and set the minimum confidence threshold as 0.8 (According to our experiments, if we further lower the minimum confidence threshold, the final CBA classifier is the same); For IRG classifier, we use the same parameters as CBA; For SVM, we always use the default setting of SV M light [12].  
242	 dataset #training #test IRG classifier CBA SVM  
243	 BC 78 19 78.95% 57.89% 36.84%  
244	 LC 32 149 89.93% 81.88% 96.64%  
245	 CT 47 15 93.33% 73.33% 73.33%  
246	 PC 102 34 88.24% 82.35% 79.41% ALL 38 34 64.71% 91.18% 97.06% Average Accuracy 83.03% 77.33% 76.66%  
247	 Table 2: Classification Results  
248	 Table 2 illustrates the percentages of correctly predicted test data for the IRG classifier, CBA and SVM on the 5 microarray datasets. We can see that the IRG classifier has the highest average accuracy. Although SVM performs very well on LC and ALL, it fails on BC. No classifier outperforms the others on all datasets. Our IRG classifier is both efficient and easily understandable and thus could be a good reference tool for biological research.  
249	 5. RELATED WORK  
250	 Association rule mining has attracted considerable interest since a rule provides a concise and intuitive description of knowledge. It has already been applied on biological data, such as [7, 8, 19].  
251	 Association rule can relate gene expressions to their cellular environments or categories, thus available for building accurate classifiers on microarray datasets as in [9, 13]. Moreover, it can discover the relationship between different genes, so that we can infer the function of an individual gene based on its relationship with others [7] and build the gene network. Association rules might reveal more patterns than clustering [5], considering that a gene may belong to many rules while it is usually grouped to one cluster (or a hierarchy of clusters).  
252	 There are many proposals about rule mining in the data mining literatures. They can be roughly divided into three classes. The first two classes are related to association rule mining. All existing association rule mining algorithms adopt the column enumeration in the mining process, therefore they are very time-consuming on microarray datasets. The first class of rule mining algorithms identifies the interesting (or optimal) rules with some interestingness measures [2]. The interesting rule discussed in [2] is quite similar to our interesting rule group. However, [2] randomly picks a rule for each rule group while FARMER discovers the upper bound and lower bounds for each interesting rule group.  
253	 The second class of rule mining algorithms aims to find all association rules satisfying user-specified constraints by identifying all frequent itemsets at the key step, such as [1, 11]. Recently the concept of closed itemset [18] is proposed to reduce redundant itemsets and rules [22]. Several efficient mining algorithms [18, 23, 21] are proposed to mine frequent closed itemsets. On the other hand, there is some work [16, 20] that investigates incorporating item constraints to reduce the number of frequent itemsets. Other work [3, 15] leverages the item constraint as rule consequent and utilizes minimum thresholds of confidence, support and other statistic constraints. FARMER differs from these approaches in term of its enumeration method and pruning strategies.  
254	 The third class of algorithms aims at mining predictive rules. One example is the decision tree induction algorithm[10]. Alternatively, some work [9, 14] has been done to build classifiers from association rules and has obtained better classification results than decision trees in many cases. It is obvious that these methods are also applicable based on the concept of interesting rule groups.  
255	 In a short paper [17], the idea of using row enumeration for mining closed patterns in biological datasets is introduced. The idea is however restricted to finding frequent closed patterns that satisfy a certain support threshold. FARMER on the contrary finds IRGs that satisfy interestingness constraints like minconf and minchi. The effectiveness of pruning with such constraints is evident in our experiments.  
256	 6. CONCLUSIONS  
257	 In this paper, we proposed an algorithm called FARMER for finding the interesting rule groups in microarray datasets. FARMER makes use of the special characteristic of microarray datasets to enhance its efficiency. It adopts the novel approach of performing row enumeration instead of the conventional column enumeration so as to overcome the extremely high dimensionality of microarray datasets. Experiments show that FARMER outperforms existing algorithms like CHARM and ColumnE by a large order of magnitude on microarray datasets.  
258	 Our IRG classifier built on interesting rule groups demonstrates the usefulness of discovered IRGs. Our experiments showed that it has the highest average accuracy compared with CBA and SVM.  
259	 Acknowledgment: We will like to thank the anonymous reviewers from various conferences for their helpful suggestions which have led to great enhancements on the paper. Anthony Tung will like to thank his wife, Monica, for the patience and support through the course of his work.  
260	 7. REFERENCES  
261	 [1] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In Proc. 1994 Int. Conf. Very Large Data Bases (VLDB'94), pages 487­499, Sept. 1994.  
262	 [2] R. J. Bayardo and R. Agrawal. Mining the most intersting rules. In Proc. of ACM SIGKDD, 1999.  
263	 [3] R. J. Bayardo, R. Agrawal, and D. Gunopulos. Constraint-based rule mining on large, dense data sets. In Proc. 1999 Int. Conf. Data Engineering (ICDE'99).  
264	 [4] K. Beyer and R. Ramakrishnan. Bottom-up computation of sparse and iceberg cubes. In Proc. 1999 ACM-SIGMOD Int. Conf. Management of Data (SIGMOD'99).  
265	 [5] Y. Cheng and G. M. Church. Biclustering of expression data. In Proc of the 8th Intl. Conf. on intelligent Systems for Mocular Biology, 2000.  
266	 [6] G. Cong, A. K. H. Tung, X. Xu, F. Pan, and J. Yang. Farmer: Finding interesting rule groups in microarray datasets. Technical Report: National University of Singapore, 2004. [7] C. Creighton and S. Hanash. Mining gene expression databases for association rules. Bioinformatics, 19, 2003.  
267	 [8] S. Doddi, A. Marathe, S. Ravi, and D. Torney. Discovery of association rules in medical data. Med. Inform. Internet. Med., 26:25­33, 2001.  
268	 [9] G. Dong, X. Zhang, L. Wong, and J. Li. Caep: Classification by aggregating emerging patterns. In Proc. 2nd Int. Conf. Discovery Science (DS'99).  
269	 [10] J. Gehrke, R. Ramakrishnan, and V. Ganti. Rainforest: A framework for fast decision tree construction of large datasets. In Proc. 1998 Int. Conf. Very Large Data Bases (VLDB'98).  
270	 [11] J. Han and J. Pei. Mining frequent patterns by pattern-growth:methodology and implications. KDD Exploration, 2, 2000.  
271	 [12] T. Joachims. Making large-scale svm learning practical. 1999. svmlight.joachims.org/.  
272	 [13] J. Li and L. Wong. Identifying good diagnostic genes or genes groups from gene expression data by using the concept of emerging patterns. Bioinformatics, 18:725­734, 2002.  
273	 [14] B. Liu, W. Hsu, and Y. Ma. Integrating classification and association rule mining. In Proc. 1998 Int. Conf. Knowledge Discovery and Data Mining (KDD'98).  
274	 [15] S. Morishita and J. Sese. Traversing itemset lattices with statistical metric prunning. In Proc. of PODS, 2002.  
275	 [16] R. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang. Exploratory mining and pruning optimizations of constrained associations rules. In Proc. 1998 ACM-SIGMOD Int. Conf. Management of Data (SIGMOD'98).  
276	 [17] F. Pan, G. Cong, A. K. H. Tung, J. Yang, and M. J. Zaki. Carpenter: Finding closed patterns in long biological datasets. In Proc. 2003 ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD'03), 2003.  
277	 [18] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules. In Proc. 7th Int. Conf. Database Theory (ICDT'99), Jan.  
278	 [19] J. L. Pfaltz and C. Taylor. Closed set mining of biological data. In Workshop on Data Mining in BIoinformatics with (SIGKDD02), 2002.  
279	 [20] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proc. 1997 Int. Conf. Knowledge Discovery and Data Mining (KDD'97), 1997.  
280	 [21] J. Wang, J. Han, and J. Pei. Closet+: Searching for the best strategies for mining frequent closed itemsets. In Proc. 2003 ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD'03), 2003.  
281	 [22] M. Zaki. Generating non-redundant association rules. In Proc. 2000 Int. Conf. Knowledge Discovery and Data Mining (KDD'00), 2000.  
